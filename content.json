{"meta":{"title":"NJTECH_cxx","subtitle":"不要因为别人5%的负面评价而否定自己100%的努力。","description":null,"author":"Cxx","url":"http://cxx0822.github.io"},"pages":[{"title":"","date":"2019-01-07T01:51:49.297Z","updated":"2019-01-07T01:51:49.297Z","comments":true,"path":"baidu_verify_TurTrUK7im.html","permalink":"http://cxx0822.github.io/baidu_verify_TurTrUK7im.html","excerpt":"","text":"TurTrUK7im"},{"title":"文章分类","date":"2019-01-07T04:57:32.000Z","updated":"2019-01-07T05:03:17.775Z","comments":true,"path":"categories/index.html","permalink":"http://cxx0822.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Gmapping算法原理及源代码解析","slug":"Gmapping算法原理及源代码解析","date":"2020-05-05T11:15:30.000Z","updated":"2020-05-07T03:08:30.619Z","comments":true,"path":"2020/05/05/Gmapping算法原理及源代码解析/","link":"","permalink":"http://cxx0822.github.io/2020/05/05/Gmapping算法原理及源代码解析/","excerpt":"","text":"平台&emsp;&emsp;Ubuntu 16.04&emsp;&emsp;ROS Kinetic Kame 建图算法概述Gmapping源代码简单使用下载并编译源代码&emsp;&emsp;Gmapping的程序框架是依托Open_slam(OpenSLAM.org的目标是为SLAM研究人员提供一个平台，使他们能够发布自己的算法。OpenSLAM.org成立于2006年，并于2018年移至github。目前开源的项目有：OpenSLAM.org)，该框架主要分成slam_gmapping和openslam_gmapping。slam_gmapping是openslam_gampping在ros下的二次封装，真正的核心代码实现都在openslam_gampping里面。&emsp;&emsp;创建工程文件夹，并在src文件夹中下载源代码：12345git clone https://github.com/ros-perception/openslam_gmapping.gitgit clone https://github.com/ros-perception/slam_gmapping.gitgit clone https://github.com/ros-planning/navigation.gitgit clone https://github.com/ros/geometry2.gitgit clone https://github.com/ros-planning/navigation_msgs.git &emsp;&emsp;安装依赖库：123sudo apt-get install libsdl1.2-devsudo apt install libsdl-image1.2-devsudo apt-get install libbullet-dev &emsp;&emsp;回到工程文件夹，使用catkin_make编译。(编译时间较长)&emsp;&emsp;其中navigation文件夹中有一些功能包需要C++ 11标准，如果在编译过程中出现该类错误，需要在CMakeLists.txt文件中加入set(CMAKE_CXX_FIAGS &quot;-std=c++11 -o3&quot;)。(先试一下修改工程文件夹中src文件夹下的CMakeLists.txt文件，如果不行，则根据错误提示进入navigation中的功能包下的CMakeLists.txt文件进行修改。) 测试Gmapping参考博客博客1 Gmapping源代码理解&emsp;&emsp;下载完成后，进入src/slam_gmapping/gmapping/src文件夹，打开main.cpp，可以看到gmapping在ros下面定义的一个节点slam_gmapping，该节点也是上节中rosrun命令下的节点。&emsp;&emsp;mian函数比较简单，只是声明了一个节点和一个SlamGMapping类，并调用了该类下面的startLiveSlam()函数，该函数的声明在该文件夹下的slam_gmapping.cpp文件中。下面则正式进入源代码的分析。 Gmapping源代码框架 startLiveSlam()&emsp;&emsp;该函数的功能为正式开始Slam进程。其代码如下：12345678910111213void SlamGMapping::startLiveSlam()&#123; entropy_publisher_ = private_nh_.advertise&lt;std_msgs::Float64&gt;(\"entropy\", 1, true); sst_ = node_.advertise&lt;nav_msgs::OccupancyGrid&gt;(\"map\", 1, true); sstm_ = node_.advertise&lt;nav_msgs::MapMetaData&gt;(\"map_metadata\", 1, true); ss_ = node_.advertiseService(\"dynamic_map\", &amp;SlamGMapping::mapCallback, this); scan_filter_sub_ = new message_filters::Subscriber&lt;sensor_msgs::LaserScan&gt;(node_, \"scan\", 5); scan_filter_ = new tf::MessageFilter&lt;sensor_msgs::LaserScan&gt;(*scan_filter_sub_, tf_, odom_frame_, 5); scan_filter_-&gt;registerCallback(boost::bind(&amp;SlamGMapping::laserCallback, this, _1)); transform_thread_ = new boost::thread(boost::bind(&amp;SlamGMapping::publishLoop, this, transform_publish_period_));&#125; &emsp;&emsp;该函数整体上为ros中的Topic通讯，大致可以分为三部分，第一部分为消息的订阅(发送)，nodeName.advertise&lt;[msg_type]&gt;([msgName],[msgCountLimit],[latct])表示创建一个Publisher对象。通过调用对象的publish()函数，可以在这个topic上发布message。advertiseService表示订阅服务器的。 &emsp;&emsp;","categories":[],"tags":[]},{"title":"ROS功能包及其应用","slug":"ROS功能包及其应用","date":"2020-04-30T05:40:02.000Z","updated":"2020-05-07T14:18:22.085Z","comments":true,"path":"2020/04/30/ROS功能包及其应用/","link":"","permalink":"http://cxx0822.github.io/2020/04/30/ROS功能包及其应用/","excerpt":"Here's something encrypted, password is required to continue reading.","text":"Hey, password is required here. 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","categories":[{"name":"ROS开发与应用","slug":"ROS开发与应用","permalink":"http://cxx0822.github.io/categories/ROS开发与应用/"}],"tags":[]},{"title":"基于turtlebot的定位与建图","slug":"基于turtlebot的定位与建图","date":"2020-04-25T10:46:37.000Z","updated":"2020-04-26T02:47:23.501Z","comments":true,"path":"2020/04/25/基于turtlebot的定位与建图/","link":"","permalink":"http://cxx0822.github.io/2020/04/25/基于turtlebot的定位与建图/","excerpt":"","text":"平台&emsp;&emsp;Ubuntu16.04&emsp;&emsp;turtlebot2&emsp;&emsp;RPLIDAR A1/2(思岚) Ubuntu与ROS系统安装&emsp;&emsp;具体详见另一篇博客Ubuntu安装，ROS安装 turtlebot配置安装turtlebot库&emsp;&emsp;升级软件版本1sudo apt-get update &emsp;&emsp;安装依赖库：1sudo apt-get install ros-kinetic-turtlebot 注1：下面依赖库可以不装(包括注2)。1sudo apt-get install ros-kinetic-turtlebot-apps ros-kinetic-turtlebot-interactions ros-kinetic-kobuki-ftdi ros-kinetic-ar-track-alvar-msgs ros-kinetic-turtlebot-simulator 注2：ros-kinetic-rocon-remocon ros-kinetic-rocon-qt-library需要单独编译安装。&emsp;&emsp;建立工程文件夹catkin_ws(可以是其他的)，并下载源代码。12345mkdir catkin_wscd catkin_wsmkdir srccd srcgit clone https://github.com/robotics-in-concert/rocon_qt_gui.git &emsp;&emsp;编译源代码并添加至环境变量。123456cd ..catkin_makesource devel/setup.bashgedit ~/.bashrc # 打开环境变量文件source ~/catkin_ws/devel/setup.bash # 在文档最下面添加该命令source ~/.bashrc # 生效 &emsp;&emsp;如编译报错，则需安装以下依赖库：1sudo apt-get install pyqt4-dev-tools sudo apt-get install pyqt5-dev-tools 测试检查硬件连接&emsp;&emsp;打开turtlebot2电源，将turtlebot2的USB线接入工控机(PC机)的USB口(最好是USB3.0)，然后打开终端，启动ROS。1roscore &emsp;&emsp;再打开一个终端，输入检测命令：1ls /dev/kobuki &emsp;&emsp;显示相应设备即可。&emsp;&emsp;如果没有显示，则执行：1rosrun kobuki_ftdi create_udev_rules 检查底盘及电机&emsp;&emsp;打开电源，再打开一个终端，输入启动底盘命令：1roslaunch turtlebot_bringup minimal.launch &emsp;&emsp;不报错，听到声音即可。&emsp;&emsp;再打开一个终端，输入键盘操作底盘命令：1roslaunch turtlebot_teleop keyboard_teleop.launch &emsp;&emsp;根据提示信息，输入指令字母，turtlebot能动即可。 激光雷达配置安装驱动&emsp;&emsp;需要下载思岚官网提供的激光雷达驱动包。 创建工程文件夹1234mkdir catkin_ws # 可以是其他名字cd catkin_wsmkdir srccd src 下载并编译123git clone https://github.com/Slamtec/rplidar_ros.gitcd ..catkin_make 添加环境变量1234source devel/setup.bashgedit ~/.bashrc # 打开环境变量文件source ~/catkin_ws/devel/setup.bash # 在文档最下面添加该命令source ~/.bashrc # 生效 设置串口权限&emsp;&emsp;使激光雷达串口可以读和写。 检查端口号1dmesg | grep ttyUSB* &emsp;&emsp;有cp210x对应的USB口即可。(RPLIDAR A系列激光雷达使用的是cp210x串口驱动)。 设置权限1sudo gedit /etc/udev/rules.d/70-ttyusb.rules &emsp;&emsp;打开后，输入：1KERNEL==\"ttyUSB[0-9]*\", MODE=\"0666\" &emsp;&emsp;运行如下程序，查看权限，有两个rw即可。1ls -l /dev |grep ttyUSB* 添加环境变量，增加激光雷达别名&emsp;&emsp;添加别名：1echo \"export TURTLEBOT_LASER_SENSOR=rplidar\" &gt;&gt; ~/.bashrc &emsp;&emsp;添加环境变量：1echo \"source ~/catkin_ws/devel/setup.bash\" &gt;&gt; ~/.bashrc &emsp;&emsp;转至rplidar_ros文件夹，输入命令：1./scripts/create_udev_rules.sh 测试激光雷达1roslaunch rplidar_ros rplidar.launch &emsp;&emsp;无错误，激光雷达正常运转即可。 基于hector-slam的定位与建图平台&emsp;&emsp;硬件：RPLIDAR A1，算法：hector-slam。 安装算法包&emsp;&emsp;具体操作见上。算法包网址：1git clone https://github.com/NickL77/RPLidar_Hector_SLAM 定位与建图&emsp;&emsp;打开激光雷达：1roslaunch rplidar_ros rplidar.launch &emsp;&emsp;运行建图算法：1roslaunch hector_slam_launch tutorial.launch","categories":[],"tags":[]},{"title":"无人清洁车","slug":"无人清洁车","date":"2020-03-13T01:13:39.000Z","updated":"2020-03-19T02:01:09.483Z","comments":true,"path":"2020/03/13/无人清洁车/","link":"","permalink":"http://cxx0822.github.io/2020/03/13/无人清洁车/","excerpt":"","text":"ROS基础知识roslaunch命令命令格式1roslaunch 功能包名称 launch文件 [参数设置(arg:=value)] &emsp;&emsp;这里roslaunch命令会在对应的功能包中找到名称匹配的launch文件，并执行这个launch文件。如果设置了参数的话，会将参数传入到launch文件中赋给对应的参数。 功能及作用&emsp;&emsp;roslaunch可以运行多个节点。并且roslaunch命令在运行节点时，还可以附加一些ROS命令选项，比如修改参数或节点的名称,设置节点的命名空间,设置ROS_ROOT及ROS_PACKAGE_PATH,以及环境变量修改等选项的ROS命令。 rosrun命令命令格式1rosrun 功能包名称 节点名称 功能及作用&emsp;&emsp;rosrun是执行指定的功能包中的一个节点的命令。 rostopic命令格式1rostopic pub 话题名称 消息类型 参数 功能及作用&emsp;&emsp;rostopic向正在广播的话题发布数据消息。 使用步骤1.打开电源，启动Ubuntu系统，连接工控机局域网/外接路由器。&emsp;&emsp;工控机局域网为：Wideora-5f92。注：挪动清洁车时，一定要打开电源，小距离移动除外。 2.手推/键盘操作移动清洁车，移动区域覆盖目标区域。ROS命令：1roslaunch anbot_bringup anbot_bringup.launch 启动键盘ROS命令：1rosrun teleop_twist_keyboard teleop_twist_keyboard.py &emsp;&emsp;实际建图时，可以通过Rviz观察当前建图信息，并调整清洁车位置。&emsp;&emsp;彩色表示当前激光雷达扫描区域，黑色表示墙等障碍物，灰白色表示空白作业区域。外圈白色线条状表示因地面等其他因素激光照射出去的范围。 3.保存地图，人工修图。ROS命令：1rostopic pub /robot_map_save std_msgs/String \"data: ''\" &emsp;&emsp;注：实际操作时，按Tab键自动补全。&emsp;&emsp;图片位于1SweepRobotManger/robot/install/share/robot_config/map/sweepmap.jpg &emsp;&emsp;注：map中sweepmap是导航实际用的地图，mymap是导航时可视化界面rviz中的地图。 &emsp;&emsp;未修之前地图：&emsp;&emsp;修图工具：Pinta，功能界面类似于画图，普通画图编辑器。&emsp;&emsp;修图目标：1.确保场景为封闭环境。2.确保边界清晰。3.确保作业区域无杂质。&emsp;&emsp;操作方法：使用画笔工具添加或加粗边界，使用橡皮擦工具清除杂质。&emsp;&emsp;修改之后地图： 4.启动导航，手动定位。ROS命令：1roslaunch anbot_bringup anbot_navigation.launch &emsp;&emsp;注：启动前按住急停按钮，确保清洁车不能移动，定位后在按下急停按钮，使其作业。&emsp;&emsp;启动该命令后，自动进入Rviz界面，手动调整机器人位姿，使其在正确的地图位置中。&emsp;&emsp;定位校准前：&emsp;&emsp;具体操作：点击2D Pose Estimate，拖动鼠标，确定方向，使激光雷达扫描区域(彩色区域)和实际地图中墙壁重合。&emsp;&emsp;定位校准后：&emsp;&emsp;注：绿色线条区域为导航算法自动生成。红色为机器人位置。 5.自动导航，启动刷盘等，开始作业。ROS命令：1rostopic pub /agent anbot_msgs/Agent \"&#123;太多了 不一一写了，tab补全即可&#125;\" &emsp;&emsp;通过键盘将brushUpDown和squeegeUpDown数值改为1即可。&emsp;&emsp;注：启动该命令前，确保清洁车处于急停不可移动状态。 附录调整激光雷达参数&emsp;&emsp;目前清洁车搭载的激光雷达型号为sick激光雷达，具体型号有561,571等。其中561款的射程为10米，571款的射程为25米。两款型号不一致，IP端口、参数配置也不一样。 硬件配置&emsp;&emsp;需要更改其IP地址，目前清洁车局域网内，激光雷达的IP地址为192.168.8.100。具体方法参考博客：地址。 软件配置&emsp;&emsp;在/share/sick_tim/launch文件夹中找到571的配置文件，将range_max参数更改为相应的射程。&emsp;&emsp;在/share/robot_config/slam_config找到gmapping_param.yaml配置文件，将maxRange参数更改为相应的射程。","categories":[],"tags":[]},{"title":"视觉系统界面设计","slug":"视觉系统界面设计","date":"2020-01-07T02:01:53.000Z","updated":"2020-01-09T07:22:43.391Z","comments":true,"path":"2020/01/07/视觉系统界面设计/","link":"","permalink":"http://cxx0822.github.io/2020/01/07/视觉系统界面设计/","excerpt":"","text":"平台&emsp;&emsp;界面设计软件：Qt&emsp;&emsp;编程语言：C++ 界面设计整体界面&emsp;&emsp;整体分为标题栏，菜单栏，状态栏和功能区，图像区，信息区和参数区等。采用灰白色主色调，华为楷体字体。 设计原则&emsp;&emsp;采用界面布局和控件功能分开设计的原则，即界面布局在Qt Creator中的设计窗口中设计。控件功能通过槽函数在编辑窗口中设计。 界面布局&emsp;&emsp;控件通过拖拽产生，其位置大小图标等信息在设计窗口的右下角部分直接编辑其属性即可。控件样式通过右击控件选择改变样式表设计。最终使用其布局工具栏对界面快速布局。&emsp;&emsp;注： 所有控件必须布局，否则无法产生效果。 可以利用其minimumSize和maximumSize属性控制控件的大小。 &emsp;&emsp;附： 2","categories":[{"name":"工业机器人视觉","slug":"工业机器人视觉","permalink":"http://cxx0822.github.io/categories/工业机器人视觉/"}],"tags":[]},{"title":"C++动态库的实现与调用","slug":"C++动态库的实现与调用","date":"2019-12-23T02:52:06.000Z","updated":"2020-01-09T07:19:37.032Z","comments":true,"path":"2019/12/23/C++动态库的实现与调用/","link":"","permalink":"http://cxx0822.github.io/2019/12/23/C++动态库的实现与调用/","excerpt":"","text":"平台&emsp;&emsp;Visual Studio 2015&emsp;&emsp;Qt Creator 概述&emsp;&emsp;动态链接库(Dynamic Link Library)DLL文件与exe文件一样也是可执行文件，但是DLL也被称之为库，因为里面封装了各种类和函数等，就像是一个库一样，存储着很多东西，主要是用来调用的。调用方式主要分为两种：隐式(通过lib文件与头文件)与显式(只通过DLL文件)。&emsp;&emsp;代码复用是提高软件开发效率的重要途径。一般而言，只要某部分代码具有通用性，就可将它构造成相对独立的功能模块并在之后的项目中重复使用。比较常见的例子是各种应用程序框架，ATL、MFC等，它们都以源代码的形式发布。由于这种复用是源码级别的，源代码完全暴露给了程序员，因而称之为白盒复用。为了弥补白盒复用的不足，就提出了二进制级别的代码复用。使用二进制级别的代码复用一定程度上隐藏了源代码，对于缓解代码耦合现象起到了一定的作用。这样的复用被称为黑盒复用。&emsp;&emsp;注：实现黑盒复用的途径不只dll一种，静态链接库甚至更高级的COM组件都是。 Visual Studio实现C++的动态链接库创建动态链接库新建工程项目CreateDLL&emsp;&emsp;打开Visual Studio 2015新建工程，并选择DLL应用程序类型。 创建.cpp和.h文件&emsp;&emsp;右击项目工程，添加-&gt;新建项，依次建立createDLL.cpp和createDLL.h文件。 编写文件createDLL.h1__declspec(dllexport) void test_fun(); createDLL.cpp&emsp;&emsp;这里写了一个简单的hello world测试程序。123456789#include \"createDLL.h\"#include &lt;iostream&gt;using namespace std;void test_fun()&#123; cout &lt;&lt; \"hello world\" &lt;&lt; endl;&#125; 编译&emsp;&emsp;重新生成解决方案 文件位置&emsp;&emsp;在Debug文件夹下，有新生成的动态链接库文件。 使用动态链接库新建工程UseDLL&emsp;&emsp;这里选择控制台应用程序。 添加文件&emsp;&emsp;将刚才的createDLL.dll,createDLL.lib和createDLL.h文件放到工程文件目录下。 &emsp;&emsp;将刚才的`createDLL.dll`,`createDLL.lib`和`createDLL.h`文件分别放到工程文件目录下的头文件和资源文件中。 新建.cpp&emsp;&emsp;调用之前的函数即可。1234567#include \"createDLL.h\"int main()&#123; test_fun(); return 0;&#125; QT实现C++的动态链接库创建动态链接库新建工程项目CreateDLL&emsp;&emsp;打开Qt Creator新建工程，并选择Library模板中的C++库。&emsp;&emsp;重命名为Qt Creator，后续操作直接下一步即可。&emsp;&emsp;创建完成后，工程文件夹下会有3个文件。 编写类&emsp;&emsp;在createdll.h和createdll.cpp文件中编写该类。如添加一个显示hello world的函数。 createdll.h1234567891011121314#ifndef CREATEDLL_H#define CREATEDLL_H#include \"createdll_global.h\"class CREATEDLLSHARED_EXPORT CreateDLL&#123;public: CreateDLL(); void test_fun(); // 添加的内容&#125;;#endif // CREATEDLL_H createdll.cpp1234567891011121314#include \"createdll.h\"#include &lt;iostream&gt; // 添加的内容using namespace std; // 添加的内容CreateDLL::CreateDLL()&#123;&#125;// 添加的内容void CreateDLL::test_fun()&#123; cout &lt;&lt; \"hello world\" &lt;&lt; endl;&#125; 编译&emsp;&emsp;构建-&gt;运行即可。如果没有报错，则在项目的同级文件夹下会产生一个名为build-CreateDLL-Desktop_Qt_5_8_0_MSVC2015_64bit-Debug文件夹(如果没有更改的话)。 文件位置&emsp;&emsp;打开其Debug文件夹，里面有新生成的dll库文件。 使用动态链接库新建工程UseDLL&emsp;&emsp;打开Qt Creator新建工程，并选择Application模板中的Qt Console Application。其次和之前类似。 添加文件&emsp;&emsp;将刚才的createdll.h和createdll.cpp文件放入到该工程的文件夹下。并添加至该工程项目中。&emsp;&emsp;除此之外，还有一个很重要的步骤！将刚才生成的CreateDLL.dll库文件(位于Debug文件夹下)添加至工程文件.pro中。语法规则为：LIBS += -L路径 -l库名(不需要加后缀)。 编写.cpp12345678910111213#include &lt;QCoreApplication&gt;#include \"createdll.h\" // 添加的内容int main(int argc, char *argv[])&#123; QCoreApplication a(argc, argv); //添加的内容 CreateDLL cdll; cdll.test_fun(); return a.exec();&#125; &emsp;&emsp;运行结果为：","categories":[{"name":"C++","slug":"C","permalink":"http://cxx0822.github.io/categories/C/"}],"tags":[]},{"title":"模板匹配算法简介","slug":"模板匹配算法简介","date":"2019-12-16T03:27:23.000Z","updated":"2020-01-09T07:22:26.450Z","comments":true,"path":"2019/12/16/模板匹配算法简介/","link":"","permalink":"http://cxx0822.github.io/2019/12/16/模板匹配算法简介/","excerpt":"","text":"基本概念&emsp;&emsp;在图像中寻找目标图像（模板），或者说，就是在图像中寻找与模板图像相似部分的一种图像处理技术。依赖于选择的方法不同，模板匹配可以处理各种情形下的变换，如照明、杂点、大小、位置以及旋转，甚至模板内部的相对移动。 分类&emsp;&emsp;HDevelop开发环境中提供的匹配的方法主要有三种，即Component-Based、Gray-Value-Based、Shape-Based,分别是基于组件（或成分、元素）的匹配，基于灰度值的匹配和基于形状的匹配。 总体流程 形状匹配基本流程 详细步骤 读取图像，这里一定要是灰度图，如果是彩色图，先用rgb1_to_gray()转成灰度图像； 确定ROI的区域。例如确定矩形，只需要确定矩形的左上点和右下点的坐标即可，利用gen_rectangle1()生成一个矩形，利用area_center()找到这个矩形的中心； 从图像中获取这个ROI区域的图像，利用reduce_domain()可以得到这个ROI。在建立模板之前，可以先对这个区域进行一些预处理，方便以后的建模，比如阈值分割，形态学的一些处理等等（后续匹配时，也需要执行预处理操作）； 利用create_shape_model()创建模板。这个函数有许多参数，具体见下文分析; 创建好模板后，这时还需要使用inspect_shape_model()来监视模板，它检查参数的适用性，还能帮助找到合适的参数；然后在使用get_shape_model_contours()获得这个模板的轮廓，用于后面的匹配； 创建好模板后，就可以打开另一幅图像，进行模板匹配了。这个过程就是在新图像中寻找与模板匹配的图像部分，利用find_shape_model()进行模板匹配，具体参数含义见下文分析； 最后利用vector_angle_to_rigid()和affine_trans_contour_xld()将其转化并显示。前一个是从一个点和角度计算一个刚体仿射变换，这个函数从匹配函数的结果中对构造一个刚体仿射变换（平移，旋转和缩放）很有用，把参考图像变为当前图像。 参数含义create_shape_model()1234567891011create_shape_model(const Hobject&amp; Template , //reduce_domain后的模板图像Hlong NumLevels, //金字塔的层数，可设为“auto”或0—10的整数Double AngleStart, //模板旋转的起始角度Double AngleExtent, //模板旋转角度范围, &gt;=0Double AngleStep, //旋转角度的步长， &gt;=0 and &lt;=pi/16const char* Optimization, //设置模板优化和模板创建方法const char* Metric, //匹配方法设置Hlong Contrast, //设置对比度Hlong MinContrast , //设置最小对比度Hlong* ModelID ) //输出模板句柄 &emsp;&emsp;NumLevels越大，找到匹配使用的时间就越小。如果金字塔的层数太大，模板不容易识别出来，这是需要将find_shape_model函数中MinScore和Greediness参数设置的低一些。如果金字塔层数太少找到模板的时间会增加。&emsp;&emsp;AngleStart和AngleExtent决定可能的旋转范围，AngleStep指定角度范围搜索的步长；&emsp;&emsp;Optimization：模板定位时使用的特征点的数量，模板点储存模式，设为none时，全点储存。当模板较小时，较少点数并不会降低模板匹配时间。在模板数据量大时有必要降低数据量(point_reduction_high)，以提高定位速度。&emsp;&emsp;Metric定义了在图像中匹配模板的条件，如果Metric=use_polarity，图像中的目标必须和模型具有一样的对比度。例如，如果模型是一个亮的目标在一个暗的背景上，那么仅仅那些比背景亮的目标可以找到。如果Metric=ignore_global_polarity,在两者对比度完全相反时也能找到目标。在上面的例子中，如果目标是比背景暗的也能将目标找到。&emsp;&emsp;Contras决定着模型点的对比度。对比度是用来测量目标与背景之间和目标不同部分之间局部的灰度值差异。Contrast的选择应该确保模板中的主要特征用于模型中。Contrast也可以是两个数值，这时模板使用近似edges_image函数中滞后阈值的算法进行分割。这里第一个数值是比较低的阈值，第二个数值是比较高的阈值。Contrast也可以包含第三个，这个数值是在基于组件尺寸选择重要模型组件时所设置的阈值，比如，比指定的最小尺寸的点数还少的组件将被抑制。这个最小尺寸的阈值会在每相邻的金字塔层之间除以2。如果一个小的模型组件被抑制，但是不使用滞后阈值，然而在Contrast中必须指定三个数值，在这种情况下前两个数值设置成相同的数值。这个参数的设置可以在inspect_shape_model函数中查看效果。如果Contrast设置为auto，create_shape_model将会自动确定三个上面描述的数值。或者仅仅自动设置对比度(auto_contrast)，滞后阈值(auto_contrast_hyst)或是 最小尺寸(auto_min_size)中一个。其他没有自动设置的数值可以按照上面的格式再进行设置。可以允许各种组合，例如：如果设置 [‘auto_contrast’,’auto_min_size’]，对比度和最小尺寸自动确定；如果设置 [‘auto_min_size’,20,30]，最小尺寸会自动设定，而滞后阈值被设为20和30。有时候可能对比度阈值自动设置的结果是不满意的，例 如，由于一些具体应用的原因当某一个模型组件是被包含或是被抑制时，或是目标包含几种不同的对比度时，手动设置这些参数效果会更好。&emsp;&emsp;MinContrast用来确定在执行find_shape_model函数进行识别时模型的哪一个对比度必须存在，也就是说，这个参数将模型从噪声图像中分离出来。因此一个好的选择应该是在图像中噪声所引起的灰度变化范围。例如，如果灰度浮动在10个灰度级内，MinContrast应该设置成10。如果模板和搜索图像是多通道图像，Metric参数设置成ignore_color_polarity，在一个通道中的噪声必须乘以通道个数的平方根再去设置MinContrast。例如，如果灰度值在一个通道的浮动范围是10个灰度级，图像是三通道的，那么MinContrast应该设置为17。很显然，MinContrast必须小于Contrast。如果要在对比度较低的图像中识别模板，MinContrast必须设置为一个相对较小的数值。如果要是模板即使严重遮挡(occluded)也能识别出来，MinContrast应该设置成一个比噪声引起的灰度浮动范围略大的数值，这样才能确保在find_shape_model函数中提取出模板准确的位置和旋转角度。如果MinContrast设置为auto，最小对比度会基于模板图像中的噪声自动定义。因此自动设定仅仅在搜索图像和模板图像噪声近似时才可以使用。 find_shape_model()1234567891011121314151617find_shape_model(const Hobject&amp; Image, //搜索图像Hlong ModelID, //模板句柄Double AngleStart, // 搜索时的起始角度Double AngleExtent, //搜索时的角度范围，必须与创建模板时的有交集。Double MinScore, // 输出的匹配的质量系数Score 都得大于该值Hlong NumMatches, // 定义要输出的匹配的最大个数Double MaxOverlap, // 当找到的目标存在重叠时，且重叠大于该值时选//择一个好的输出const char* SubPixel, // 计算精度的设置，五种模式，多选2，3Hlong NumLevels, // 搜索时金字塔的层数Double Greediness , //贪婪度，搜索启发式，一般都设为0.9，越高速度快//容易出现找不到的情况Halcon::HTuple* Row, //输出匹配位置的行坐标Halcon::HTuple* Column, //输出匹配位置的列坐标Halcon::HTuple* Angle, //输出匹配角度Halcon::HTuple* Score ) //输出匹配质量 &emsp;&emsp;Score是一个0到1之间的数，是模板在搜索图像中可见比例的近似测量。如果模板的一半被遮挡，该值就不能超过0.5。&emsp;&emsp;Image的domain定义了模型参考点的搜索区域，参数AngleStart和AngleExtent确定了模型搜索的旋转角度，参数MinScore定义模板匹配时至少有个什么样的质量系数才算是在图像中找到模板。MinScore设置的越大,搜索的就越快。如果模板在图像中没有被遮挡， MinScore可以设置为0.8这么高甚至0.9。&emsp;&emsp;NumMatches定义了在图像上找到模板的最大的个数。如果匹配时的质量系数大于MinScore的目标个数多于NumMatches，仅仅返回质量系数最好的NumMatches个目标位置。如果找的匹配目标不足NumMatches，那么就只返回找到的这几个。参数MinScore优于NumMatches。&emsp;&emsp;如果模型具有对称性，会在搜索图像的同一位置和不同角度上找到多个与目标匹配的区域。参数MaxOverlap是0到1之间的，定义了找到的两个目标区域最多重叠的系数，以便于把他们作为两个不同的目标区域分别返回。如果找到的两个目标区域彼此重叠并且大于MaxOverlap，仅仅返回效果最好的一个。重叠的计算方法是基于找到的目标区域的任意方向的最小外接矩形(看smallest_rectangle2)。如果MaxOverlap=0, 找到的目标区域不能存在重叠, 如果MaxOverla p=1，所有找到的目标区域都要返回。&emsp;&emsp;SubPixel确定找到的目标是否使用亚像素精度提取。如果SubPixel设置为none(或者false背景兼容)，模型的位置仅仅是一个像素精度和在create_shape_model中定义的角度分辨率。如果SubPixel设置为interpolation(或true)，位置和角度都是亚像素精度的。在这种模式下模型的位置是在质量系数函数中插入的，这种模式几乎不花费计算时间，并且能达到足够高的精度，被广泛使用。&emsp;&emsp;NumLevels是在搜索时使用的金字塔层数，如有必要，层数截成创建模型时的范围。如果NumLevels=0，使用创建模板时金字塔的层数。另外NumLevels还可以包含第二个参数，这个参数定义了找到匹配模板的最低金字塔层数。NumLevels=[4,2]表示匹配在第四层金字塔开始，在第二层金字塔找到匹配（最低的设为1）。可以使用这种方法降低匹配的运行时间，但是这种模式下位置精度是比正常模式下低的，所谓正常模式是在金字塔最底层匹配。因此如果需要较高的精度，应该设置SubPixel至少为least_squares。如果金字塔最底层设置的过大，可能不会达到期望的精度，或者找到一个不正确的匹配区域。这是因为在较高层的金字塔上模板是不够具体的，不足以找到可靠的模板最佳匹配。在这种情况下最低金字塔层数应设为最小值。&emsp;&emsp;参数Greediness确定在搜索时的“贪婪程度”。如果Greediness=0，使用一个安全的搜索启发式，只要模板在图像中存在就一定能找到模板，然而这种方式下搜索是相对浪费时间的。如果Greediness=1，使用不安全的搜索启发式，这样即使模板存在于图像中，也有可能找不到模板，但只是少数情况。如果设置Greediness=0.9，在几乎所有的情况下，总能找到模型的匹配。 实战代码解释读取图像12345read_image (Image, 'C:/Users/Cxx/Desktop/Image.jpg') //读取图像get_image_size (Image, Width, Height)//获取图像的大小dev_close_window ()//关闭窗口dev_open_window (0, 0, Width, Height, 'black', WindowHandle)//打开一个新窗口dev_display (Image)//显示图像 创建ROI区域（人为给定取值）123gen_circle (Circle, 270, 280, 90)//创建圆形区域 reduce_domain (Image, Circle, ImageReduced)//获得ROI区域的图像area_center (Circle, Area, Row1, Column1)//求出中心坐标 创建模板并监视模板12345create_scaled_shape_model (ImageReduced, 5, rad(-45), rad(90), 'auto', 0.8, 1.1, 'auto', 'auto', 'ignore_global_polarity', 'auto', 'auto', ModelID)// 准备一个可缩放比例的匹配轮廓模型inspect_shape_model (ImageReduced, ModelImages, ModelRegions, 4, 20)//创建一个轮廓匹配模型基于金字塔的图像 get_shape_model_params (ModelID, NumLevels, AngleStart, AngleExtent, AngleStep, ScaleMin, ScaleMax, ScaleStep, Metric, MinContrast)//返回一个轮廓匹配模型的参数get_shape_model_contours (ModelContours, ModelID, 1)write_shape_model (ModelID, 'C:/Users/Cxx/Desktop/img_model3.shm')//保存模板 仿射变换12345dev_set_color ('red')//设置颜色dev_set_line_width (2)//设置线宽vector_angle_to_rigid (0, 0, 0, Row1, Column1, 0, HomMat2D)//通过点和角度方面来计算一个放射变换矩阵affine_trans_contour_xld (ModelContours, ContoursAffineTrans, HomMat2D)//通过任意2D放射变换转换为XLD轮廓 stop () 模板匹配12345678910111213141516read_image (Image1, 'C:/Users/Cxx/Desktop/Image1.jpg')//读取另一张图片dev_display (Image1)//显示图像find_scaled_shape_model (Image1, ModelID, rad(-45), rad(90), 0.8, 1.1, 0.5, 0, 0.5, 'least_squares', 5, 0.8, Row2, Column2, Angle, Scale, Score)//模板匹配for I := 0 to |Score| - 1 by 1 *生成一个单位矩阵 hom_mat2d_identity (HomMat2DIdentity) *平移 hom_mat2d_translate (HomMat2DIdentity, Row2[I], Column2[I], HomMat2DTranslate) *旋转 hom_mat2d_rotate (HomMat2DTranslate, Angle[I], Row2[I], Column2[I], HomMat2DRotate) *放缩，得到最终的仿射变换矩阵 hom_mat2d_scale (HomMat2DRotate, Scale[I], Scale[I], Row2[I], Column2[I], HomMat2DScale) *仿射变换 affine_trans_contour_xld (ModelContours, ModelTrans, HomMat2DScale) dev_display (ModelTrans)endfor &emsp;&emsp;最终显示的模板匹配结果为： 其他形态学 膨胀：通过结构元素（正方形，圆形等），对原图像进行对应法则处理，增加像素 腐蚀：减少像素 开运算：先腐蚀后膨胀，减少像素 闭运算：先膨胀后腐蚀，增加像素 注： 膨胀&gt;闭运算，腐蚀&gt;开运算 对二值化图像：腐蚀，膨胀等：改变形状；对灰度图像：腐蚀：变暗，膨胀：变亮，开闭运算同理。 Bloab分析&emsp;&emsp;二值化-&gt;形态学-&gt;特征选择 预处理 中值滤波：求中间值，消除过亮或过暗的点，即消除椒盐噪声； 均值滤波：求均值 高斯滤波：加权值 参考博客 基于HALCON的模板匹配方法总结 Halcon 模板匹配参数详解 halcon第十四讲：基于形状的模板匹配","categories":[{"name":"工业机器人视觉","slug":"工业机器人视觉","permalink":"http://cxx0822.github.io/categories/工业机器人视觉/"}],"tags":[]},{"title":"Cartographer的原理与应用","slug":"Cartographer的原理与应用","date":"2019-11-11T05:01:32.000Z","updated":"2020-01-09T07:11:16.945Z","comments":true,"path":"2019/11/11/Cartographer的原理与应用/","link":"","permalink":"http://cxx0822.github.io/2019/11/11/Cartographer的原理与应用/","excerpt":"","text":"平台&emsp;&emsp;Ubuntu 16.04&emsp;&emsp;ROS kinect&emsp;&emsp;Cartographer Cartographer的安装安装依赖库protobuf3&emsp;&emsp;注：这里需要安装3.6以上的版本。1234567891011sudo apt-get install autoconf autogengit clone https://github.com/protocolbuffers/protobuf.gitcd protobufgit submodule update --init --recursive./autogen.sh./configuremake# 这一步可能会报错，无视就好make checksudo make installsudo ldconfig &emsp;&emsp;注：需要分开依次执行，下同。&emsp;&emsp;检查安装的版本号：1protoc --version 安装Cartographer&emsp;&emsp;安装wstool,rosdep,ninja工具：12sudo apt-get updatesudo apt-get install python-wstool python-rosdep ninja-build &emsp;&emsp;创建工作空间carto_ws(可以是其他名字)，并初始化。123mkdir carto_wscd carto_wswstool init src 12wstool merge -t src https://raw.githubusercontent.com/googlecartographer/cartographer_ros/master/cartographer_ros.rosinstallwstool update -t src &emsp;&emsp;如果报错或长时间未安装，则更改下载地址：cd src/，gedit .rosinstall。 &emsp;&emsp;将最后一个git来源网址由https://ceres-solver.googlesource.com/ceres-solver.git改为https://github.com/ceres-solver/ceres-solver.git。 &emsp;&emsp;回到工作空间，重新执行：1wstool update -t src 123sudo rosdep init # 报错可忽视rosdep update # 报错则重新执行(网络原因)rosdep install --from-paths src --ignore-src --rosdistro=$&#123;ROS_DISTRO&#125; -y &emsp;&emsp;注：如果报错，未设置环境变量，则执行以下命令：12echo \"source /opt/ros/kinetic/setup.bash\" &gt;&gt; ~/.bashrcsource ~/.bashrc &emsp;&emsp;重新执行：1rosdep install --from-paths src --ignore-src --rosdistro=$&#123;ROS_DISTRO&#125; -y &emsp;&emsp;最终出现：#All required rosdeps installed successfully。 编译12catkin_make_isolated --install --use-ninjasource install_isolated/setup.bash &emsp;&emsp;注：如果这里报错，检查一下protoc的版本，至少是3.6的，如果不是，按照上文重新安装protoc依赖库。 测试1234567891011# Download the 2D backpack example bag.wget -P ~/Downloads https://storage.googleapis.com/cartographer-public-data/bags/backpack_2d/cartographer_paper_deutsches_museum.bag # Launch the 2D backpack demo.roslaunch cartographer_ros demo_backpack_2d.launch bag_filename:=$&#123;HOME&#125;/Downloads/cartographer_paper_deutsches_museum.bag # Download the 3D backpack example bag.wget -P ~/Downloads https://storage.googleapis.com/cartographer-public-data/bags/backpack_3d/with_intensities/b3-2016-04-05-14-14-00.bag # Launch the 3D backpack demo.roslaunch cartographer_ros demo_backpack_3d.launch bag_filename:=$&#123;HOME&#125;/Downloads/b3-2016-04-05-14-14-00.bag &emsp;&emsp;测试前，需打开roscore节点管理器，如果报错，检查下是否刷新了环境变量(source ~/carto_ws/install_isolated/setup.bash)。 参考博客&emsp;&emsp;博客1，博客2","categories":[{"name":"ROS开发与应用","slug":"ROS开发与应用","permalink":"http://cxx0822.github.io/categories/ROS开发与应用/"}],"tags":[]},{"title":"基于ROS的视觉SLAM","slug":"基于ROS的视觉SLAM","date":"2019-10-24T01:13:20.000Z","updated":"2020-01-09T07:12:15.694Z","comments":true,"path":"2019/10/24/基于ROS的视觉SLAM/","link":"","permalink":"http://cxx0822.github.io/2019/10/24/基于ROS的视觉SLAM/","excerpt":"","text":"平台&emsp;&emsp;Ubuntu16.04(两台计算机)&emsp;&emsp;ROS Kinetic Kame&emsp;&emsp;turtlebot2&emsp;&emsp;Kinectv2 硬件安装 打开电源打开turtlebot2机器人的电源开关(确定电池有电)，此时底座的状态指示灯status常亮绿灯。 连接笔记本用usb转接线将底座的usb接口和电脑的笔记本usb接口接上。 连接kinectv2kinect的一端接turtlebot底座的12V 5A的接口，另一端接笔记本的usb接口。 软件安装——turtlebot更新软件源1sudo apt-get update 安装包文件(自动安装)12sudo apt-get install ros-kinetic-turtlebot ros-kinetic-turtlebot-apps ros-kinetic-turtlebot-interactions 12sudo apt-get install ros-kinetic-kobuki-ftdi ros-kinetic-ar-track-alvar-msgs ros-kinetic-turtlebot-simulator 安装包文件(手动安装)&emsp;&emsp;注：这一步可以先跳过，如果测试不成功再执行。&emsp;&emsp;先安装依赖库 12sudo apt-get install pyqt4-dev-toolssudo apt-get install pyqt5-dev-tools &emsp;&emsp;创建src文件夹，下载源码 123cd ~/catkin_ws/srcgit clone https://github.com/robotics-in-concert/rocon_qt_gui.gitgit clone https://github.com/turtlebot/turtlebot_simulation.git &emsp;&emsp;在catkin_ws目录下编译：catkin_make。 测试连接turtlebot2和计算机，输入测试命令：1ls /dev/kobuki &emsp;&emsp;打开turtlebot开关，输入命令：1roslaunch turtlebot_bringup minimal.launch &emsp;&emsp;运行后听到响声即表示连接成功。 参考博客&emsp;&emsp;网址 软件安装——kinetcv2安装包文件12345sudo apt-get install build-essential cmake pkg-config sudo apt-get install libusb-1.0-0-dev sudo apt-get install libturbojpeg libjpeg-turbo8-dev sudo apt-get install libglfw3-dev sudo apt-get install libopenni2-dev &emsp;&emsp;注：依次运行，下同。 安装依赖库 libfreenect2123456git clone https://github.com/OpenKinect/libfreenect2.git cd libfreenect2 mkdir build &amp;&amp; cd build cmake .. -DCMAKE_INSTALL_PREFIX=$HOME/freenect2 make make install 安装源文件 iai-kinect2123456cd ~/catkin_ws/src/ git clone https://github.com/code-iai/iai_kinect2.git cd iai_kinect2 rosdep install -r --from-paths . cd ~/catkin_ws catkin_make -DCMAKE_BUILD_TYPE=\"Release\" 刷新环境变量12gedit ~/.bashrcsource ~/catkin_ws/devel/setup.bash 运行节点123roscore roslaunch kinect2_bridge kinect2_bridge.launch rosrun kinect2_viewer kinect2_viewer &emsp;&emsp;注：分3个终端打开。 参考博客&emsp;&emsp;网址","categories":[{"name":"ROS开发与应用","slug":"ROS开发与应用","permalink":"http://cxx0822.github.io/categories/ROS开发与应用/"}],"tags":[]},{"title":"ROS的安装及简单使用","slug":"ROS的安装及简单使用","date":"2019-10-23T02:11:49.000Z","updated":"2020-03-20T09:35:36.654Z","comments":true,"path":"2019/10/23/ROS的安装及简单使用/","link":"","permalink":"http://cxx0822.github.io/2019/10/23/ROS的安装及简单使用/","excerpt":"","text":"平台&emsp;&emsp;Ubuntu 16.04 ROS简介&emsp;&emsp;ROS即机器人操作系统(Robot Operating System)，ROS系统是起源于2007年斯坦福大学人工智能实验室的项目与机器人技术公司Willow Garage的个人机器人项目之间的合作，2008年之后就由Willow Garage来进行推动。2010年Willow Garage公司发布了开源机器人操作系统ROS。&emsp;&emsp;ROS是面向机器人的开源的元操作系统(meta-operatingsystem)。它能够提供类似传统操作系统的诸多功能，如硬件抽象、底层设备控制、常用功能实现、进程间消息传递和程序包管理等。此外，它还提供相关工具和库，用于获取、编译、编辑代码以及在多个计算机之间运行程序完成分布式计算。&emsp;&emsp;目前主流的ROS版本如下：&emsp;&emsp;本博客采用的是Kinetic Kame版本，也是目前为止(2019年)较为稳定的一个版本。 ROS安装安装 Ubuntu 16.04&emsp;&emsp;首先安装好Ubuntu 16.04。(中英文皆可) 设置 sources.list&emsp;&emsp;为Ubuntu的包管理器增加源，设置计算机接受来自于 packages.ros.org的软件。1sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" &gt; /etc/apt/sources.list.d/ros-latest.list' &emsp;&emsp;这一步会根据Ubuntu Linux发行版本的不同，添加不同的源。Ubuntu的版本通过lsb_release -sc获得。一旦添加了正确的软件库，操作系统就知道去哪里下载程序，并根据命令自动安装软件。 设置密钥&emsp;&emsp;这一步是为了确认源代码是正确的，并且没有人在未经所有者授权的情况下，修改任何程序代码。通常情况下，当添加完软件库时，已经添加了软件库的密钥，并将其添加到操作系统的可信任列表中。1sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654 检查软件更新&emsp;&emsp;确保包管理器的索引已经更新至最新。1sudo apt-get update 正式安装&emsp;&emsp;这里安装的是桌面完整版，也是推荐安装的版本。1sudo apt-get install ros-kinetic-desktop-full &emsp;&emsp;这一步比较耗时，耐心等待。 初始化 rosdep&emsp;&emsp;在使用ROS之前，需要先初始化rosdep。rosdep使得你可以为你想要编译的源码，以及需要运行的 ROS 核心组件，简单地安装系统依赖。12sudo rosdep initrosdep update &emsp;&emsp;注：多行命令，需依次输入，下同。 环境设置&emsp;&emsp;如果在每次一个新的终端启动时，ROS环境变量都能自动地添加进你的bash会话是非常方便，这可以通过如下命令来实现：12echo \"source /opt/ros/kinetic/setup.bash\" &gt;&gt; ~/.bashrcsource ~/.bashrc 构建包所需的依赖&emsp;&emsp;到这一步，应该已经安装好了运行核心ROS包的所有东西。要创建和管理你自己的ROS workspace，还有单独发布的许多的工具。比如，rosinstall是一个常用的命令行工具，使你可以通过一个命令为ROS包简单地下载许多源码树。1sudo apt-get install python-rosinstall python-rosinstall-generator python-wstool build-essential &emsp;&emsp;至此，已经安装完了ROS的所有部分，下面可以进行一个简单的测试。 测试启动ROS核心管理器&emsp;&emsp;打开一个终端，输入以下命令：1roscore 启动乌龟仿真器节点&emsp;&emsp;再打开一个终端，输入以下命令：1rosrun turtlesim turtlesim_node 启动控制乌龟操作节点&emsp;&emsp;再打开一个终端，输入以下命令：1rosrun turtlesim turtle_teleop_key &emsp;&emsp;点击该终端，使其处于激活状态，然后按下键盘的方向键就可以看到小乌龟的运动了。 ROS实例——两台计算机的通信硬件平台&emsp;&emsp;两台安装了Ubuntu16.04和ROS kinetic的计算机&emsp;&emsp;路由器 软件配置安装chrony包，用于实现同步123sudo apt-get install chronysudo apt-get install ntpdatesudo ntpdate ntp.ubuntu.com 安装SSH服务器库12sudo apt-get install openssh-serverps -e|grep ssh 修改配置文件查看hostname和IP地址&emsp;&emsp;首先利用hostname命令查看服务器和客户端电脑的名字，假设为server_name和client_name。&emsp;&emsp;其次利用ifconfig命令查看服务器和客户端的IP地址(连接至同一局域网)，假设IP地址分别为：server_IP和client_IP(wlp4s0的网络地址)。 修改/etc/hosts文件和环境变量1.打开/etc/hosts文件：sudo gedit /etc/hosts，添加如下内容：12server_IP server_nameclient_IP client_name &emsp;&emsp;注：中间为tab键，实际操作时，注意替换名字。&emsp;&emsp;输入命令重启网络：sudo /etc/init.d/networking restart 2.打开bash文件：sudo gedit ~/.bashrc，服务器的计算机增加的内容为：12export ROS_MASTER_URL=http://client_IP:11311export ROS_HOSTNAME=server_name &emsp;&emsp;客户端的计算机增加的内容为：12export ROS_MASTER_URL=http://server_IP:11311export ROS_HOSTNAME=client_name 3.在服务器的计算机中可以输入以下内容进行测试：12ssh server_nameping client_name &emsp;&emsp;同理，客户端也可以进行测试。 编写程序创建文件夹 创建工作空间 123mkdir -p ~/catkin_ws/srccd ~/catkin_ws/srccatkin_init_workspace 创建功能包 1catkin_create_pkg test roscpp geometry_msgs tf &emsp;&emsp;注：这里的test是功能包的名字，roscpp,geometry_msgs,tf是需要的依赖库。 编写cpp文件&emsp;&emsp;在test/src里面新建一个move_turtle.cpp文件。其内容如下：123456789101112131415161718192021222324252627282930313233#include &lt;ros/ros.h&gt;#include &lt;signal.h&gt;#include &lt;geometry_msgs/Twist.h&gt;ros::Publisher cmdVelPub;void shutdown(int sig)&#123; cmdVelPub.publish(geometry_msgs::Twist());//使机器人停止运动 ROS_INFO(\"move_turtle_goforward ended!\"); ros::shutdown();&#125;int main(int argc, char** argv)&#123; ros::init(argc, argv, \"move_turtle\"); //初始化ROS,它允许ROS通过命令行进行名称重映射 ros::NodeHandle node;//为这个进程的节点创建一个句柄 cmdVelPub = node.advertise&lt;geometry_msgs::Twist&gt;(/mobile_base/commands/velocity, 1); //在/mobile_base/commands/velocity topic上发布一个geometry_msgs/Twist的消息 ros::Rate loopRate(10);//ros::Rate对象可以允许你指定自循环的频率 signal(SIGINT, shutdown); ROS_INFO(\"move_turtle cpp start...\"); geometry_msgs::Twist speed; // 控制信号载体 Twist message while (ros::ok()) &#123; speed.linear.x = 0.1; // 设置线速度为0.1m/s，正为前进，负为后退 speed.angular.z = 0; // 设置角速度为0rad/s，正为左转，负为右转 cmdVelPub.publish(speed); // 将刚才设置的指令发送给机器人 loopRate.sleep();//休眠直到一个频率周期的时间 &#125; return 0;&#125; &emsp;&emsp;该程序的作用是：使turtlebot一直以0.1m/s的速度前进。 修改CMakeLists.txt&emsp;&emsp;在test功能包中的CMakeList.txt文件的末尾加上两句：12add_executable(move_turtle src/move_turtle.cpp) target_link_libraries(move_turtle $&#123;catkin_LIBRARIES&#125;) 编译工程&emsp;&emsp;回到工作空间1cd ~/catkin_ws/ &emsp;&emsp;编译1catkin_make &emsp;&emsp;设置环境变量1source devel/setup.bash &emsp;&emsp;附：将工作空间永久添加到环境变量中：12echo \"source ~/catkin_ws/devel/setup.bash\" &gt;&gt; ~/.bashrcsource ~/.bashrc 启动turtlebot 打开turtlebot电源，并使其和客户端计算机连接，将客户端计算机放在turtlebot上。 在客户端计算机中，打开节点管理器(打开终端输入)：roscore，再打开一个终端启动节点：roslaunch kobuki_node minimal.launch。 在服务器计算机中也打开节点管理器，再打开一个终端，启动刚才的工程：rosrun test move_turtle。 此时就会看到turtlebot一直在直线运行，直到按下ctrl + c。 附：ROS官网教程","categories":[{"name":"ROS开发与应用","slug":"ROS开发与应用","permalink":"http://cxx0822.github.io/categories/ROS开发与应用/"}],"tags":[]},{"title":"NAO_GolfVision_ML_C++使用说明","slug":"NAO-GolfVision-ML-C-使用说明","date":"2019-08-29T10:05:35.000Z","updated":"2019-09-03T13:31:31.620Z","comments":true,"path":"2019/08/29/NAO-GolfVision-ML-C-使用说明/","link":"","permalink":"http://cxx0822.github.io/2019/08/29/NAO-GolfVision-ML-C-使用说明/","excerpt":"","text":"平台&emsp;&emsp;Windows 10&emsp;&emsp;Visual studio 2015 概述&emsp;&emsp;本博客为我的Github上项目的说明文档。 添加文件至项目工程&emsp;&emsp;打开VS2015，并新建工程文件夹，然后将GitHub上的所有文件放至Project1-&gt;Project1文件夹中。&emsp;&emsp;然后依次将.h和.cpp文件分别添加头文件和源文件中。&emsp;&emsp;最后我们重新生成编译方案，检查代码是否正确。&emsp;&emsp;注：编译前，一定要确保Opencv3已经配置正确，且切换到Debug ×64模式。 生成特征向量文件&emsp;&emsp;打开主函数，打开计算正样本函数train.cal_pos_vector();，并关闭其余函数，运行开始调试(不执行)。注意将属性-&gt;链接器-&gt;系统中的子系统设置为控制台。12345678910111213141516#include \"classifierTrain.h\"int main()&#123; clock_t start, end; start = clock(); ClassifierTrain train; train.cal_pos_vector(\"img_train_pos/*.jpg\", \"label_train_pos/*.xml\", \"data_1.txt\"); //train.cal_neg_vector(\"img_train_neg/*.jpg\", \"data_2.txt\"); //train.result_test(\"img_test/*.jpg\"); end = clock(); double endtime = (double)(end - start) / CLOCKS_PER_SEC; cout &lt;&lt; \"Total time:\" &lt;&lt; endtime &lt;&lt; endl; //s为单位 return 0;&#125; &emsp;&emsp;在Project1文件夹中会生成data_1.txt文件。 &emsp;&emsp;然后打开计算负样本函数train.cal_neg_vector();，并关闭其余函数，运行开始调试(不执行)。&emsp;&emsp;在Project1文件夹中会生成data_2.txt文件。 配置训练数据&emsp;&emsp;打开Project1文件夹中的data.xml文件，将data_1.txt和data_2.txt中的数据放入到&lt;datamat&gt;中，并更改其中的&lt;rows&gt;和&lt;cols&gt;(根据实际情况而定)。并配置好&lt;labelsmat&gt;中的信息，即行列数一致，其中1的个数为data_1.txt中的行数，0的个数为data_2.txt中的行数。 预测分析&emsp;&emsp;打开测试函数result_test();，并关闭其余函数，运行开始调试(不执行)。&emsp;&emsp;总体上和之前python的基本一致。","categories":[{"name":"NAO高尔夫比赛","slug":"NAO高尔夫比赛","permalink":"http://cxx0822.github.io/categories/NAO高尔夫比赛/"}],"tags":[]},{"title":"PyTorch的简单使用","slug":"PyTorch的简单使用","date":"2019-08-18T01:56:38.000Z","updated":"2020-01-09T07:19:17.290Z","comments":true,"path":"2019/08/18/PyTorch的简单使用/","link":"","permalink":"http://cxx0822.github.io/2019/08/18/PyTorch的简单使用/","excerpt":"","text":"概述&emsp;&emsp;2017年1月，Facebook人工智能研究院(FAIR)团队在GitHub上开源了PyTorch，并迅速占领GitHub热度榜榜首。&emsp;&emsp;本博客为我的Github上的项目的使用说明。该项目主要是利用PyTorch深度学习框架进行简单的目标分类。 安装方法一——官网&emsp;&emsp;登陆PyTorch的官网，在下面找到相应的版本，执行命令即可(windows的安装命令可能是pip)：&emsp;&emsp;但实际测试发现，这种方式下载的很慢，我们可以用浏览器打开后面的网站，然后在网站中找到需要的版本，在点击下载即可。(可以用IDM下载器)然后使用命令pip install .\\torch-1.2.0-cp35-cp35m-win_amd64.whl和pip install .\\torchvision-0.4.0-cp35-cp35m-win_amd64.whl即可。 方法二——Anacoda+清华源&emsp;&emsp;这个也是大部分人的安装方法，网上有很多博客介绍，读者可以自行百度学习。 数据集&emsp;&emsp;对于目标分类的数据集，其文件夹目录如下所示：12345678910111213dataSet train 1 2 3 valid 1 2 3 test 1 2 3 &emsp;&emsp;首先新建3个文件夹，即train，valid和test。然后再分别新建类别文件夹，这里的1,2,3指的是3个类别，最后将图片按类别放入各自的文件夹中。 读取数据集&emsp;&emsp;load_data.py12345678910111213141516# -*-coding:utf-8-*-import torchimport torchvisionfrom torchvision import datasets, transformsimport osimport os.pathdef load_data(data_dir, image_size, batch_size): data_transform = transforms.Compose([transforms.Resize([image_size, image_size]), transforms.ToTensor()]) image_datasets = datasets.ImageFolder(root=os.path.join(data_dir), transform=data_transform) dataloader = torch.utils.data.DataLoader(dataset=image_datasets, batch_size=batch_size, shuffle=True) return dataloader &emsp;&emsp;PyTorch常用的读取图片数据集的函数为ImageFolder()，但使用该函数前必须保证图片已经按照刚才的文件夹形式存放好。&emsp;&emsp;该函数的函数声明如下：1ImageFolder(root,transform=None,target_transform=None,loader=default_loader) &emsp;&emsp;其中，root是在指定的root路径下面寻找图片，transform是对PIL Image图片进行转换操作，target_transform是对label进行变换，loader是指定加载图片的函数，默认操作是读取PIL image对象。&emsp;&emsp;这里我们需要对原始数据集进行简单的处理，即统一大小并转换为Tensor变量。也就是transforms.Resize()和transforms.ToTensor()两个函数。transforms.Compose()指的是将多个操作合并在一起。(当然这里也可以增加其他的图片处理函数)&emsp;&emsp;最后我们用DataLoader()读取数据集。其函数声明为：1DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0) &emsp;&emsp;其中dataset是传入的数据集，batch_size是每个batch有多少个样本，shuffle是在每个epoch开始的时候，对数据进行重新排序，sampler是自定义从数据集中取样本的策略，如果指定这个参数，那么shuffle必须为False，batch_sampler与sampler类似，但是一次只返回一个batch的indices(索引)，num_workers这个参数决定了有几个进程来处理data loading。0意味着所有的数据都会被load进主进程。(默认为0)&emsp;&emsp;这里的dataset就是之前ImageFolder()的返回值，这里我们选择shuffle为True，其他除了batch_size均为默认值。&emsp;&emsp;dataloader本质是一个可迭代对象，使用iter()访问，不能使用next()访问；使用iter(dataloader)返回的是一个迭代器，然后可以使用next访问。&emsp;&emsp;我们可以使用下面的测试函数简单看一下数据集。123456789if __name__ == \"__main__\": data_dir_train = \"dataSet/train\" train_data = load_data(data_dir_train, 224, 4) images, labels = next(iter(train_data)) img = torchvision.utils.make_grid(images) img = img.numpy().transpose(1, 2, 0) import matplotlib.pyplot as plt plt.imshow(img) plt.show() &emsp;&emsp;显示结果为： 搭建网络模型&emsp;&emsp;model.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# -*-coding:utf-8-*-import torchimport torch.nn.functional as Fclass Models(torch.nn.Module): def __init__(self, image_dim, n_classes): super(Models, self).__init__() self.image_dim = image_dim self.n_classes = n_classes self.Conv = torch.nn.Sequential( torch.nn.Conv2d(in_channels=self.image_dim, out_channels=64, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.MaxPool2d(kernel_size=2, stride=2), torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.MaxPool2d(kernel_size=2, stride=2), torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.MaxPool2d(kernel_size=2, stride=2), torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.MaxPool2d(kernel_size=2, stride=2), ) self.Classes = torch.nn.Sequential( torch.nn.Linear(in_features=14 * 14 * 512, out_features=256), # 14 * 14 * 512是根据图像的输入和网络结构算出来的 torch.nn.ReLU(), torch.nn.Dropout(p=0.5), torch.nn.Linear(in_features=256, out_features=self.n_classes) ) def forward(self, input): x = self.Conv(input) x = x.view(-1, 14 * 14 * 512) x = self.Classes(x) return x &emsp;&emsp;在Pytorch中，搭建模型的常用做法是新建一个类，并继承torch.nn.Module这个类，然后在__init__()方法中定义自己的卷积、池化和全连接层等。这里比较简单，熟悉深度学习CNN网络结构的都能看懂。唯一和其他框架不同的是，这里的全连接层的第一个参数，需要自己计算，即代码中的self.Classes中的第一个in_features=14 * 14 * 512，这个是根据输入图片的大小，这里为224*224，然后经过一系列网络结构后得到的输出Tensor后的大小。读者有兴趣可以自己计算一遍，如果更改了图片的大小也需要重新计算。&emsp;&emsp;附：计算公式 \\frac{n+2p-f}{s}+1训练模型&emsp;&emsp;train.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# -*-coding:utf-8-*-import torchfrom torch.autograd import Variablefrom tensorboardX import SummaryWriterfrom model import Modelsfrom load_data import * import yamlwith open(\"info.yml\") as stream: my_data = yaml.load(stream, Loader=yaml.FullLoader) # python3.6 可能需要去掉 Loader=yaml.FullLoaderdata_dir_train = my_data['data_dir_train']data_dir_valid = my_data['data_dir_valid']image_dim = my_data['image_dim'] n_classes = my_data['n_classes'] image_size = my_data['image_size']batch_size = my_data['batch_size']learning_rate = my_data['learning_rate']epochs = my_data['epochs']writer = SummaryWriter()def my_train(): use_gpu = torch.cuda.is_available() dataloader = &#123;\"train\": load_data(data_dir_train, image_size, batch_size), \"valid\": load_data(data_dir_valid, image_size, batch_size)&#125; model = Models(image_dim, n_classes) loss_f = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) if use_gpu: model = model.cuda() for epoch in range(epochs): print(\"Epoch &#123;&#125;/&#123;&#125;\".format(epoch + 1, epochs)) print(\"-\" * 10) running_loss = 0.0 train_correct = 0 train_total = 0 for i, data in enumerate(dataloader[\"train\"], 0): inputs, train_labels = data if use_gpu: inputs, labels = Variable(inputs.cuda()), Variable(train_labels.cuda()) else: inputs, labels = Variable(inputs), Variable(train_labels) optimizer.zero_grad() outputs = model(inputs) _, train_predicted = torch.max(outputs.data, 1) train_correct += (train_predicted == labels.data).sum() loss = loss_f(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() train_total += train_labels.size(0) print('train %d epoch loss: %.3f acc: %.3f ' % (epoch + 1, running_loss / train_total, 100 * train_correct / train_total)) writer.add_scalar('Train/Loss', running_loss / train_total, epoch + 1) writer.add_scalar('Train/Acc', 100 * train_correct / train_total, epoch + 1) # 模型测试 correct = 0 test_loss = 0.0 test_total = 0 test_total = 0 model.eval() for data in dataloader[\"valid\"]: images, labels = data if use_gpu: images, labels = Variable(images.cuda()), Variable(labels.cuda()) else: images, labels = Variable(images), Variable(labels) outputs = model(images) _, predicted = torch.max(outputs.data, 1) loss = loss_f(outputs, labels) test_loss += loss.item() test_total += labels.size(0) correct += (predicted == labels.data).sum() print('test %d epoch loss: %.3f acc: %.3f ' % (epoch + 1, test_loss / test_total, 100 * correct / test_total)) writer.add_scalar('Test/Loss', test_loss / test_total, epoch + 1) writer.add_scalar('Test/Acc', 100 * correct / test_total, epoch + 1) writer.close() torch.save(model, 'model.pt')if __name__ == \"__main__\": my_train() &emsp;&emsp;首先从配置文件中导入一些必要的参数信息。在正式训练前，可以先检查一下是否装了GPU加速，即torch.cuda.is_available()。为了更好的显示效果，可以安装tensorboardX图形化显示工具。先定义一个writer，即writer = SummaryWriter()。下面正式进入训练。&emsp;&emsp;训练分为训练集和测试集，并分别计算其正确率和Loss值。然后打印每次迭代的相应信息，并将正确率和Loss写入tensorboardX的scalar中，最后关闭writer并保存模型。&emsp;&emsp;训练过程：&emsp;&emsp;训练结束后，会在文件夹内生成model.pt文件和runs文件夹。其中model.pt就是模型文件，runs文件夹下会有一个以时间命名的文件夹，里面存放的就是tensorboardX生成的日志文件。cd到日志文件的上一层文件夹下，并输入tensorboard查看指令：tensorboard --logdir=***(***就是日志文件的上一层文件夹，不需要引号)。然后将生成的网址放入浏览器中就可以看到可视化的结果了(可能需要IE浏览器)。 预测&emsp;&emsp;predict.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import torchfrom torch.autograd import Variablefrom model import Modelsfrom load_data import * import matplotlib.pyplot as pltimport yamlwith open(\"info.yml\") as stream: my_data = yaml.load(stream, Loader=yaml.FullLoader)data_dir_test = my_data['data_dir_test']data_dir_valid = my_data['data_dir_valid']image_dim = my_data['image_dim'] n_classes = my_data['n_classes'] image_size = my_data['image_size']batch_size = 4def my_predict(): use_gpu = torch.cuda.is_available() test_data = load_data(data_dir_test, image_size=image_size, batch_size=batch_size) X_test, y_test = next(iter(test_data)) model = torch.load('model.pt') if use_gpu: model = model.cuda() if use_gpu: images = Variable(X_test.cuda()) else: images = Variable(X_test) outputs = model(images) _, predicted = torch.max(outputs.data, 1) print(\"Predict Label is: \", predicted.data) print(\"Real Label is :\", y_test.data) img = torchvision.utils.make_grid(X_test) img = img.numpy().transpose([1, 2, 0]) # 转成numpy在转置 plt.imshow(img) plt.show()if __name__ == \"__main__\": my_predict() &emsp;&emsp;首先利用load()函数导入模型，然后再将测试图片放入模型中得到预测结果。并将预测标签和实际标签分别打印出来。如图所示：&emsp;&emsp;实际测试下来发现，检测效果还是不错的，只有个别几个会判断错误。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://cxx0822.github.io/categories/深度学习/"}],"tags":[]},{"title":"支持向量机的原理及python实现","slug":"支持向量机的原理及python实现","date":"2019-08-11T11:21:14.000Z","updated":"2020-01-09T07:22:57.890Z","comments":true,"path":"2019/08/11/支持向量机的原理及python实现/","link":"","permalink":"http://cxx0822.github.io/2019/08/11/支持向量机的原理及python实现/","excerpt":"","text":"概述&emsp;&emsp;支持向量机(support vector machine，一般称为SVM。是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。&emsp;&emsp;因为其较低的错误率，使其被认为是机器学习中目前为止最好的分类器。 基本概念&emsp;&emsp;上图一共有四个部分，A为原始数据集，一共有两类，分别在左边和右边，B，C，D分别给出了一条可以将两类分开的直线。这条直线被称为分隔超平面。如果是在二维平面上，这个超平面就是一条直线，如果是三维的，就是一个平面。如果数据集是N维的话，就需要一个N-1维的对其分隔，也就是分类的决策边界。&emsp;&emsp;对于上图的三种决策边界，我们明显认为D的决策边界是最好的，因为其距离两类的间距都是最大的。也就是说，我们希望找到离分隔超平面最近的点，确保它们离分隔面的距离尽可能地远。这里点到分割面地距离被称为间隔。我们希望间隔尽可能地大。&emsp;&emsp;而所谓的支持向量就是离分隔超平面最近的那些点，机的意思就是决策边界。所以接下来的任务就是最大化支持向量到分隔面的距离。 任务——寻找最大间隔&emsp;&emsp;首先我们要确定点到决策边界的距离，也就是所谓的函数间隔(可能有的书这里指的是几何间隔)。&emsp;&emsp;分隔超平面的直线可以记为$y=\\omega ^Tx + b$(这里的$\\omega$和$x$都是一个向量，$b$类似于截距)，然后计算点A到分隔面的法线或垂线的长度，利用点到平面的公式可以得到其函数间隔为： d=\\frac{\\left | \\omega ^{T}x+b \\right |}{\\left \\| \\omega \\right \\|}&emsp;&emsp;所以我们对一个数据点进行分类时，当超平面离数据点的“间隔”越大，分类的置信度也越大。为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化这个“间隔”值。&emsp;&emsp;但是只有这个间隔也不够，因为我们有两类数据，所以还需要再定义类别标签的概念。&emsp;&emsp;这里的类别的定义和之前的机器学习算法不同，使用的是-1和+1两种标签，即： g\\left ( z \\right )=g\\left ( \\omega ^{T}x+b \\right )=\\left\\{\\begin{matrix} -1 & z\\geqslant 0\\\\ 1 & z < 0 \\end{matrix}\\right.&emsp;&emsp;为什么这样定义呢？因为之前我们定义的间隔公式中含有绝对值项$\\left | \\omega ^{T}x+b \\right |$，这对后面的计算来说不太方便，而现在我们可以将其替换为$lable\\left ( \\omega ^{T}x+b \\right )$，因为这是一个恒正的表达式。如果数据点处于正方向(+1类)，并且离分隔面很远的位置时，$\\left ( \\omega ^{T}x+b \\right )$会是一个很大的正数，同时$lable\\left ( \\omega ^{T}x+b \\right )$也是一个很大的正数。反之，如果在负方向，其最终的值也是一个正数(负负得正)。而且标签值为+1和-1，也不影响最终的间距值。&emsp;&emsp;综上所述，我们最终的目标任务可以用以下的数学表达式表示： arg \\underset{\\omega,b}{max}\\left \\{ \\underset{n}{min}\\left ( label\\cdot \\left ( \\omega ^{T}x+b \\right ) \\right ) \\cdot \\frac{1}{\\left \\| \\omega \\right \\|}\\right \\}&emsp;&emsp;公式前面的$argmax$表示的是间隔最大值时，参数$\\omega,b$的取值。$min$表示的是具有最小间隔的数据点。&emsp;&emsp;但是这里面都是乘积项，对其求最值比较困难，如果能固定某个乘积项，只求其中一个的最值就会简单很多，而经过之前的分析，$lable*\\left ( \\omega ^{T}x+b \\right )$项都是大于0的，在准确地说其值是$\\geqslant 1$的(如下图所示)。那么其最小值为1，就可以去掉该项。&emsp;&emsp;所以我们就将此问题转换为了约束条件下的极值问题，即&emsp;&emsp;目标函数为： max\\frac{1}{\\left \\| \\omega \\right \\|}&emsp;&emsp;约束条件为： s.t. \\quad \\quad y_i*\\left ( \\omega ^{T}x_i+b \\right) \\geqslant 1,i=1,2,...,n注：这里的$label$替换为了$y_i$。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://cxx0822.github.io/categories/机器学习/"}],"tags":[]},{"title":"NAO_GolfVision_ML使用说明","slug":"NAO-GolfVision-ML使用说明","date":"2019-08-05T01:50:24.000Z","updated":"2019-08-11T09:41:40.222Z","comments":true,"path":"2019/08/05/NAO-GolfVision-ML使用说明/","link":"","permalink":"http://cxx0822.github.io/2019/08/05/NAO-GolfVision-ML使用说明/","excerpt":"","text":"概述&emsp;&emsp;本博客是我的Github上的NAO_GolfVision_ML项目的使用说明。该项目是NAO高尔夫比赛中的视觉系统设计，主要是利用opencv和机器学习算法对视觉系统中的目标进行分类检测。详细的代码解释见另一篇博客：NAO比赛视觉系统设计 平台&emsp;&emsp;Windows 10&emsp;&emsp;python2.7 32(NAO支持的版本) 文件夹结构&emsp;&emsp;该文件夹主要分为三个部分，第一部分是数据集，包含图片和标签的数据集，其中图片为NAO摄像头实际拍摄的图像，像素大小为640*480，标签为labelImg软件标注生成的xml文件，其中正负样本分开存放。&emsp;&emsp;第二部分是代码，其中TargetDetection.py和TargetFeature.py是opnecv对目标的检测和机器学习对目标的特征提取文件，Classifier,py和ClassifierTrain,py是机器学习的分类器和训练文件。其中ClassifierTrain,py是主函数文件。&emsp;&emsp;第三部分是其他文件，主要包含3张测试图片，即足球、红球和黄杆，3张数据文件，即总数据，正样本数据和负样本数据，另外包含一个renamefile.py，主要是为了统一图片和标签的名字。 TargetDetection.py&emsp;&emsp;该文件包含3个类，其中TargetDetection是基类，HoughDetection是霍夫圆检测类，ContoursDetection是轮廓检测类。读者也可以使用其他检测算法新建自己的检测类。 TargetDetection&emsp;&emsp;该类包含图像预处理函数preProcess()，滤波函数filter()和滑动条函数sliderObjectHSV()。 使用方法123456if __name__ == '__main__': Img = cv2.imread(\"stick.jpg\") # 注意替换照片 # 测试HSV滑动条函数 tarDet = TargetDetection(Img) tarDet.sliderObjectHSV(\"stick\") &emsp;&emsp;首先打开一张测试图片，实际比赛中可以将此替换为NAO拍摄的图片，然后实例化类，并调用滑动条函数。注意图片名字和类别名字要一致。 &emsp;&emsp;不断调整滑动条的参数，以得到理想效果。 HoughDetection&emsp;&emsp;该类包含霍夫圆检测函数houghDetection()，信息转换函数circle2Rect()，显示结果函数showHoughResult()和霍夫圆检测滑动条函数houghSlider()。 使用方法123456if __name__ == '__main__': Img = cv2.imread(\"redBall.jpg\") # 注意替换照片 # 测试霍夫圆检测滑动条(球类目标专用) houghDet = HoughDetection(Img) houghDet.houghSlider(\"redBall\") &emsp;&emsp;不断调整滑动条的参数，以得到理想效果。 ContoursDetection&emsp;&emsp;该类包含轮廓检测函数contoursDetection()，信息转换函数contour2Rect()，显示结果函数showContourResult()和轮廓检测滑动条函数contoursSlider()。 使用方法123456if __name__ == '__main__': Img = cv2.imread(\"stick.jpg\") # 注意替换照片 # 测试轮廓检测滑动条(黄杆专用) ContoursDet = ContoursDetection(Img) ContoursDet.contoursSlider(\"stick\") &emsp;&emsp;不断调整滑动条的参数，以得到理想效果。 TargetFeature.py&emsp;&emsp;该文件包含2个类，其中HogFeature是提取HOG特征，ColorFeature是提取颜色特征。读者也可以使用其他特征提取算法。最后统一成向量的形式即可。 使用方法&emsp;&emsp;这部分主要是结合之后的分类器训练使用。 Classifier.py&emsp;&emsp;该文件包含2个类，其中Logistic是逻辑回归分类器，KNN是K近邻分类器。读者也可以使用自己的分类器。 使用方法&emsp;&emsp;这部分主要是结合之后的分类器训练使用。 ClassifierTrain.py&emsp;&emsp;该文件包含以下函数： parseXml()：解析标注文件函数， reshapeBallRect()：重造球类目标矩形框函数， reshapeStickRect()：重造黄杆类目标矩形框函数， circle2Rect()：转换信息函数， calColorFeature()：计算颜色特征函数， calHOGFeature()：计算HOG特征函数， calPosVector()：计算正样本向量函数 calNegVector()：计算负样本向量函数 resultTest()：分类结果测试函数 使用方法&emsp;&emsp;首先将数据集正确的放入到文件夹中，越多越好，至少上百张，其次更改calPosVector()函数里面的路径位置及信息。&emsp;&emsp;其中画图部分的函数，即：12345 # cv2.rectangle(srcImg, (newInitX, newInitY), (newEndX, newEndY), (0, 0, 255), 2) # 画矩形 # cv2.imshow(\"test \" + str(i), srcImg) # cv2.waitKey(300) # cv2.destroyAllWindows() &emsp;&emsp;在实际训练时需要注释掉，否则会把标注框也认为是正样本。该部分主要是为了测试标注框是否准确，读者可以先保留该部分运行一遍，再注释后运行一遍，实际的特征向量以加了注释后的为准。 12if __name__ == '__main__': calPosVector(\"data_pos.txt\") # 计算正样本的特征向量 &emsp;&emsp;运行过程中，会显示出特征向量的总数，如果是320则是正确的，当然了如果读者自己加入了其他的特征提取，可以自己算一遍。&emsp;&emsp;最后所有的特征向量会存放在data_pos.txt的文件夹下。 &emsp;&emsp;负样本的使用方法同上。其最终的特征向量会存放在data_neg.txt文件夹下，最后将这2个txt文件的数据合并到data.txt文件中。 &emsp;&emsp;最后再调用测试分类结果函数resultTest()即可，在里面输入相应的分类器，当然也可以是自己的。&emsp;&emsp;最终结果如图所示，红色为正确的，黄色为错误的。实际测试发现，只要目标不在边界上，其正确率几乎可以达到100%，对于边界上的情况，个别几个情况分类错误，但这对比赛也并没有太大的影响。整体效果还是非常不错的，可以达到比赛的实时检测要求。 注：我这里只对足球类目标进行了检测，没用对红球和黄杆测试，读者有兴趣的话可以自己采集数据并测试一下，欢迎大家留言讨论。 附：sklearn机器学习库实现分类器&emsp;&emsp;这里提供一个强大的机器学习库sklearn来实现之前的分类器，首先需要通过pip安装(pip install -U scikit-learn)，由于NAO本身并不支持这个第三方库，所以我们如果要使用的话，需要将下载好的sklearn库上传至NAO中。&emsp;&emsp;和之前一样，还是新建一个py文件，里面可以新建若干各类，每个类实现一个分类器。 实现原理&emsp;&emsp;sklearn实现机器学习算法特别简单，大致可以分为三步，1.读取数据，2.构建分类器并训练参数，3.使用分类器预测。&emsp;&emsp;下面以Logistic回归为例，详细讲解：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import sklearnfrom sklearn.linear_model import LogisticRegressionCV, LinearRegressionfrom sklearn.model_selection import train_test_splitimport numpy as npfrom sklearn.externals import joblibclass LogisticSk(object): def __init__(self, filename): self.filename = filename def file2matrix(self): fr = open(self.filename) arrayOfLines = fr.readlines() numberOfLines = len(arrayOfLines) returnMat = np.zeros((numberOfLines, 320)) classLabelVector = [] index = 0 for line in arrayOfLines: line = line.strip() listFromLine = line.split(' ') returnMat[index, :] = listFromLine[0:320] if listFromLine[-1] == '0': classLabelVector.append(0) elif listFromLine[-1] == '1': classLabelVector.append(1) index += 1 return returnMat, classLabelVector def trainClassify(self): X, Y = self.file2matrix() X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0) lr = LogisticRegressionCV(multi_class=\"ovr\", fit_intercept=True, Cs=np.logspace(-2, 2, 20), cv=2, penalty=\"l2\", solver=\"lbfgs\", tol=0.01) lr.fit(X_train, Y_train) return lr def saveClassify(self, model, path): joblib.dump(model, path) def readClassify(self, path): return joblib.load(path) def predictClassify(self, lr, X_test): Y_predict = lr.predict(X_test) return Y_predict &emsp;&emsp;这里为了和之前的变量名不一样，在分类器后面加了SK，表示是用sklearn库实现的。 读取数据&emsp;&emsp;这部分可以参考之前的file2matrix()函数。 构建分类器并训练&emsp;&emsp;在构建分类器之前，我们可以先用train_test_split()函数将数据集分为训练集和测试集，以便后续的分析(当然也可以直接用原来的数据)。&emsp;&emsp;接下来就是使用sklearn库中自带的分类器训练，这里注意首先要将分类器所在的类导入进来，比如这里是LogisticRegressionCV()分类器函数，它是在sklearn.linear_model类中的。然后可以设置分类器的参数，当然也可以使用默认值。最后使用fit()函数训练以得到相应的参数。 保存和读取模型&emsp;&emsp;训练一次数据集通常需要花费一定的时间，这对实时性的要求显然是不利的，所以通常的做法是先将训练好的模型保存下来，然后再读取。&emsp;&emsp;sklearn中保存和读取的模块是joblib(也有其他的)，其dump()和load()函数分别是读取和保存。注：保存的模型一般后缀名为.m。 使用模型预测&emsp;&emsp;使用predict()函数预测即可，输入参数为待预测的数据。 结果分析&emsp;&emsp;实际测试下来发现，sklearn库的分类器函数和自己写的分类器的效果几乎差不多，总体效果还是比较好的。","categories":[{"name":"NAO高尔夫比赛","slug":"NAO高尔夫比赛","permalink":"http://cxx0822.github.io/categories/NAO高尔夫比赛/"}],"tags":[]},{"title":"深度学习的数据集处理","slug":"深度学习的数据集处理","date":"2019-07-26T05:46:21.000Z","updated":"2019-09-03T13:26:18.159Z","comments":true,"path":"2019/07/26/深度学习的数据集处理/","link":"","permalink":"http://cxx0822.github.io/2019/07/26/深度学习的数据集处理/","excerpt":"","text":"概述&emsp;&emsp;只是整理，不展开叙述 平台&emsp;&emsp;Ubuntu / Windows&emsp;&emsp;python3.5 / 3.6 目标分类数据集&emsp;&emsp;目标分类的数据集通常为若干个类别文件夹，然后每个文件夹存放着若干个该类别的图片。数据集通常由图片+类别标签组成的。图片是不需要标注的。 文件夹设置&emsp;&emsp;首先新建一个文件夹dataSet(当然可以取其他名字，下同)，然后新建2个文件夹test和val分别存放训练集数据和验证集数据，另外在新建1个文件夹record保存tfrecords数据格式的文件：1234dataSet record test val 获取数据集&emsp;&emsp;目标分类的数据集的大小尽可能地小一点，仅包含分类物体即可，如下面的几个图片：&emsp;&emsp;可以从网上获取或自己拍摄采集。 放入数据集&emsp;&emsp;在test和val文件夹下依次新建分类的类别文件夹，并将数据集放入各自的文件夹中。 生成数据集文本文件&emsp;&emsp;这一步主要是生成记录数据集路径的txt文件，以便后续读取方便。&emsp;&emsp;create_labels_files.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546# -*-coding:utf-8-*-import osimport os.pathdef get_files_list(dir): files_list = [] for parent, dirnames, filenames in os.walk(dir): for filename in filenames: curr_file = parent.split(os.sep)[-1] if curr_file == 'flower': labels = 0 elif curr_file == 'guitar': labels = 1 elif curr_file == 'animal': labels = 2 elif curr_file == 'houses': labels = 3 elif curr_file == 'plane': labels = 4 files_list.append([os.path.join(curr_file, filename), labels]) return files_listdef write_txt(content, filename, mode='w'): with open(filename, mode) as f: for line in content: str_line = \"\" for col, data in enumerate(line): if not col == len(line) - 1: str_line = str_line + str(data) + \" \" else: str_line = str_line + str(data) + \"\\n\" f.write(str_line)if __name__ == '__main__': train_dir = 'dataSet/train' train_txt = 'dataSet/train.txt' train_data = get_files_list(train_dir) write_txt(train_data, train_txt, mode='w') val_dir = 'dataSet/val' val_txt = 'dataSet/val.txt' val_data = get_files_list(val_dir) write_txt(val_data, val_txt, mode='w') &emsp;&emsp;生成的train.txt： 制作tfrecords数据格式&emsp;&emsp;create_tf_record.py，主要的函数为： create_records()：用于制作records数据的函数； read_records()：用于读取records数据的函数； get_batch_images():用于生成批训练数据的函数； get_example_nums：统计tf_records图像的个数(example个数)； disp_records(): 解析record文件，并显示图片，主要用于验证生成record文件是否成功。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205# -*-coding: utf-8 -*-import tensorflow as tfimport numpy as npimport osimport cv2import matplotlib.pyplot as pltimport randomfrom PIL import Imagedef _int64_feature(value): return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))def _bytes_feature(value): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))def float_list_feature(value): return tf.train.Feature(float_list=tf.train.FloatList(value=value))def get_example_nums(tf_records_filenames): nums = 0 for record in tf.python_io.tf_record_iterator(tf_records_filenames): nums += 1 return numsdef show_image(title, image): plt.imshow(image) plt.axis('on') plt.title(title) plt.show()def load_labels_file(filename, labels_num=1, shuffle=False): images = [] labels = [] with open(filename) as f: lines_list = f.readlines() if shuffle: random.shuffle(lines_list) for lines in lines_list: line = lines.rstrip().split(' ') label = [] for i in range(labels_num): label.append(int(line[i + 1])) images.append(line[0]) labels.append(label) return images, labelsdef read_image(filename, resize_height, resize_width, normalization=False): bgr_image = cv2.imread(filename) if len(bgr_image.shape) == 2: print(\"Warning:gray image\", filename) bgr_image = cv2.cvtColor(bgr_image, cv2.COLOR_GRAY2BGR) rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB) if resize_height &gt; 0 and resize_width &gt; 0: rgb_image = cv2.resize(rgb_image, (resize_width, resize_height)) rgb_image = np.asanyarray(rgb_image) if normalization: rgb_image = rgb_image / 255.0 return rgb_imagedef get_batch_images(images, labels, batch_size, labels_nums, one_hot=False, shuffle=False, num_threads=1): min_after_dequeue = 200 capacity = min_after_dequeue + 3 * batch_size if shuffle: images_batch, labels_batch = tf.train.shuffle_batch([images, labels], batch_size=batch_size, capacity=capacity, min_after_dequeue=min_after_dequeue, num_threads=num_threads) else: images_batch, labels_batch = tf.train.batch([images, labels], batch_size=batch_size, capacity=capacity, num_threads=num_threads) if one_hot: labels_batch = tf.one_hot(labels_batch, labels_nums, 1, 0) return images_batch, labels_batchdef read_records(filename, resize_height, resize_width, type=None): filename_queue = tf.train.string_input_producer([filename]) reader = tf.TFRecordReader() _, serialized_example = reader.read(filename_queue) features = tf.parse_single_example( serialized_example, features=&#123; 'image_raw': tf.FixedLenFeature([], tf.string), 'height': tf.FixedLenFeature([], tf.int64), 'width': tf.FixedLenFeature([], tf.int64), 'depth': tf.FixedLenFeature([], tf.int64), 'label': tf.FixedLenFeature([], tf.int64) &#125; ) tf_image = tf.decode_raw(features['image_raw'], tf.uint8) tf_height = features['height'] tf_width = features['width'] tf_depth = features['depth'] tf_label = tf.cast(features['label'], tf.int32) tf_image = tf.reshape(tf_image, [resize_height, resize_width, 3]) if type is None: tf_image = tf.cast(tf_image, tf.float32) elif type == 'normalization': tf_image = tf.cast(tf_image, tf.float32) * (1. / 255.0) elif type == 'centralization': tf_image = tf.cast(tf_image, tf.float32) * (1. / 255) - 0.5 return tf_image, tf_labeldef create_records(image_dir, file, output_record_dir, resize_height, resize_width, shuffle, log=5): images_list, labels_list = load_labels_file(file, 1, shuffle) writer = tf.python_io.TFRecordWriter(output_record_dir) for i, [image_name, labels] in enumerate(zip(images_list, labels_list)): image_path = os.path.join(image_dir, images_list[i]) if not os.path.exists(image_path): print('Err:no image', image_path) continue image = read_image(image_path, resize_height, resize_width) image_raw = image.tostring() if i % log == 0 or i == len(images_list) - 1: print('------------processing:%d-th------------' % (i)) print('current image_path=%s' % (image_path), 'shape:&#123;&#125;'.format(image.shape), 'labels:&#123;&#125;'.format(labels)) label = labels[0] example = tf.train.Example(features=tf.train.Features(feature=&#123; 'image_raw': _bytes_feature(image_raw), 'height': _int64_feature(image.shape[0]), 'width': _int64_feature(image.shape[1]), 'depth': _int64_feature(image.shape[2]), 'label': _int64_feature(label) &#125;)) writer.write(example.SerializeToString()) writer.close()def disp_records(record_file, resize_height, resize_width, show_nums=4): tf_image, tf_label = read_records(record_file, resize_height, resize_width, type='normalization') init_op = tf.initialize_all_variables() with tf.Session() as sess: sess.run(init_op) coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) for i in range(show_nums): image, label = sess.run([tf_image, tf_label]) print('shape:&#123;&#125;,tpye:&#123;&#125;,labels:&#123;&#125;'.format(image.shape, image.dtype, label)) show_image(\"image:%d\" % (label), image) coord.request_stop() coord.join(threads)def batch_test(record_file, resize_height, resize_width): tf_image, tf_label = read_records(record_file, resize_height, resize_width, type='normalization') image_batch, label_batch = get_batch_images(tf_image, tf_label, batch_size=4, labels_nums=5, one_hot=False, shuffle=False) init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init) coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(coord=coord) for i in range(4): images, labels = sess.run([image_batch, label_batch]) show_image(\"image\", images[0, :, :, :]) print('shape:&#123;&#125;,tpye:&#123;&#125;,labels:&#123;&#125;'.format(images.shape, images.dtype, labels)) coord.request_stop() coord.join(threads)if __name__ == '__main__': resize_height = 299 resize_width = 299 shuffle = True log = 5 image_dir = 'dataSet/train' train_labels = 'dataSet/train.txt' train_record_output = 'dataSet/record/train&#123;&#125;.tfrecords'.format(resize_height) create_records(image_dir, train_labels, train_record_output, resize_height, resize_width, shuffle, log) train_nums = get_example_nums(train_record_output) print(\"save train example nums=&#123;&#125;\".format(train_nums)) image_dir = 'dataSet/val' val_labels = 'dataSet/val.txt' val_record_output = 'dataSet/record/val&#123;&#125;.tfrecords'.format(resize_height) create_records(image_dir, val_labels, val_record_output, resize_height, resize_width, shuffle,log) val_nums = get_example_nums(val_record_output) print(\"save val example nums=&#123;&#125;\".format(val_nums)) batch_test(train_record_output, resize_height, resize_width) 目标检测数据集&emsp;&emsp;目标检测数据集的格式一般为VOC格式，即由图片文件夹，标注文件夹和txt图片信息文件夹组成。 文件夹设置&emsp;&emsp;首先新建一个文件夹VOCdevkit，然后在里面在新建1个文件夹VOC2019(当然也可以取其他年份)，然后新建3个文件夹Annotations，ImageSets和JPEGImages，其中ImageSets文件夹下再新建一个Main文件夹，在里面新建4个txt文件：test.txt，train.txt，trainval.txt，val.txt。12345678910VOCdevkit VOC2019 Annotations ImageSets Main test.txt train.txt trainval.txt val.txt JPEGImages 获取数据集&emsp;&emsp;目标检测的数据集的大小一般要比目标分类的大一些，可以包含多个检测目标，最后还需要对每个数据集利用标注软件进行标注。 标注数据集&emsp;&emsp;软件：labelImg 放入数据集&emsp;&emsp;将数据集图片放入到JPEGImages文件夹中，其对应的标注文件放入到Annotations中，然后在VOC2019下新建一个可以产生txt文件的py文件：create_label.py：123456789101112131415161718192021222324252627282930313233343536import osimport random trainval_percent = 0.2train_percent = 0.8xmlfilepath = 'Annotations'txtsavepath = 'ImageSets\\Main'total_xml = os.listdir(xmlfilepath) num = len(total_xml)list = range(num)tv = int(num * trainval_percent)tr = int(tv * train_percent)trainval = random.sample(list, tv)train = random.sample(trainval, tr) ftrainval = open('ImageSets/Main/trainval.txt', 'w')ftest = open('ImageSets/Main/test.txt', 'w')ftrain = open('ImageSets/Main/train.txt', 'w')fval = open('ImageSets/Main/val.txt', 'w') for i in list: name = total_xml[i][:-4] + '\\n' if i in trainval: ftrainval.write(name) if i in train: ftest.write(name) else: fval.write(name) else: ftrain.write(name) ftrainval.close()ftrain.close()fval.close()ftest.close() &emsp;&emsp;首先将数据集8:2分成train.txt和trainval.txt，然后再将trainval.txt再8:2分成test.txt和val.txt。 生成数据集文本文件&emsp;&emsp;对于目标检测来说，还需要生成一个包含数据集路径、标注框信息和类别信息的txt文件。在最外面的文件夹在新建一个转换文件：voc_annotation.py：123456789101112131415161718192021222324252627282930313233import xml.etree.ElementTree as ETfrom os import getcwdsets=[('2019', 'train'), ('2019', 'val'), ('2019', 'test')]classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]def convert_annotation(year, image_id, list_file): in_file = open('VOCdevkit/VOC%s/Annotations/%s.xml'%(year, image_id)) tree=ET.parse(in_file) root = tree.getroot() for obj in root.iter('object'): difficult = obj.find('difficult').text cls = obj.find('name').text if cls not in classes or int(difficult)==1: continue cls_id = classes.index(cls) xmlbox = obj.find('bndbox') b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text), int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text)) list_file.write(\" \" + \",\".join([str(a) for a in b]) + ',' + str(cls_id))wd = getcwd()for year, image_set in sets: image_ids = open('VOCdevkit/VOC%s/ImageSets/Main/%s.txt'%(year, image_set)).read().strip().split() list_file = open('%s_%s.txt'%(year, image_set), 'w') for image_id in image_ids: list_file.write('%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg'%(wd, year, image_id)) convert_annotation(year, image_id, list_file) list_file.write('\\n') list_file.close() &emsp;&emsp;更改相应的信息，最后运行会生成3个txt文件，即2019_train.txt，2019_test.txt和2019_val.txt。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://cxx0822.github.io/categories/深度学习/"}],"tags":[]},{"title":"基于MATLAB的深度学习——入门篇","slug":"基于MATLAB的深度学习——入门篇","date":"2019-07-21T07:37:14.000Z","updated":"2019-08-04T01:40:00.866Z","comments":true,"path":"2019/07/21/基于MATLAB的深度学习——入门篇/","link":"","permalink":"http://cxx0822.github.io/2019/07/21/基于MATLAB的深度学习——入门篇/","excerpt":"","text":"概述&emsp;&emsp;MATLAB从2016年开始就提供深度神经网络的相关工具了，现如今，深度学习的基本使用模块已经相对完善，并且支持逐层拖拽的simulink型的架构设计方式。&emsp;&emsp;Deep Learning Toolbox™提供了一个用于通过算法、预训练模型和应用程序来设计和实现深度神经网络的框架。您可以使用卷积神经网络(ConvNet、CNN)和长短期记忆 (LSTM) 网络对图像、时序和文本数据执行分类和回归。应用程序和绘图可帮助您可视化激活值、编辑网络架构和监控训练进度。&emsp;&emsp;对于小型训练集，您可以使用预训练深度网络模型(包括 SqueezeNet、Inception-v3、ResNet-101、GoogLeNet和VGG-19)以及从TensorFlow®-Keras和Caffe导入的模型执行迁移学习。&emsp;&emsp;要加速对大型数据集的训练，您可以将计算和数据分布到桌面计算机上的多核处理器和GPU中(使用Parallel Computing Toolbox™)，或者扩展到群集和云，包括Amazon EC2® P2、P3 和 G3 GPU 实例(使用 MATLAB® Distributed Computing Server™)。 平台&emsp;&emsp;Windows 10&emsp;&emsp;MATLAB 2018a&emsp;&emsp;Deep Learning Toolbox&emsp;&emsp;Computer Vision Toolbox 注：如果没有Deep Learning Toolbox和Computer Vision Toolbox，需要自己安装，在MATLAB主页的APP中，点击获取更多App，在打开的附加功能资源管理器中，搜索这2个工具箱并下载安装即可。 一个简单的例子&emsp;&emsp;在MATLAB官方文档中，提供了一些简单的入门实例。下面我们挑选第一个案例进行简单的分析。 使用 GoogLeNet 对图像进行分类&emsp;&emsp;首先是使用一个典型的的深度卷积神经网络GoogLeNet对图像进行分类。&emsp;&emsp;GoogLeNet已经对超过一百万个图像进行了训练，可以将图像分为1000个对象类别（例如键盘、咖啡杯、铅笔和多种动物）。该网络已基于大量图像学习了丰富的特征表示。网络以图像作为输入，然后输出图像中对象的标签以及每个对象类别的概率。 加载预训练网络1net = googlenet &emsp;&emsp;加载一个已知网络十分简单，直接令变量名等于网络名即可(第一次加载可能需要联网下载)。下面我们使用disp(net)简单看一个googlenet的内容：&emsp;&emsp;每个net都有2个属性，即Layers(网络层)和Connections(层连接)，Layers为数组，指的是网络图层，指定为Layer阵列。Connections是表，图层连接，指定为具有两列的表。每个表行表示图层图中的连接。第一列Source指定每个连接的源。第二列Destination指定每个连接的目标。连接源和目标要么是图层名称,要么layerName/IOName的窗体,其中IOName是图层输入或输出的名称。&emsp;&emsp;我们可以继续disp(net.Layers)查看其网络结构：&emsp;&emsp;可以看出，googlenet一共有144层，第一列为网络名称，第二列为网络的类型，第三列为网络的具体参数。&emsp;&emsp;这里我们需要网络结构里面的第一个和最后一个参数，即图像输入和分类类别，先定义2个变量保存该信息。12inputSize = net.Layers(1).InputSize;classNames = net.Layers(end).ClassNames; &emsp;&emsp;我们可以简单看下这2个参数里面的信息： 读取图像并调整图像大小&emsp;&emsp;有了训练好的网络结构后，我们就可以用图片进行分类预测了。&emsp;&emsp;首先读取一张图片，这里使用了官方文档里面的图片，然后要将其调整到网络需要的输入图片大小(否则会编译报错)。12I = imread('peppers.png');I = imresize(I,inputSize(1:2)); &emsp;&emsp;使用imresize将图像大小调整为网络的输入大小。调整大小会略微更改图像的纵横比。当然也可以使用更好的调整大小的函数。&emsp;&emsp;B = imresize(A, [numrows numcols])：numrows和numcols分别指定目标图像的高度和宽度。这里的numrows和numcols就是刚才inputSize的第1列和第2列数组，即224,224。 对图像进行分类&emsp;&emsp;使用classify对图像进行分类并计算类概率。用于分类的网络训练为针对每个输入图像输出单个标签，即使图像包含多个对象时也是如此。最后我们也可以显示图像及预测的标签，以及具有该标签的图像的预测概率。1234[label,scores] = classify(net,I);figureimshow(I)title(string(label) + \", \" + num2str(100*scores(classNames == label),3) + \"%\"); &emsp;&emsp;classify()：使用经过训练的深度学习神经网络对数据进行分类，一共有3种形式：&emsp;&emsp;其返回值label就是scores中数组最高的那个对应的label，scores是一个1000*1的向量(因为googlenet一共有1000个分类类别)，里面依次存放着每个列别的置信度。scores(classNames == label)即表示找到label对应的那个scores。 其结果如图所示： 显示排名靠前的预测值&emsp;&emsp;根据scores的特点，我们还可以显示出排名前五的预测标签，并以直方图形式显示它们的相关概率。由于网络将图像分类为如此多的对象类别，并且许多类别是相似的，因此在评估网络时通常会考虑准确度排名前五的几个类别。网络以高概率将图像分类为甜椒。1234567891011[~,idx] = sort(scores,'descend'); % 降序排列idx = idx(5:-1:1);classNamesTop = net.Layers(end).ClassNames(idx); % 获取前5个类别scoresTop = scores(idx);figurebarh(scoresTop) %绘制水平条形图xlim([0 1])title('Top 5 Predictions')xlabel('Probability')yticklabels(classNamesTop) &emsp;&emsp;其结果如下： 标注自己的数据集","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://cxx0822.github.io/categories/深度学习/"}],"tags":[]},{"title":"典型的深度神经网络","slug":"典型的深度神经网络","date":"2019-07-06T01:50:31.000Z","updated":"2019-08-04T01:39:32.978Z","comments":true,"path":"2019/07/06/典型的深度神经网络/","link":"","permalink":"http://cxx0822.github.io/2019/07/06/典型的深度神经网络/","excerpt":"","text":"AlexNet简介&emsp;&emsp; 网络结构&emsp;&emsp;该网络包含8个带权重的层；前5层是卷积层，剩下的3层是全连接层。最后一层全连接层的输出是1000维softmax的输入，softmax会产生1000类标签。 卷积层C1&emsp;&emsp;该层的处理流程是：卷积—&gt;ReLU—&gt;池化—&gt;归一化。&emsp;&emsp;卷积：输入图像是227×227×3，使用96个11×11×3的卷积核，步长为4，不边缘填充，所以得到的FeatureMap为55×55×96。(55=(227-11)/4+1)&emsp;&emsp;ReLU：将卷积层输出的FeatureMap输入到ReLU函数中。&emsp;&emsp;池化：使用3×3步长为2的池化单元，输出为27×27×96。(27=(55−3)/2+1)&emsp;&emsp;局部响应归一化：使用k=2,n=5,α=10−4,β=0.75进行局部归一化，输出的仍然为27×27×96，输出分为两组，每组的大小为27×27×48。 卷积层C2&emsp;&emsp;该层的处理流程是：卷积—&gt;ReLU—&gt;池化—&gt;归一化&emsp;&emsp;卷积：输入是2组27×27×48。使用2组，每组128个尺寸为5×5×48的卷积核，边缘填充padding=2，卷积的步长为1。则输出的FeatureMap为2组，每组的大小为27×27×128。一共是27×27×256。(27=(27+2∗2−5)/1+1)&emsp;&emsp;ReLU：将卷积层输出的FeatureMap输入到ReLU函数中。&emsp;&emsp;池化运算的尺寸为3×3，步长为2，池化后图像的尺寸为13=(27−3)/2+1，所以输出为13×13×256。&emsp;&emsp;局部响应归一化：使用k=2,n=5,α=10−4,β=0.75进行局部归一化，输出的仍然为13×13×256，输出分为2组，每组的大小为13×13×128。 卷积层C3&emsp;&emsp;该层的处理流程是：卷积—&gt;ReLU&emsp;&emsp;卷积：输入是13×13×256，使用2组共384个尺寸为3×3×256的卷积核，边缘填充padding=1，卷积的步长为1。则输出的FeatureMap为13×13×384。(13=(13+2∗1−3)/1+1)&emsp;&emsp;ReLU：将卷积层输出的FeatureMap输入到ReLU函数中。 卷积层C4&emsp;&emsp;该层的处理流程是：卷积—&gt;ReLU&emsp;&emsp;卷积：输入是13×13×384，分为两组，每组为13×13×192。使用2组，每组192个尺寸为3×3×192的卷积核，边缘填充padding=1，卷积的步长为1。则输出的FeatureMap为13×13×384，分为两组，每组为13×13×192。(13=(13+2∗1−3)/1+1)&emsp;&emsp;ReLU：将卷积层输出的FeatureMap输入到ReLU函数中。 卷积层C5&emsp;&emsp;该层处理流程为：卷积—&gt;ReLU—&gt;池化&emsp;&emsp;卷积：输入为13×13×384，分为两组，每组为13×13×192。使用2组，每组为128个尺寸为3×3×192的卷积核，边缘填充padding=1，卷积的步长为1。则输出的FeatureMap为13×13×256。(13=(13+2∗1−3)/1+1)&emsp;&emsp;ReLU：将卷积层输出的FeatureMap输入到ReLU函数中。&emsp;&emsp;池化：池化运算的尺寸为3×3，步长为2，池化后图像的尺寸为(13−3)/2+1=6，即池化后的输出为6×6×256 全连接层FC6&emsp;&emsp;该层的流程为：（卷积）全连接 —&gt;ReLU —&gt;Dropout&emsp;&emsp;卷积-&gt;全连接：输入为6×6×256，该层有4096个卷积核，每个卷积核的大小为6×6×256。由于卷积核的尺寸刚好与待处理特征图（输入）的尺寸相同，即卷积核中的每个系数只与特征图（输入）尺寸的一个像素值相乘，一一对应，因此，该层被称为全连接层。由于卷积核与特征图的尺寸相同，卷积运算后只有一个值，因此，卷积后的像素层尺寸为4096×1×1，即有4096个神经元。&emsp;&emsp;ReLU：这4096个运算结果通过ReLU激活函数生成4096个值。&emsp;&emsp;Dropout：抑制过拟合，随机的断开某些神经元的连接或者是不激活某些神经元。 全连接层FC7&emsp;&emsp;流程为：全连接—&gt;ReLU—&gt;Dropout&emsp;&emsp;全连接：输入为4096的向量。&emsp;&emsp;ReLU：这4096个运算结果通过ReLU激活函数生成4096个值。&emsp;&emsp;Dropout：抑制过拟合，随机的断开某些神经元的连接或者是不激活某些神经元。 输出层&emsp;&emsp;第七层输出的4096个数据与第八层的1000个神经元进行全连接，经过训练后输出1000个float型的值，这就是预测结果。 参数数量&emsp;&emsp;卷积层的参数 = 卷积核的数量 * 卷积核 + 偏置 TensorFlow实现VGG简介&emsp;&emsp;VGGNet是牛津大学计算机视觉组(Visual Geometry Group)和Google DeepMind公司的研究员一起研发的深度卷积神经网络。&emsp;&emsp;VGG在AlexNet基础上做了改进，整个网络都使用了同样大小的3*3卷积核尺寸和2*2最大池化尺寸，网络结构简洁。 网络结构&emsp;&emsp;VGG一共有五组卷积，每组卷积之后紧接着最大池化层，后面接上三个全连接层，最后softmax输出。一共有两种形式，即VGG16和VGG19，后面的数字含义为卷积层和全连接层的个数，不包括池化层。 第一组卷积&emsp;&emsp;2个卷积层+1个池化层&emsp;&emsp;卷积层：conv3 - 64，卷积核为3×3×3，步长为1，填充为1，共64个，输出为：224*224*64，(224=(224+2∗1−3)/1+1)。(输入图像是224×224×3) 第2个卷积层结构不变，输出不变，下同。&emsp;&emsp;池化层：池化核为2×2，步长为2，填充为0，共64个，输出为：112*112*64，(112=(224+2∗0−2)/2+1)。 第二组卷积&emsp;&emsp;2个卷积层+1个池化层&emsp;&emsp;卷积层：conv3 - 128，卷积核为3×3×3，步长为1，填充为1，共128个，输出为：112*112*128，(112=(112+2∗1−3)/1+1)。&emsp;&emsp;池化层：池化核为2×2，步长为2，填充为0，共128个，输出为：56*56*128，(56=(112+2∗0−2)/2+1)。 第三组卷积&emsp;&emsp;2个卷积层+1个池化层&emsp;&emsp;卷积层：conv3 - 256，卷积核为3×3×3，步长为1，填充为1，共256个，输出为：56*56*256，(56=(56+2∗1−3)/1+1)。&emsp;&emsp;池化层：池化核为2×2，步长为2，填充为0，共256个，输出为：28*28*256，(28=(56+2∗0−2)/2+1)。 第四组卷积&emsp;&emsp;2个卷积层+1个池化层&emsp;&emsp;卷积层：conv3 - 512，卷积核为3×3×3，步长为1，填充为1，共512个，输出为：28*28*512，(28=(28+2∗1−3)/1+1)。&emsp;&emsp;池化层：池化核为2×2，步长为2，填充为0，共512个，输出为：14*14*512，(14=(28+2∗0−2)/2+1)。 第五组卷积&emsp;&emsp;2个卷积层+1个池化层&emsp;&emsp;卷积层：conv3 - 512，卷积核为3×3×3，步长为1，填充为1，共512个，输出为：14*14*512，(14=(14+2∗1−3)/1+1)。&emsp;&emsp;池化层：池化核为2×2，步长为2，填充为0，共512个，输出为：7*7*512，(7=(14+2∗0−2)/2+1)。 全连接层和输出层&emsp;&emsp;和AlexNet类似。 参数数量改进 去掉了LRN层，作者发现深度网络中LRN的作用并不明显，干脆取消了。 采用更小的卷积核-3x3，Alexnet中使用了更大的卷积核，比如有7x7的，因此VGG相对于Alexnet而言，参数量更少。 池化核变小，VGG中的池化核是2x2，stride为2，Alexnet池化核是3x3，步长为2。 &emsp;&emsp;这样做改进都是有一些原因的，首先为了更好的探究深度对网络的影响，必须要解决参数量的问题，更深的网络意味着更多的参数，训练更困难，使用大卷积核时尤其明显。作者通过分析，认为由于卷积神经网络的特性，3x3大小的卷积核足以捕捉到横、竖以及斜对角像素的变化。使用大卷积核会带来参数量的爆炸不说，而且图像中会存在一些部分被多次卷积，可能会给特征提取带来困难，所以在VGG中，普遍使用3x3的卷积。 TensorFlow实现GoogLeNetTensorFlow实现项目文件结构说明数据集预处理数据集下载生成train.txt和val.txt制作tfrecords数据格式tfrecords数据格式简介&emsp;&emsp;TFRecords文件包含了tf.train.Example协议内存块(protocol buffer)(协议内存块包含了字段Features)。我们可以写一段代码获取你的数据，将数据填入到Example协议内存块(protocol buffer)，将协议内存块序列化为一个字符串， 并且通过tf.python_io.TFRecordWriter写入到TFRecords文件。&emsp;&emsp;从TFRecords文件中读取数据， 可以使用tf.TFRecordReader的tf.parse_single_example解析器。这个操作可以将Example协议内存块(protocol buffer)解析为张量。 写入tfrecords数据&emsp;&emsp;一个Example中包含Features，Features里包含Feature的字典。最后，Feature里包含有一个FloatList，或者ByteList，或者Int64List。1234567891011121314151617181920212223def create_records(image_dir, file, output_record_dir, resize_height, resize_width, shuffle): images_list, labels_list = load_labels_file(file, 1, shuffle) writer = tf.python_io.TFRecordWriter(output_record_dir) for i, [image_name, labels] in enumerate(zip(images_list, labels_list)): image_path = os.path.join(image_dir, images_list[i]) if not os.path.exists(image_path): print('Err:no image', image_path) continue image = read_image(image_path, resize_height, resize_width) image_raw = image.tostring() label = labels[0] example = tf.train.Example(features=tf.train.Features(feature=&#123; 'image_raw': _bytes_feature(image_raw), 'height': _int64_feature(image.shape[0]), 'width': _int64_feature(image.shape[1]), 'depth': _int64_feature(image.shape[2]), 'label': _int64_feature(label) &#125;)) writer.write(example.SerializeToString()) writer.close() &emsp;&emsp;首先利用load_labels_file()函数(后面会给出其定义，下同)将images和labels加载进来，然后创建一个TFRecordWriter对象，这个对象就负责把记录写到指定的文件中，其函数参数为TFRecords，即文件路径。然后利用read_image()读取图片，最后将数据填入到Example协议内存块，其中图片的格式为字符串格式，其他均为整型格式。附：辅助函数定义：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# -*-coding: utf-8 -*-import tensorflow as tfimport numpy as npimport osimport cv2import matplotlib.pyplot as pltimport randomfrom PIL import Image# 生成整型的属性def _int64_feature(value): return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))# 生成字符串型的属性def _bytes_feature(value): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))# 生成实数型的属性def float_list_feature(value): return tf.train.Feature(float_list=tf.train.FloatList(value=value))def get_example_nums(tf_records_filenames): nums = 0 for record in tf.python_io.tf_record_iterator(tf_records_filenames): nums += 1 return numsdef show_image(title, image): plt.imshow(image) plt.axis('on') plt.title(title) plt.show()def load_labels_file(filename, labels_num=1, shuffle=False): images = [] labels = [] with open(filename) as f: lines_list = f.readlines() if shuffle: random.shuffle(lines_list) for lines in lines_list: line = lines.rstrip().split(' ') label = [] for i in range(labels_num): label.append(int(line[i + 1])) # 单label，即line[1]就是label images.append(line[0]) labels.append(label) return images, labelsdef read_image(filename, resize_height, resize_width, normalization=False): bgr_image = cv2.imread(filename) if len(bgr_image.shape) == 2: print(\"Warning:gray image\", filename) bgr_image = cv2.cvtColor(bgr_image, cv2.COLOR_GRAY2BGR) rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB) if resize_height &gt; 0 and resize_width &gt; 0: rgb_image = cv2.resize(rgb_image, (resize_width, resize_height)) rgb_image = np.asanyarray(rgb_image) if normalization: rgb_image = rgb_image / 255.0 return rgb_imagedef get_batch_images(images, labels, batch_size, labels_nums, one_hot=False, shuffle=False, num_threads=1): min_after_dequeue = 200 capacity = min_after_dequeue + 3 * batch_size # 保证capacity必须大于min_after_dequeue参数值 if shuffle: images_batch, labels_batch = tf.train.shuffle_batch([images, labels], batch_size=batch_size, capacity=capacity, min_after_dequeue=min_after_dequeue, num_threads=num_threads) else: images_batch, labels_batch = tf.train.batch([images, labels], batch_size=batch_size, capacity=capacity, num_threads=num_threads) if one_hot: labels_batch = tf.one_hot(labels_batch, labels_nums, 1, 0) return images_batch, labels_batch &emsp;&emsp;整体比较简单，就不多做解释了。 读取tfrecords数据&emsp;&emsp;一旦生成了TFRecords文件，为了高效地读取数据，TF中使用队列(queue)读取数据。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283def read_records(filename, resize_height, resize_width, type=None): filename_queue = tf.train.string_input_producer([filename]) reader = tf.TFRecordReader() _, serialized_example = reader.read(filename_queue) features = tf.parse_single_example( serialized_example, features=&#123; 'image_raw': tf.FixedLenFeature([], tf.string), 'height': tf.FixedLenFeature([], tf.int64), 'width': tf.FixedLenFeature([], tf.int64), 'depth': tf.FixedLenFeature([], tf.int64), 'label': tf.FixedLenFeature([], tf.int64) &#125; ) tf_image = tf.decode_raw(features['image_raw'], tf.uint8) tf_height = features['height'] tf_width = features['width'] tf_depth = features['depth'] tf_label = tf.cast(features['label'], tf.int32) tf_image = tf.reshape(tf_image, [resize_height, resize_width, 3]) if type is None: tf_image = tf.cast(tf_image, tf.float32) elif type == 'normalization': tf_image = tf.cast(tf_image, tf.float32) * (1. / 255.0) elif type == 'standardization': tf_imag = tf.cast(tf_image, tf.float32) * (1. / 255) - 0.5 return tf_image, tf_label``` &amp;emsp;&amp;emsp;首先利用`string_input_producer()`将字符串输出到一个输入管道队列，然后利用`parse_single_example()`解析器解析。 #### 测试```pythondef batch_test(record_file, resize_height, resize_width): # 读取record函数 tf_image, tf_label = read_records(record_file, resize_height, resize_width, type='normalization') image_batch, label_batch = get_batch_images(tf_image, tf_label, batch_size=4, labels_nums=2, one_hot=False, shuffle=True) init = tf.global_variables_initializer() with tf.Session() as sess: # 开始一个会话 sess.run(init) coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(coord=coord) for i in range(4): # 在会话中取出images和labels images, labels = sess.run([image_batch, label_batch]) # 这里仅显示每个batch里第一张图片 show_image(\"image\", images[0, :, :, :]) print('shape:&#123;&#125;,tpye:&#123;&#125;,labels:&#123;&#125;'.format(images.shape, images.dtype, labels)) # 停止所有线程 coord.request_stop() coord.join(threads)if __name__ == '__main__': # 参数设置 resize_height = 224 # 指定存储图片高度 resize_width = 224 # 指定存储图片宽度 shuffle = True # 产生train.record文件 image_dir = 'dataset/train' train_labels = 'dataset/train.txt' # 图片路径 train_record_output = 'dataset/record/train.tfrecords' create_records(image_dir, train_labels, train_record_output, resize_height, resize_width, shuffle) train_nums = get_example_nums(train_record_output) print(\"save train example nums=&#123;&#125;\".format(train_nums)) # 产生val.record文件 image_dir = 'dataset/val' val_labels = 'dataset/val.txt' # 图片路径 val_record_output = 'dataset/record/val.tfrecords' create_records(image_dir, val_labels, val_record_output, resize_height, resize_width, shuffle) val_nums = get_example_nums(val_record_output) print(\"save val example nums=&#123;&#125;\".format(val_nums)) # 测试显示函数 # disp_records(train_record_output,resize_height, resize_width) batch_test(train_record_output, resize_height, resize_width)","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://cxx0822.github.io/categories/深度学习/"}],"tags":[]},{"title":"基于YOLO的目标检测","slug":"基于YOLO的目标检测","date":"2019-06-23T10:39:28.000Z","updated":"2020-01-09T04:15:16.907Z","comments":true,"path":"2019/06/23/基于YOLO的目标检测/","link":"","permalink":"http://cxx0822.github.io/2019/06/23/基于YOLO的目标检测/","excerpt":"","text":"摘要&emsp;&emsp; 平台&emsp;&emsp;系统：Ubuntu&emsp;&emsp;框架：Darknet 配置环境opencv配置下载opencv3.4.0&emsp;&emsp;从官网下载即可，网址，选择Sources源代码，尽量选择3.4.0及以下的版本，高版本的不一定能配置成功。 安装依赖&emsp;&emsp;依次执行下面4条命令即可。 sudo apt-get install gcc g++ cmake pkg-config build-essential sudo apt-get install build-essential cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev sudo apt-get install python-dev python-numpy libavcodec-dev libavformat-dev libswscale-dev sudo apt-get install libgtk2.0-dev libavcodec-dev libavformat-dev libtiff4-dev libswscale-dev libjasper-dev注：安装过程中，有yes/no选项时，选择yes即可。 编译&emsp;&emsp;刚才从官网下载的只是opencv的源码，我们还要对其编译后才能正常使用。 cd进入到opencv文件夹，比如我的opencv位置是在home目录下：cd opencv-3.4.0 新建一个文件夹build：mkdir build cd进入到build文件夹：cd build cmake编译：cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local ..这里需要下载一个压缩包，如果网络环境不好的话，可能会卡很长时间，这里我们可以先离线下载好压缩包。首先在github网站上找到压缩包并下载，网址，然后再打开opencv文件夹内的3rdparty/ippicv里面的ippicv.cmake文件，并将里面的压缩包下载网址改为本地的文件路径，如file://Downloads。最后再重新执行上面命名即可。 make -j8 sudo make install 环境配置&emsp;&emsp;最后添加Ubuntu的环境变量就全部完成了。 首先将OpenCV的库添加到路径：sudo gedit /etc/ld.so.conf.d/opencv.conf执行此命令后打开的可能是一个空白的文件，在文件末尾添加：/usr/local/lib注：这里很多博客都这么写，我当时也这么做的，但实际运行时，仍然报错，说找不到libopencv_highgui.so.3.4文件，后来才发现我的这个文件并不在该路径下，而是在该路径下的x86_64-linux-gnu文件夹内，所以正确的应该是/usr/local/lib/x86_64-linux-gnu。所以读者在实际配置时，这里填的应该是libopencv_highgui.so.3.4文件的上一级文件夹路径。 生效配置文件：sudo ldconfig 配置bash:sudo gedit /etc/bash.bashrc在末尾添加：PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig export PKG_CONFIG_PATH保存，执行如下命令使得配置生效：source /etc/bash.bashrc更新：sudo updatedb至此，所有opencv的配置都已经完成。 darknet配置下载源代码&emsp;&emsp;登陆darknet官网，在里面找到其GitHub地址，然后下载即可。注：如果读者想在Windows上使用YOLO，可以自己新建build文件夹编译，也可以使用别人的：网址。&emsp;&emsp;然后解压缩，并cd进入该文件夹下，输入make指令编译：12cd darknet-mastermake &emsp;&emsp;在继续输入./darknet，如果没有报错，则配置成功。 更改配置文件&emsp;&emsp;打开里面的Makefile文件，修改里面的配置信息，如果装了GPU，将GPU和CUDNN设置为1，如果装了opnecv，将OPENCV设置为1。(我这里只装了opencv) 下载权重&emsp;&emsp;进入darknet官网中，选择里面的YOLO选项，进入YOLO主页，在里面找到权重的下载地址，点击下载即可。注：可能会很慢，可以找别人下好的百度云下载。 测试&emsp;&emsp;输入下面指令：1./darknet detect cfg/yolov3.cfg yolov3.weights data/dog.jpg &emsp;&emsp;最终的结果如图所示，生成每个类别的概率信息和标注框信息。 初识darknet源代码框架&emsp;&emsp;cfg：存放的是各种常见网络结构的配置文件，如yolo、rcnn等，如果想要定义自己的网络结构需要编写自己的cfg文件；&emsp;&emsp;data：各种数据集；&emsp;&emsp;examples：存放的各种检测算法的例子，如detector.c就是检测的代码，根据你输入run_detector函数的参数是train还是test转到其内部的train_detector或者test_detector，此文件夹中最重要的文件是darknet.c；&emsp;&emsp;include：只有一个文件darknet.h是darknet的头文件，主要是一些定义和函数声明；&emsp;&emsp;scripts：几个shell脚本，用来获取数据集的；&emsp;&emsp;src：绝大部分的源码，里面有BN层的实现、卷积层的实现、正则化等等。 编译&emsp;&emsp;编译好了，这时我们会发现文件夹里多出了obj、backup、results三个文件夹和libdarknet.a静态库、libdarknet.so动态库。动态链接的基本思想简单来说，就是不对那些组成程序的目标文件进行链接，而是当程序运行时才进行链接，从而解决了静态链接空间浪费的问题。obj文件夹下就是所有目标文件，backup文件夹主要存放训练的模型，result文件夹主要存放训练的结果。 简单使用&emsp;&emsp;darknet的使用也是很简单，我们先在darknet官网上下载好已经训练出的yolov3的权重，之后在终端运行：1./darknet detect cfg/yolov3.cfg yolov3.weights data/dog.jpg &emsp;&emsp;这个命令结合代码非常好理解，我们首先在examples文件夹下的darknet.c文件里面找到darknet的主函数:&emsp;&emsp;这是主函数的部分截图，可以看出该程序是根据给定的argv(命令行参数)来决定程序的走向，如本例argv[1](命令行的第一个参数，下同)为detect，根据主函数中的第6个if判断语句可以看出，该程序会转到test_dector函数中，也就是测试检测，后面的args[2]、args[3]、args[4]都是该函数的形参。那test_dector()函数又是什么意思呢？&emsp;&emsp;这个函数的定义并不在当前的darknet.c文件中，在第8行可以看到其使用了extern来声明这是一个外部函数，该函数是在examples文件夹下的detector.c文件里面，其函数头为：1void test_detector(char *datacfg, char *cfgfile, char *weightfile, char *filename, float thresh, float hier_thresh, char *outfile, int fullscreen) &emsp;&emsp;该函数由darknet.c中的主函数调用，该函数为一个前向推理测试函数，不包括训练过程，因此如果要使用该函数，必须提前训练好网络，并加载训练好的网络参数文件。&emsp;&emsp;其主要参数含义为：&emsp;&emsp;datacfg：数据集信息文件路径(也即cfg/*.data文件)，文件中包含有关数据集的信息，比如cfg/coco.data；&emsp;&emsp;cfgfile：网络配置文件路径(也即cfg/*.cfg文件)，包含一个网络所有的结构参数，比如cfg/yolo.cfg；&emsp;&emsp;weightfile：已经训练好的网络权重文件路径，比如darknet网站上下载的yolo.weights文件；&emsp;&emsp;filename：待进行检测的图片路径(单张图片)。&emsp;&emsp;综上所述，这个命令代码的含义就是：运用yolov3.cfg的网络框架和yolov3.weights权重去测试dog.jpg这张图片并显示结果。其中数据集为默认的coco.data。当然了，我们也可以使用其他的网络结构去测试其他图片。 初识Yolo目标检测&emsp;&emsp;在正式讲解yolo之前，我们先简单了解一下计算机视觉中的目标检测。&emsp;&emsp;图像分类是计算机视觉最基本的任务之一，在图像分类的基础上，还有更复杂和有意思的任务，如目标检测，物体定位，图像分割等。其中目标检测是一件比较实际的且具有挑战性的计算机视觉任务，其可以看成图像分类与定位的结合，给定一张图片，目标检测系统要能够识别出图片的目标并给出其位置。&emsp;&emsp;最近几年比较流行的目标检测算法可以分为两类，一类是基于Region Proposal(候选区域)的R-CNN系算法(R-CNN，Fast R-CNN, Faster R-CNN)，它们是two-stage(两步走)的，需要先使用启发式方法(selective search)或者CNN网络(RPN)产生Region Proposal，然后再在Region Proposal上做分类与回归。而另一类是Yolo，SSD这类one-stage(一步走)算法，其仅仅使用一个CNN网络直接预测不同目标的类别与位置。&emsp;&emsp;第一类方法是准确度高一些，但是速度慢，但是第二类算法是速度快，但是准确性要低一些。这可以在下图中看到。&emsp;&emsp;其中FPS表示每秒帧率，用来评估速度，mAP表示多类别的平均精度，用来评估准确率。 Yolo名称含义&emsp;&emsp;Yolo其全称是You Only Look Once: Unified, Real-Time Object Detection，这个题目取得非常好，基本上把Yolo算法的特点概括全了：You Only Look Once说的是只需要一次CNN运算，Unified指的是这是一个统一的框架，提供end-to-end的预测，而Real-Time体现是Yolo算法速度快。 Yolo之前的目标检测思想&emsp;&emsp;在Yolo出现之前，目标检测中比较常用的思想是滑动窗口技术。其基本原理就是采用不同大小和比例(宽高比)的窗口在整张图片上以一定的步长进行滑动，然后对这些窗口对应的区域做图像分类，这样就可以实现对整张图片的检测了，如下图所示，如DPM就是采用这种思路。&emsp;&emsp;但是这个方法有致命的缺点，就是你并不知道要检测的目标大小是什么规模，所以你要设置不同大小和比例的窗口去滑动，而且还要选取合适的步长。但是这样会产生很多的子区域，并且都要经过分类器去做预测，这需要很大的计算量，所以你的分类器不能太复杂，因为要保证速度。解决思路之一就是减少要分类的子区域，这就是R-CNN的一个改进策略，其采用了selective search(选择性搜索)方法来找到最有可能包含目标的子区域(Region Proposal)，其实可以看成采用启发式方法过滤掉很多子区域，这会提升效率。 Yolo的检测思想&emsp;&emsp;Yolo算法不再是窗口滑动了，而是直接将原始图片分割成互不重合的小方块，然后通过卷积直接生成特征图，我们可以认为特征图的每个元素也是对应原始图片的一个小方块，然后用每个元素来可以预测那些中心点在该小方格内的目标，这就是Yolo算法的朴素思想。&emsp;&emsp;整体来看，Yolo算法采用一个单独的CNN模型实现end-to-end的目标检测，整个系统如下图所示：首先将输入图片resize到448x448，然后送入CNN网络，最后处理网络预测结果得到检测的目标。相比R-CNN算法，其是一个统一的框架，其速度更快，而且Yolo的训练过程也是end-to-end的。也就是这里的窗口就是整幅图片。 再看Darknet——测试自己的数据集&emsp;&emsp;之前我们使用了Yolo作者提供的数据集和权重系数来预测图片，那我们该如何建立自己的数据集和权重系数呢？ 数据集配置&emsp;&emsp;Yolo属于有监督学习，即事先知道分类的标签值，所以我们需要采集大量的数据集供Yolo学习。下面以检测黄杆为例，讲解如何配置数据集。 重命名数据集&emsp;&emsp;我们采集到的数据集可以是从网上找的，也可以是手机拍摄的，也可以是从视频里面截取的等等，但往往其命名格式都是互不相同，如果没有一个统一的命名规则，直接训练的话，显然不是太好，所以我们首先要将采集到的数据集统一命名：123456789101112131415161718import osdef rename_files(dir_path): \"\"\" 批量重命名文件 参数： dirPath：文件路径 \"\"\" file_list = os.listdir(dir_path) index = 0 for item in file_list: oldname = dir_path + r\"\\\\\" + file_list[index] newname = dir_path + r\"\\\\\" + \".jpg\" os.rename(oldname, newname) index += 1 &emsp;&emsp;这段代码的作用就是把文件夹下的图片依次命名为：0.jpg，1.jpg…，当然你也可以自己设置命名规则。代码比较简单，就不多做讲解了。 标注数据集&emsp;&emsp;有了数据集，接下来就是对数据集的每张照片进行人工标注了。这里推荐使用标注软件labelImg(自行百度下载即可)，其使用方法也十分简单。&emsp;&emsp;首先打开data中的predefined_classes.txt，将其信息修改为要标注的类别，比如stick，有几类就写多少。然后打开软件，首先选择Open Dir打开图像文件夹，然后选择Change Save Dir选择要保存的xml文件的文件路径（标注完成后会生成一个xml文件存放标注信息），然后点击Create RectBox创建矩形。&emsp;&emsp;在需要的区域拖拽鼠标即可，然后在弹出的对话框中选择类别的名称，最后点击save保存，并选择Next Image切换到下一张。这时就可以看到保存xml文件夹中会出现对应照片的xml文件了。 准备数据集&emsp;&emsp;有了原始数据集和标注文件后，我们就可以配置属于Yolo的数据集了。&emsp;&emsp;首先创建一个文件夹VOCdevkit，当然你也可以起个其他的名字，只不过Yolo作者是这么做的，其源代码也是照着这个来的，所以一般就起这个就可以了。然后将其放入到scripts文件夹下，因为后续要使用的vov_label.py在该文件夹下，所以会比较方便。然后再依次新建下列文件夹，最终的文件目录如下：123456VOCdevkit ——VOC2019 #文件夹的年份可以自己取————Annotations #放入所有的xml文件————ImageSets ——————Main ————JPEGImages #放入所有的图片文件 &emsp;&emsp;然后我们把图片放入JPEGImages文件夹中，把标注文件放入Annotations文件夹中。其中Main文件夹下需要再新建4个txt文件，分别为：test.txt(测试集)，train.txt(训练集)，val.txt(验证集)，trainval.txt(训练和验证集)，其中训练集和验证集是必须的，所以这里我只新建了2个txt文件。 生成train.txt和val.txt&emsp;&emsp;这两个文本文件存放的就是训练集和验证集中图片的文件名，注意这里只是文件名，不包含后缀。我们可以使用下面的voc_train.py(自己新建，放在scripts文件夹下)生成：123456789101112131415161718192021222324import osfrom os import listdir, getcwdfrom os.path import joinif __name__ == '__main__': source_folder='/home/cxx/Desktop/darknet/scripts/VOCdevkit/VOC2019/JPEGImages/' dest='/home/cxx/Desktop/darknet/scripts/VOCdevkit/VOC2019/ImageSets/Main/train.txt' dest2='/home/cxx/Desktop/darknet/scripts/VOCdevkit/VOC2019/ImageSets/Main/val.txt' file_list=os.listdir(source_folder) train_file=open(dest,'a') val_file=open(dest2,'a') for file_obj in file_list: file_path=os.path.join(source_folder,file_obj) file_name,file_extend=os.path.splitext(file_obj) file_num=int(file_name) if(file_num&lt;150): train_file.write(file_name+'\\n') else : val_file.write(file_name+'\\n') train_file.close()val_file.close() &emsp;&emsp;这里的源文件夹就是刚才放图片的文件夹JPEGImages(路径根据情况自行修改)，目标文件就是我们新建的train.txt和val.txt，然后利用splitext()分离文件名和后缀名。这里面的判断条件file_num&lt;150，可以根据需求自行修改，因为我这里只有300张数据集，所以将前150张作为训练集，后150张作为验证集。&emsp;&emsp;然后运行指令：python voc_train.py即可。打开Main文件夹中的train.txt和val.txt可以看出里面存放着图片的文件名了。(这里奇怪的是，代码中并没有乱序的函数，为啥生成的文件名是乱序的。) 生成2019_train.txt、2019_val.txt、train.txt&emsp;&emsp;有了刚才图片的文件名信息，我们就可以生成Yolo需要的图片路径信息和标注框信息了。这里Yolo的作者提供了相应的python转换代码voc_train.py(在scripts文件夹下)，修改相应信息直接运行即可：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import xml.etree.ElementTree as ETimport pickleimport osfrom os import listdir, getcwdfrom os.path import joinsets=[('2019', 'train'), ('2019', 'val')]classes = [\"stick\"]def convert(size, box): dw = 1./(size[0]) dh = 1./(size[1]) x = (box[0] + box[1])/2.0 - 1 y = (box[2] + box[3])/2.0 - 1 w = box[1] - box[0] h = box[3] - box[2] x = x*dw w = w*dw y = y*dh h = h*dh return (x,y,w,h)def convert_annotation(year, image_id): in_file = open('VOCdevkit/VOC%s/Annotations/%s.xml'%(year, image_id)) out_file = open('VOCdevkit/VOC%s/labels/%s.txt'%(year, image_id), 'w') tree=ET.parse(in_file) root = tree.getroot() size = root.find('size') w = int(size.find('width').text) h = int(size.find('height').text) for obj in root.iter('object'): difficult = obj.find('difficult').text cls = obj.find('name').text if cls not in classes or int(difficult)==1: continue cls_id = classes.index(cls) xmlbox = obj.find('bndbox') b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text)) bb = convert((w,h), b) out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n')wd = getcwd()for year, image_set in sets: if not os.path.exists('VOCdevkit/VOC%s/labels/'%(year)): os.makedirs('VOCdevkit/VOC%s/labels/'%(year)) image_ids = open('VOCdevkit/VOC%s/ImageSets/Main/%s.txt'%(year, image_set)).read().strip().split() list_file = open('%s_%s.txt'%(year, image_set), 'w') for image_id in image_ids: list_file.write('%s/VOCdevkit/VOC%s/JPEGImages/%s.jpg\\n'%(wd, year, image_id)) convert_annotation(year, image_id) list_file.close()os.system(\"cat 2019_train.txt 2019_val.txt &gt; train.txt\") &emsp;&emsp;最后一行的意思是将2个文件合并到一个文件里面。&emsp;&emsp;首先将sets和classes列表中的内容更改为自己的，其中set里面的2019就是刚才新建文件夹的年份，后面的train和val就是Main文件夹中的2个txt文件。classes就是分类的类别，我这里只有1类，就是stick。最后将最后一行的文件夹名字更改一下即可。&emsp;&emsp;然后运行指令：python voc_label.py即可。打开VOC2019文件夹，可以看到里面有1个新的文件夹labels，打开可以发现就是刚才的标注框的信息，即[类别,x,y,w,h]。在scipts文件夹下，可以看到新建了3个txt文件，里面存放的是数据集的绝对路径，待会修改配置文件的时候需要用到。&emsp;&emsp;至此数据集的配置工作终于弄完了，下面就是修改一些配置文件了，相对来说比较轻松。 下载预先权重&emsp;&emsp;在修改配置文件之前，我们先下载一个权重系数文件。这里我们为什么要下载别人的权重系数，不直接自己训练出来呢？因为如果我们不使用别人的，通常的做法是随机生成一个，但我们的网络结构很大，一旦随机的不好，刚开始的时候其效果将会非常不好，导致训练的次数和时间上升，而如果用别人训练好的，虽然检测的目标不一样，但至少比随机产生的要好很多，收敛也会加快。其实这也就是迁移学习的思想。&emsp;&emsp;运行指令wget https://pjreddie.com/media/files/darknet53.conv.74下载即可，如果下载较慢，可以将网址复制到浏览器中下载。下载完放在scripts文件夹中。 修改配置文件&emsp;&emsp;我们一共要修改3个配置文件。 cfg/voc.data&emsp;&emsp;根据目录找到该文件，并打开：12345classes= 1 #classes为训练样本集的类别总数train = /home/cxx/Desktop/darknet/scripts/2019_train.txt #train的路径为训练样本集所在的路径valid = /home/cxx/Desktop/darknet/scripts/2019_train.txt #valid的路径为验证样本集所在的路径names = data/voc.names #names的路径为data/voc.names文件所在的路径 backup = backup &emsp;&emsp;比较简单，修改即可。最好是完整的路径。 data/voc.name&emsp;&emsp;根据目录找到该文件，并打开：1stick #修改为自己样本集的标签名 &emsp;&emsp;比较简单，修改即可。 cfg/yolov3-voc.cfg&emsp;&emsp;根据目录找到该文件，并打开。这里比较复杂，一共需要更改2个部分，共四处地方。&emsp;&emsp;第一部分是[net]部分，就是网络结构的参数。原始文件中batch = 64,subdivision = 16，其含义为每轮迭代会从所有训练集里随机抽取batch = 64个样本参与训练，所有这些 batch个样本又被均分为subdivision = 16次送入网络参与训练，以减轻内存占用的压力。&emsp;&emsp;这看起来很好，但如果显卡不行或者显存不够大的话，训练会很慢甚至崩溃掉，所以电脑不好的可以将这2个值改小一点，注意值最好是2的幂次。&emsp;&emsp;第二部分是softmax层的修改，也就是[yolo]和上一个[convolutional]，一共有3处yolo，每处都同样的改法。&emsp;&emsp;首先是[convolutional]中的filters大小，其计算公式为：3*(classes+5)，这里我的classes为1，所以更改为：3*(1+5)=18，其次是[yolo]中的classes，更改为1。最后的random，如果显存不好，将其设置为0。&emsp;&emsp; 至此，所有的配置都弄好了，下面我们就可以正式训练了。 开始训练&emsp;&emsp;训练指令十分简单，一行命令即可：1./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg scripts/darknet53.conv.74 -gpus 0,1 &emsp;&emsp;注意相应的文件路径，如果是cpu训练的话，就把后面的gpu删掉即可。 配置文件详解及问题再看Yolo——内部原理Unified Detection(统一检测)&emsp;&emsp;首先将输入的图片分割成S*S的网格，然后每个单元格负责去检测那些中心点落在该格子内的目标。这句话比较绕口，可以反过来理解，即如果一个目标的中心点在某个单元格内，那么这个单元格就要负责预测这个目标(数据集都是提前标注好的，事先是知道每个目标的中心点的。)。例如在下图中，狗这个目标的中心落在左下角一个单元格内(第5行第2列)，那么该单元格负责预测这个狗。(对于单元格内没有目标的，当然就不用检测了，这里可以通过后面的公式看出，直接把没目标的单元格过滤掉了。) bounding box&emsp;&emsp;每个单元格一共会预测B个边界框bounding box(下图中间上面的黑色小框)，其中每个bounding box都包含5个confidence score(置信度得分)：[x,y,w,h,confidence]。其中x,y指的是bounding box的中心点坐标，w,h指的是bounding box的宽和高，confidence是置信度，具体由后面的公式计算得出。&emsp;&emsp;例如在YOLOv1中，S和B的取值为7和2，也就是一共有7*7*2=98个bounding box。&emsp;&emsp;其实这个和之前的two-stage目标检测算法，如CNN类似，这里的边界框相当于之前的候选区域，只不过之前是用滑动窗口来选择，这里采用的是1个单元格2个框的思想。因为如果是滑动窗口的话，遍历一张图，需要的窗口太多了，计算量很大，而且窗口都是固定大小，不能随意放缩，而且很多窗口都是冗余的。而yolo的思想就是利用置信度里的x,y,w,h来调节窗口的大小。而且每个单元格都会有相应的边界框，也不至于漏掉某个目标的检测。&emsp;&emsp;在YOLOv1中，采取的做法是1个grid cell(单元格)2个bounding box，然后利用后面的损失函数和网络结构不断地修正bounding box里面的参数，直到和真实框相接近。这样做会带来2个不好的结果：1、位置精确性差，对于小目标物体以及物体比较密集的也检测不好，比如一群小鸟。2、虽然可以降低将背景检测为物体的概率，但同时导致召回率较低。所以在YOLOv2中，针对bounding box做了点改进。 anchor box&emsp;&emsp;YOLOv1是利用全连接层直接预测bounding box的坐标，而YOLOv2借鉴了Faster R-CNN的思想，引入了anchor box(锚点框)。所谓锚点框就是提前设定好几个预测框的大小。&emsp;&emsp;在YOLOv2和YOLOv3中，为了得到更加精细的anchor box，其个数和大小都是由K-means聚类算法计算出来的(具体细节可百度其他博客)，在YOLOv2中k=5，而YOLOv3中k=9，也就是说，在YOLOv3中，会提前设定好9个anchor box的尺寸大小，这个可以在其源代码中看到：110,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326 &emsp;&emsp;每个anchor prior由两个数字组成，一个代表高度另一个代表宽度(当然最后要归一化处理)。这里之所以取9个，是因为v3输出了3个不同尺度的feature map。采用多尺度来对不同尺寸的目标进行检测，越精细的grid cell就可以检测出越精细的物体，每个尺寸3个anchor box，所以一共是9个。如下图左边的框图所示，每个grid cell有9个anchor box(其边框可以超出图像边缘)。&emsp;&emsp;在之前的YOLOv1中，我们一共只有98个预测框，而在YOLOv3中，我们一共有13*13*9=1521(YOLOv3中将网格划分为13*13)。显然，预测框的个数越多，其准确率也就越高。&emsp;&emsp;在YOLOv3中，对anchor box还做了以下的改进： 9个anchor box会被三个输出张量平分的。根据大中小三种size各自取自己的anchor box。 每个输出y在每个自己的网格都会输出3个预测框，这3个框是9除以3得到的，这是作者设置的，我们可以从输出张量的维度来看，13x13x255。255是怎么来的呢，3*(5+80)。80表示80个种类，5表示位置信息和置信度，3表示要输出3个prediction。在代码上来看，3*(5+80)中的3是直接由num_anchors//3得到的。 作者使用了logistic回归来对每个anchor box包围的内容进行了一个目标性评分(objectness score)。根据目标性评分来选择anchor prior进行predict，而不是所有anchor prior都会有输出。 &emsp;&emsp;虽然现在我们已经有了这么多的预测框，但真正需要的并不多，接下来做的就是，从这么多预测框中筛选出我们需要的，然后在不断调整其参数，直至和真实框相接近。 IOU和NMS&emsp;&emsp;在理解这两个概念之前，我们先看一下bounding box里面的第5个参数confidence的计算公式：&emsp;&emsp;第一项的含义就是，如果grid cell里面没有object，则该项为0，那么confidence就是0，如果有，则为1，那么confidence就等于第二项的值。所以如何判断一个grid cell中是否包含object呢？答案是：如果一个object的ground truth(真实框)的中心点坐标在一个grid cell中，那么这个grid cell就是包含这个object，也就是说这个object的预测就由该grid cell负责。(这一步就过滤掉很多bounding box了。)&emsp;&emsp;第二项是IOU，即交并比，如下图所示：&emsp;&emsp;红色框表示真实框，紫色框表示预测框，交并比的含义就是2个框的交集除以2个框的并集，也就是图中黄色部分除以绿色部分。其值为0时，表示没有重叠，为1时表示完全重叠，我们可以设定一个阈值来判断其预测框是否有目标存在(一般设定为0.5，即IOU的值为0.5时，则认为检测到目标了)。&emsp;&emsp;最后每个grid cell还要预测C个类别概率(YOLO是一个多目标检测)，表示一个grid cell在包含object的条件下属于某个类别的概率，所以其最终的计算公式为：&emsp;&emsp;也就是每个bounding box的confidence和每个类别的score相乘，最终得到每个bounding box属于哪一类的confidence score。&emsp;&emsp;但是经过上面几步之后，仍然会存在一些一个目标下很多宽高接近的冗余框，所以最后我们还需要利用NMS(非极大值抑制)去掉重复率较大的bounding box。其原理也十分简单，就是选择IOU最大的那个框。 x,y,w,h&emsp;&emsp;对于YOLO中的bounding box部分，还有最后一部分，就是x,y,w,h的理解。在YOLOv1中，对x,y采取的直接预测，其值并没有加过多的限制，也就是说很可能出现anchor box预测的目标离该单元格很远，这样会导致模型不稳定，特别在早期迭代的时候，需要很长时间才能收敛，所以在YOLOv2中对此做了改进：采用相对预测的方法，即每一个anchor box只负责检测周围正负一个单位以内的目标bounding box。其计算公式如下：&emsp;&emsp;其实就是把预测框的值转换为预测框的实际位置。&emsp;&emsp;黑色虚线框是bounding box，蓝色矩形框就是预测的结果。首先将图像划分成13*13大小的网格，然后每个网格预测5个bounding box，然后每个bounding box预测5个值：tx,ty,tw,th,to(to就是confidence)。cx和cy表示cell和图像左上角的横纵距离，为了保证之前说的每一个anchor box只能预测该单元格内的目标，tx和ty都经过了sigmoid函数处理，这样其值的范围就在0到1之间，这样的归一化处理也使得模型训练更加稳定，当然最后的实际位置要加上之前的横纵距离；pw和ph表示bounding box的宽高，为了保证预测的和实际的之间是一种放缩关系，这里加入了e的指数次方，以保证后面的系数是大于0的。&emsp;&emsp;注：这里采用的思想就是通过平移和放缩来实现预测框和实际框的不断逼近。 总结&emsp;&emsp;总结一下，我对YOLO边界框的理解就是：先用Kmeans人为的设定9个anchor box，然后将输入图片分为S*S个网格，每个网络都将这9个anchor box遍历一遍，然后通过置信度计算公式筛选出最好的边界框，最后通过卷积网络和损失函数不断地训练学习，校正置信度里面(x,y,w,h)四个参数，以得到最好地边框效果。 网络结构&emsp;&emsp;YOLO从v1到v3，其网络结构也经历了较大的变化，从最开始的GoogLeNet到Darknet 53，模型变得越来越复杂，但其运行时间和精度确越来越好。 YOLOv1&emsp;&emsp;v1的网络结构主要是采用GoogLeNet模型，卷积层提取特征，全连接层预测类别概率和坐标。最后输出的结果是7*7*30。但v1稍微对里面的一些细节做了改进： 将Inception Module替换成1*1和3*3的卷积 v1一共有24层卷积，2个全连接层。 使用Leaky Relu作为激活函数。 在第一个全连接层后面加上一个ratio=0.5的Dropout层。&emsp;&emsp;训练时，首先利用ImageNet 1000-class的数据集预先训练v1网络中的前20个卷积层和一个平均池化层，最后再加一个全连接层。输入图像的大小为224*224。正是训练时，再采用v1网络，同时输入改为448*448。 YOLOv2&emsp;&emsp;YOLOv2的全名为YOLO9000：Better,Faster,Stronger，无论是精度还是速度上都得到了很大的提升。&emsp;&emsp;这是论文原文里面列出来的YOLOv2针对v1的改进，其中大部分都是针对网络结构的改进。 Batch Normalization&emsp;&emsp;在每一个卷积层后添加batch normalization。 High Resolution Classifier&emsp;&emsp;之前的预训练的输入图像大小是先224*224，然后再448*448,现在的做法是，将输入大小改成448*448，先在ImageNet数据集上训练10轮，这样训练后的网络就可以适应高分辨率的输入了。 New Network&emsp;&emsp;YOLOv2使用了一个新的分类网络作为特征提取部分，作者使用了较多的3*3卷积核，在每一次池化操作后把通道数翻倍。借鉴了network in network的思想，网络使用了全局平均池化，把1*1的卷积核置于3*3的卷积核之间，用来压缩特征。也用了batch normalization稳定模型训练。&emsp;&emsp;最终得出的基础模型就是Darknet-19，如下图，其包含19个卷积层、5个最大值池化层。 Loss Function&emsp;&emsp;从YOLOv1到YOLOv3中，损失函数几乎没有怎么改变，最多就是把其中几个的均方误差改成了交叉熵形式。&emsp;&emsp;损失函数如上图所示，里面包含3个部分，即坐标预测、bbox预测和类别预测。 坐标预测(坐标损失)&emsp;&emsp;坐标预测为公式中的前两行，第一行是box中心坐标(x,y)的预测，第二行为宽和高的预测。这里是用宽和高的平方根来代替原来的宽和高，这样做主要是因为，相同的宽和高误差，对于小的目标精度影响比大的目标要大。&emsp;&emsp;如上图所示，有2组坐标。第一个是不加平方根的，其误差值都一样，但对于加了平方根的，明显小目标的误差更大一点。&emsp;&emsp;这里前面的λ和1系数见下文分析。 bbox预测(confidencd损失)&emsp;&emsp;bbox预测为公式中的第三、四行。第三行为含有object时的置信度(confidence)预测，第四行为不含有时的预测。这里的bbox预测和之前的坐标预测，两者的λ系数是不一样的。&emsp;&emsp;因为很多grid cell是不包含物体的，这样的话很多grid cell的confidence score为0，如果权值一致，容易导致模型不稳定，训练发散。所以可以采用设置不同权重方式来解决，一方面提高坐标预测的权重，另一方面降低没有object的box的confidence loss权值，论文中将这2个权重系数分别设为5和0.5。而对于包含object的box权重系数还是原来的1。 类别预测(分类损失)&emsp;&emsp;第五行表示预测类别的误差，注意前面的系数只有在grid cell包含object的时候才为1。 实现过程&emsp;&emsp;以YOLOv1为例，输入N个图像，每个图像包含M个object，每个object包含4个坐标(x，y，w，h)和1个label。&emsp;&emsp;然后通过网络得到7*7*30大小的三维矩阵。每个1*30的向量前5个元素表示第一个bounding box的4个坐标和1个confidence，后5个元素表示第二个bounding box的4个坐标和1个confidence。最后20个表示这个grid cell所属类别。这30个都是预测的结果，也就是都是网络生成的值。&emsp;&emsp;然后就可以计算损失函数的第一、二 、五行，这一部分比较好理解，x,y,w,h都是输入值，是已知的，前面的1_ij^obj指的是判断第i个网格中第j个bbox是否负责这个object，那怎么判断呢？与object的ground truth box的IOU最大的bbox负责该object，也就是说IOU最大时，此系数为1，否则为0。第五行指的是类别概率，也是输入值，是已知的。其前面的系数1_i^obj表示判断是否有object的中心落在网格i中，也就是说有object时，此系数为1，否则为0 。&emsp;&emsp;比较难理解的是confident的损失。预测的confidence可以根据ground truth和预测的bounding box计算出的IOU，和是否有object的0,1值相乘得到(见上文分析)。而真实的confidence是0或1值，即有object则为1，没有object则为0。(1表示此时IOU最大，也就是真实框) 总结&emsp;&emsp;综上所述，并不是网络的所有输出都要计算loss的，具体地说： 有物体中心落入的grid cell，需要计算分类损失，两个预测框bounding box都要计算置信度损失，预测的bounding box与groud truth中IOU较大的那个预测框bounding box需要计算xywh损失。 最关键的部分，没有物体中心落入的grid cell，只需要计算置信度(confidence)损失。 另一种实现YOLO的框架——Keras安装环境&emsp;&emsp;首先我们需要安装Tensor Flow框架(直接pip安装即可)，如果显卡较好，可以安装GPU版本，具体安装过程这里就不叙述了。其次需要安装Keras框架(直接pip安装即可)。Keras是一个由Python编写的开源人工神经网络库，可以作为Tensorflow、Microsoft-CNTK和Theano的高阶应用程序接口，进行深度学习模型的设计、调试、评估、应用和可视化。最后再下载Keras-YOLO的源代码即可，GitHub网站。 文件结构 font：字体目录(不知道干啥用的)。 model_data：模型数据，主要存放数据集类别信息和anchor的大小。可以更改成自己的数据集信息。 yolo3：里面有2个py文件，一个是model.py，就是构建yolo3的主要模块文件，包含网络结构，损失函数等，另一个是utils.py，主要是一些model.py用到的辅助性的工具函数。 coco_annoatation.py：将.json文件转换为txt文件，voc_annoatation.py：将.xml文件转换为txt文件。最后生成的txt文件包括训练的图片的路径信息、标注框信息和类别信息。 convert.py：把darknet的.weights权重转换为keras的.h5权重文件。 kmeans.py：通过聚类得到数据最佳的anchors。 train.py：训练yolov3的文件。 yolo.py：构建以yolov3为底层构件的yolo检测模型，因为上面的yolov3还是分开的单个函数，功能并没有融合在一起，即使在训练的时候所有的yolov3组件还是分开的功能，并没有统一接口，供在模型训练完成之后，直接使用。通过yolo.py融合所有的组件。 yolo_video.py：使用yolo.py文件中的yolo检测模型，并且对视频或图像中的物体进行检测。 yolov3.cfg，yolov3-tiny.cfg 构建yolov3或yolov3-tiny检测模型的整个超参文件。 快速使用 首先需要下载一个权重文件：网址，然后将权重放在主文件夹下。 执行如下命令将darknet下的yolov3配置文件转换成keras适用的h5文件： 1python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 &emsp;&emsp;注：可能需要输入python3&emsp;&emsp;运行完会在model_data文件夹下生成yolo.h5文件。 运行预测图像程序1python yolo_video.py --image &emsp;&emsp;然后根据提示输入图片位置：&emsp;&emsp;其结果如下所示： 训练自己的数据集创建数据集文件夹VOCdevkit&emsp;&emsp;在最外面的文件夹下新建一个VOCdevkit文件夹，这里的VOCdevkit文件夹和darknet框架下的是一样的，这里就不再叙述了。&emsp;&emsp;这里提供另一个生成随机数据集的py文件：123456789101112131415161718192021222324252627282930313233343536import osimport random trainval_percent = 0.2train_percent = 0.8xmlfilepath = 'Annotations'txtsavepath = 'ImageSets\\Main'total_xml = os.listdir(xmlfilepath) num = len(total_xml)list = range(num)tv = int(num * trainval_percent)tr = int(tv * train_percent)trainval = random.sample(list, tv)train = random.sample(trainval, tr) ftrainval = open('ImageSets/Main/trainval.txt', 'w')ftest = open('ImageSets/Main/test.txt', 'w')ftrain = open('ImageSets/Main/train.txt', 'w')fval = open('ImageSets/Main/val.txt', 'w') for i in list: name = total_xml[i][:-4] + '\\n' if i in trainval: ftrainval.write(name) if i in train: ftest.write(name) else: fval.write(name) else: ftrain.write(name) ftrainval.close()ftrain.close()fval.close()ftest.close() &emsp;&emsp;将其放在JPEGImages文件夹的同级目录下，运行后，会在Main文件夹下生成4个txt文件，里面存放着随机的图片名字。&emsp;&emsp;这样我们的VOC数据集就制作完成了。 生成train.txt，val.txt和test.txt&emsp;&emsp;和Darknet一样，这些文件主要包含每张图片的位置信息、标记框的信息和类别信息。打开voc_annotation.py文件，修改sets和classes列表里面的信息，运行即可。&emsp;&emsp;运行结束后会在YOLO的源代码文件夹下产生这些文件。 修改配置文件&emsp;&emsp;这里一共需要修改2个配置文件信息，第一个是model_data文件夹里面的voc_class.txt文件，修改为自己的类别即可。第二个是最外面的文件夹中的训练文件train.py，将_main()函数里面的文件路径更改为自己的即可，注意这里的log日志文件夹需要自己新建。在训练前可以先运行kmeans.py生成最好的anchor box(注意修改里面的文件路径)，然后将其放入到model_data文件夹中。&emsp;&emsp;然后直接运行train.py即可训练。 训练&emsp;&emsp;整个训练过程分两次，每次50次迭代。第二次迭代时，如果Loss几乎没有变化会提前终止。 结果分析&emsp;&emsp;训练完之后，我们还可以计算每个类别的平均精度等信息，即AP和mAP等。 下载源代码&emsp;&emsp;首先需要下载2个源代码，网址1，网址2。下载完成后，将里面的内容全部放到YOLO的源代码文件夹中： 生成groundtruths&emsp;&emsp;首先生成真实框的信息，即：&lt;class_name&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt; [&lt;difficult&gt;]。&emsp;&emsp;先将class_list.txt里面的内容换成自己的类别信息(我这里用的是VOC2007数据集)：&emsp;&emsp;然后运行下面的命令即可：1python convert_keras-yolo3.py --gt test.txt &emsp;&emsp;convert_keras-yolo3.py就是刚才下载的转换文件，test.txt为测试集的txt文件，(我这里是2019_test.txt)，运行结束后，会在from_kerasyolo3文件夹中看到每张图片的真实框信息。(注，可能需要换成python3)。&emsp;&emsp;然后将里面所有的txt文件都复制到input-&gt;ground-truth文件夹里面。 生成detections&emsp;&emsp;其次生成预测框的信息，即：&lt;class_name&gt; &lt;confidence&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt;。&emsp;&emsp;首先更改config.yml里面的信息：&emsp;&emsp;前面4个信息就是一些文件的路径，model_name指的是用的是哪个模型训练的(yolo3文件夹里面的model.py里面的模型)，我这里是tiny_yolo_body，当然也可以是自己的模型(此处的模型需要和训练时候的模型一致)。log_dir指的是权重文件的路径，后面2个可以不用改。&emsp;&emsp;然后将刚才下载的文件夹里面的model.py文件替换掉YOLO原始源代码里面的yolo3文件夹里面的model.py。&emsp;&emsp;然后再运行下面的命令：1python yolo_new.py -g config.yml --weights logs/000/xx.h5 &emsp;&emsp;yolo_new.py就是刚才下载的转换文件，xx.h5就是权重文件。(可能要装一个第三方库)&emsp;&emsp;程序会显示一共用了多少时间和每个图片预测的时间。&emsp;&emsp;程序运行结束后，会在源代码文件夹中生成一个名字很长的txt文件，将其重命名为pred.txt。 &emsp;&emsp;最后运行下面的命令即可：1python convert_keras-yolo3.py --pred pred.txt &emsp;&emsp;运行结束后，会在from_kerasyolo3文件夹中看到每张图片的预测框信息：&emsp;&emsp;然后将里面所有的txt文件都复制到input-&gt;detection_results文件夹里面。 运行main.py&emsp;&emsp;在运行main.py之前，还需要将测试集的图片放到input-&gt;images-optional里面，虽然这一步原作者写的是可选，但如果不做的话程序会报错。(为了方便，可以把所有数据集都拷过来)&emsp;&emsp;最后就是运行main.py了，如果装了Opencv的话，会看到很酷的动画。&emsp;&emsp;最后会显示每个类别的AP值和mAP值。 另一种实现YOLO的框架——PyTorch安装环境&emsp;&emsp;这里详见我的另一篇博客PyTorch的简单使用 快速使用 首先从Github网站上下载PyTorch的源代码：网址。 下载Yolov3的权重文件：网址，然后将权重放在weights文件夹下。 将待检测图片放入到data/samples文件夹下。 运行检测程序 1python3 detect.py --cfg cfg/yolov3.cfg --weights weights/yolov3.weights &emsp;&emsp;在生成的output文件夹下会看到检测结果： 训练自己的数据集创建数据集&emsp;&emsp;将自己的数据集JPEGImages和标注文件Annotations放到data目录下，并新建文件ImageSets，labels文件夹，复制JPEGImages，重命名images。最终的文件目录格式为：123456data Annotations images ImageSets JPEGImages labels 生成train.txt，val.txt和test.txt&emsp;&emsp;首先在最外面的文件夹(yolov3)下创建2个py文件：&emsp;&emsp;create_txt.py：123456789101112131415161718192021222324252627282930313233343536import osimport randomtrainval_percent = 0.1train_percent = 0.9xmlfilepath = 'data/Annotations'txtsavepath = 'data/ImageSets'total_xml = os.listdir(xmlfilepath)num = len(total_xml)list = range(num)tv = int(num * trainval_percent)tr = int(tv * train_percent)trainval = random.sample(list, tv)train = random.sample(trainval, tr)ftrainval = open('data/ImageSets/trainval.txt', 'w')ftest = open('data/ImageSets/test.txt', 'w')ftrain = open('data/ImageSets/train.txt', 'w')fval = open('data/ImageSets/val.txt', 'w')for i in list: name = total_xml[i][:-4] + '\\n' if i in trainval: ftrainval.write(name) if i in train: ftest.write(name) else: fval.write(name) else: ftrain.write(name)ftrainval.close()ftrain.close()fval.close()ftest.close() &emsp;&emsp;voc_label.py：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# -*-coding:utf-8-*-import xml.etree.ElementTree as ETimport pickleimport osfrom os import listdir, getcwdfrom os.path import joinsets = ['train', 'test', 'val']classes = [\"stick\"]def convert(size, box): dw = 1. / size[0] dh = 1. / size[1] x = (box[0] + box[1]) / 2.0 y = (box[2] + box[3]) / 2.0 w = box[1] - box[0] h = box[3] - box[2] x = x * dw w = w * dw y = y * dh h = h * dh return (x, y, w, h)def convert_annotation(image_id): in_file = open('data/Annotations/%s.xml' % (image_id), encoding='utf-8') out_file = open('data/labels/%s.txt' % (image_id), 'w', encoding='utf-8') tree = ET.parse(in_file) root = tree.getroot() size = root.find('size') w = int(size.find('width').text) h = int(size.find('height').text) for obj in root.iter('object'): difficult = obj.find('difficult').text cls = obj.find('name').text if cls not in classes or int(difficult) == 1: continue cls_id = classes.index(cls) xmlbox = obj.find('bndbox') b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text)) bb = convert((w, h), b) out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n')wd = getcwd()print(wd)for image_set in sets: if not os.path.exists('data/labels/'): os.makedirs('data/labels/') image_ids = open('data/ImageSets/%s.txt' % (image_set), encoding='utf-8').read().strip().split() list_file = open('data/%s.txt' % (image_set), 'w') for image_id in image_ids: list_file.write('data/images/%s.jpg\\n' % (image_id)) convert_annotation(image_id) list_file.close() &emsp;&emsp;voc_label.py中的classes列表改为自己的类别。&emsp;&emsp;运行完create_txt.py后，会在ImageSets文件夹中生成几个txt，里面存放的数据集的名字。运行完voc_label.py后，会在data文件夹中生成所需要的train.txt，test.txt，val.txt，这里存放的都是数据集的路径。 修改配置文件&emsp;&emsp;这里一共需要修改3个配置文件信息，首先是data文件夹里面的coco.names和coco.data。打开coco.names，然后输入自己的类别即可。然后打开coco.data，更改里面的配置信息：&emsp;&emsp;然后打开cfg文件夹下，选择想要训练的模型，并更改相应的参数，可以参照上文的darknet框架部分。&emsp;&emsp;附：配置文件解析 训练&emsp;&emsp;训练之前需要先下载预训练权重，并将其放入weights文件夹下，权重下载地址。注：tiny版本的权重为yolov3-tiny.conv.15。或百度云链接，提取码：jvwg。&emsp;&emsp;然后切换至主文件夹目录下，运行命令即可：1python train.py --data data/coco.data --cfg cfg/yolov3-tiny.cfg &emsp;&emsp;注：最近的官网源代码对train.py进行了更新，增加了权重文件的参数设置，所以运行命令更改为：1python train.py --data data/coco.data --cfg cfg/yolov3-tiny.cfg --weights weights/yolov3-tiny.conv.15 &emsp;&emsp;注：Ubuntu系统下可能为python3。 &emsp;&emsp;训练过程如图所示： 预测&emsp;&emsp;训练结束后，会在weights文件夹下生成很多权重文件，其中best.pt就是最好的权重。首先将需要检测的图片放到data文件夹下的samples文件夹下，然后切换至主文件夹下，运行以下命令：1python detect.py --data data/coco.data --cfg cfg/yolov3-tiny.cfg --weights weights/best.pt &emsp;&emsp;注：最近的官网源代码对detect.py进行了更新，将data参数设置改为了names参数设置，所以运行命令更改为：1python detect.py --names data/coco.names --cfg cfg/yolov3-tiny.cfg --weights weights/best.pt &emsp;&emsp;运行结束后，会在主文件夹下生成output文件夹，里面存放着预测结果。 结果分析&emsp;&emsp;训练结束后，除了生成权重文件外，还生成了results.txt文本文件，该文件记录了刚才训练过程中的日志信息，只需执行一条简单的命令就可以将其可视化。1python -c from utils import utils;utils.plot_results() &emsp;&emsp;运行结束后，会在主文件夹下生成一个result.png。&emsp;&emsp;注：不知道为啥Classification没有显示…","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://cxx0822.github.io/categories/深度学习/"}],"tags":[]},{"title":"基于卷积神经网络的猫狗分类器","slug":"基于卷积神经网络的猫狗分类器","date":"2019-06-04T00:26:56.000Z","updated":"2019-08-04T01:38:56.502Z","comments":true,"path":"2019/06/04/基于卷积神经网络的猫狗分类器/","link":"","permalink":"http://cxx0822.github.io/2019/06/04/基于卷积神经网络的猫狗分类器/","excerpt":"","text":"摘要&emsp;&emsp;Cats vs. Dogs（猫狗大战）是Kaggle大数据竞赛某一年的一道赛题，利用给定的数据集，用算法实现猫和狗的识别。本博客设计了一个简单的两层卷积神经网络，利用TensorFlow深度学习框架进行模型训练，最终得到一个简单的二分类器。&emsp;&emsp;本博客从数据集开始讲起，然后通过模型搭建、模型训练和模型测试依次讲解深度学习目标检测常用的步骤，并穿插着TensorFlow框架的基础知识讲解，语言通俗易懂，十分适合刚入门的初学者学习。 平台&emsp;&emsp;系统：Windows 10&emsp;&emsp;环境：python 3.5.4&emsp;&emsp;编译器：Visual Studio Code 数据集处理获取数据集&emsp;&emsp;我们可以使用Kaggle官网上提供的数据集，读者可以在GitHub上下载，网址，或者从百度云下载：data，提取码：z9kn。当然了，我们也可以自己拍摄或采集一些数据集，为了保持和官网数据集命名格式的一致性，可以使用python的os库对自己采集的数据集进行批量重命名(如果是用官网的数据集，可以先跳过这一部分)。 123456789101112131415161718import osdef rename_files(dir_path): \"\"\" 批量重命名文件 参数： dirPath：文件路径 \"\"\" file_list = os.listdir(dir_path) index = 0 for item in file_list: oldname = dir_path + r\"\\\\\" + file_list[index] newname = dir_path + r\"\\\\\" + \"dog.\" + str(index) + \".jpg\" os.rename(oldname, newname) index += 1 &emsp;&emsp;首先利用os.listdir()从文件夹中获取所有的文件名，返回值file_list依次存放着每个文件的名字。然后利用for循环依次遍历列表，最后利用os.rename()对其重命名。(如果需要重命名猫的照片，将dog替换成cat即可。) 123if __name__ == \"__main__\": image_dir = r\"D:\\\\TensorFlow\\\\dog_and_cat\\\\test\" renameFiles(image_dir) &emsp;&emsp;例如我在D:\\\\TensorFlow\\\\dog_and_cat\\\\test文件夹中存放着采集的数据集，调用rename_files()函数后，可以看到所有的文件名字已经全部重命名。 打乱数据集&emsp;&emsp;为了提高数据集的鲁棒性和防止采集数据时的人为性，我们可以先将数据集随机打乱，至于是读取数据时还是训练时打乱，读者可以自己选择。本博客采用读取数据时打乱。 123456789101112131415161718192021222324252627282930313233343536373839404142434445def get_all_files(file_path, is_random=True): \"\"\" 获取图片路径及其标签 参数： file_path: a string 图片所在目录 file_path：true or flase 是否乱序 返回值： imageList：a list 图像列表 lableList：a list 标签列表 \"\"\" image_list, label_list = [], [] cat_count, dog_count = 0, 0 # 从文件夹中读取文件名字 for item in os.listdir(file_path): # listdir()：返回指定路径下的文件和文件夹列表 item_path = file_path + '\\\\' + item item_label = item.split('.')[0] if os.path.isfile(item_path): # isfile()：判断某一路径是否为文件 image_list.append(item_path) else: raise ValueError('no file.') if item_label == 'cat': label_list.append(0) cat_count += 1 else: label_list.append(1) dog_count += 1 print(\"There are %d cats, %d dogs.\" % (cat_count, dog_count)) image_list = np.asarray(image_list) # 当数据源是ndarray时，array会copy出一个副本，占用新的内存，但asarray不会 label_list = np.asarray(label_list) if is_random: rnd_index = np.arange(len(image_list)) np.random.shuffle(rnd_index) # 将文件乱序 # shuffle()：将序列的所有元素随机排序 image_list = image_list[rnd_index] label_list = label_list[rnd_index] return image_list, label_list &emsp;&emsp;首先初始化存储数据和标签的列表，然后依次遍历文件夹并读取名字，如果是文件则将文件名存储到image_list列表中，并将文件名的第一个参数存储到label_list列表中，然后分别统计猫和狗的数量。最后我们利用numpy中的shuffle()函数将列表打乱。 1234if __name__ == \"__main__\": image_dir = r\"D:\\\\TensorFlow\\\\dogAndCat\\\\test\" train_list = get_all_files(image_dir) print(train_list) &emsp;&emsp;这里我们采用10张猫和10张狗的照片进行了一个简单的测试，其结果如图所示：&emsp;&emsp;可以看出所有的数据集都已经全部打乱。 数据集分批次处理&emsp;&emsp;这里的处理主要涉及到两个方面，第一，分批次获取数据集，因为一次性将所有25000张图片载入内存不现实也不必要，所以将图片分成不同批次进行训练，第二，由于采集的数据集大小并不统一，所以很有必要先将其调整到一个统一的大小。&emsp;&emsp;在讲解如何分批处理前，首先要知道TensorFlow是如何读取数据的。 tensorflow读取数据机制&emsp;&emsp;TensorFlow中为了充分利用GPU，减少GPU等待数据的空闲时间，使用了两个线程分别执行数据读入和数据计算。具体来说就是使用一个线程源源不断的将硬盘中的图片数据读入到一个内存队列中，另一个线程负责计算任务，所需数据直接从内存队列中获取。&emsp;&emsp;TensorFlow在内存队列之前，还设立了一个文件名队列，文件名队列存放的是参与训练的文件名，要训练N个epoch(1个epoch等于使用训练集中的全部样本训练一次)，则文件名队列中就含有N个批次的所有文件名。如图所示：&emsp;&emsp;在N个epoch的文件名最后是一个结束标志，当TensorFlow读到这个结束标志的时候，会抛出一个OutofRange 的异常，外部捕获到这个异常之后就可以结束程序了。&emsp;&emsp;而创建TensorFlow的文件名队列就需要使用到 tf.train.slice_input_producer()函数。 tf.train.slice_input_producer()&emsp;&emsp;tf.train.slice_input_producer()是一个tensor生成器，作用是按照设定，每次从一个tensor列表中按顺序或者随机抽取出一个tensor放入文件名队列。&emsp;&emsp;其函数头为：1slice_input_producer(tensor_list, num_epochs=None, shuffle=True, seed=None, capacity=32, shared_name=None, name=None) &emsp;&emsp;tensor_list是输入，格式为tensor的列表；一般为[data, label]，即由特征和标签组成的数据集，num_epochs是抽取batch(批次)的次数，如果没有给定值，那么将会抽取无数次batch(这会导致你训练过程停不下来)，如果给定值，那么在到达次数之后就会报OutOfRange的错误，shuffle是是否随机打乱，seed是随机种子，capcity是队列容量的大小，为整数，name是名称。&emsp;&emsp;其返回值为tensor的列表，其结果和tensor_list一致。例如将之前的train_list作为输入，其结果显示为：12345if __name__ == \"__main__\": image_dir = r\"D:\\\\TensorFlow\\\\dog_and_cat\\\\test\" train_list = get_all_files(image_dir, is_random=False) intput_queue = tf.train.slice_input_producer(train_list, shuffle=False) print(intput_queue) &emsp;&emsp;结果为：[&lt;tf.Tensor &#39;input_producer/GatherV2:0&#39; shape=() dtype=string&gt;, &lt;tf.Tensor &#39;input_producer/GatherV2_1:0&#39; shape=() dtype=int32&gt;]，其中第一个就是对应的image_list的向量，第二个为label_list。&emsp;&emsp;有了队列之后就可以使用tf.train.batch()或tf.train.shuffle_batch()来生成批次大小为batch_size的tensor。 tf.train.batch()和tf.train.shuffle_batch()&emsp;&emsp;其函数头为：1tf.train.batch([data, label], batch_size=batch_size, capacity=capacity,num_threads=num_thread,allow_smaller_final_batch=True) 1tf.train.shuffle_batch([data, label], batch_size=batch_size, capacity=capacity,num_threads=num_thread,allow_smaller_final_batch=True) &emsp;&emsp;[data，label]是输入的样本和标签，batch_size是batch的大小，capcity是队列的容量，num_threads是线程数，使用多少个线程来控制整个队列，allow_smaller_final_batch这个是当最后的几个样本不够组成一个batch的时候用的参数，如果为True则会重新组成一个batch。这2个区别在于一个是顺序产生，一个是随机产生(有shuffle是随机产生)。 batch取值&emsp;&emsp;这里比较重要的一个参数是batch的取值，batch_size(批尺寸)是机器学习/深度学习中一个重要参数。其含义及取值可以参考这篇博客：网址&emsp;&emsp;本博客采用的取值为batch_size=1，即每次只训练一个样本，也就是在线学习(Online Learning)。这也是Stochastic Gradient Descent(SGD，随机梯度下降算法)的更新规则，即：一次只进行一次更新，就没有冗余，而且比较快，并且可以新增样本。 生成数据集12345678910111213141516171819202122232425262728293031323334353637def get_batch(train_list, image_size, batch_size, capacity, is_random=True): \"\"\" 分批次获取图片 参数： train_list: 2-D list, [image_list, label_list] image_size: a int, 训练图像大小(image_size*image_size) batch_size: a int, 每个批次包含的样本数量 capacity: a int, 队列容量 is_random: True or False, 是否乱序 返回值： image_train_batch：图像批次 label_train_batch：标签批次 \"\"\" intput_queue = tf.train.slice_input_producer(train_list, shuffle=False) # 从路径中读取图片 image_train = tf.read_file(intput_queue[0]) image_train = tf.image.decode_jpeg(image_train, channels=3) # 这里是jpg格式 image_train = tf.image.resize_images(image_train, [image_size, image_size]) image_train = tf.cast(image_train, tf.float32) / 255. # 转换数据类型并归一化 # 图片标签 label_train = intput_queue[1] # 获取批次 if is_random: image_train_batch, label_train_batch = tf.train.shuffle_batch([image_train, label_train], batch_size=batch_size, capacity=capacity, min_after_dequeue=100, num_threads=2) else: image_train_batch, label_train_batch = tf.train.batch([image_train, label_train], batch_size=batch_size, capacity=capacity, num_threads=1) return image_train_batch, label_train_batch &emsp;&emsp;获取到图片队列后，首先用read_file()读取图片，然后按照图片格式进行解码。本博客中训练数据是jpg格式的，所以使用decode_jpeg()解码器，如果是其他格式，就要用其他解码器。注意decode出来的数据类型是uint8，之后模型卷积层里面conv2d()要求输入数据为float32类型，所以如果删掉标准化步骤之后，需要进行类型转换。最后还需要将图片裁剪成相同大小(img_W和img_H)。这里使用resize_images()对图像进行缩放，而不是裁剪，采用NEAREST_NEIGHBOR插值方法。标签队列比较简单，直接获取即可。然后在利用tf.train.batch()将其分批次处理。&emsp;&emsp;同样的我们进行简单的测试： if __name__ == &quot;__main__&quot;: image_dir = r&quot;D:\\\\TensorFlow\\\\dog_and_cat\\\\test&quot; train_list = get_all_files(image_dir, is_random=False) image_train_batch, label_train_batch = get_batch(train_list, 256, 1, 200, False) print(image_train_batch, label_train_batch) &emsp;&emsp;这里我们将batch_size设置为1，也就是一次取一张照片，其输出结果为：Tensor(&quot;batch:0&quot;, shape=(1, 256, 256, 3), dtype=float32) Tensor(&quot;batch:1&quot;, shape=(1,), dtype=int32)。 数据集可视化&emsp;&emsp;之前的叙述都只是搭建模型，下面我们可以启动TensorFlow会话将图片显示出来。这里需要使用tf.train.Coordinator()来创建一个线程管理器(协调器)对象。 if __name__ == &quot;__main__&quot;: import matplotlib.pyplot as plt image_dir = r&quot;D:\\\\TensorFlow\\\\dog_and_cat\\\\test&quot; train_list = get_all_files(image_dir, is_random=False) print(train_list) image_train_batch, label_train_batch = get_batch(train_list, 250, 1, 250, False) sess = tf.Session() coord = tf.train.Coordinator() # 创建一个线程管理器(协调器)对象 threads = tf.train.start_queue_runners(sess=sess, coord=coord) # 启动tensor的入队线程 try: for step in range(10): if coord.should_stop(): break image_batch, label_batch = sess.run([image_train_batch, label_train_batch]) # 返回列表的值 if label_batch[0] == 0: label = &#39;Cat&#39; else: label = &#39;Dog&#39; # 显示图片 plt.imshow(image_batch[0]) plt.title(label) plt.show() except tf.errors.OutOfRangeError: print(&#39;Done.&#39;) finally: coord.request_stop() coord.join(threads=threads) sess.close() &emsp;&emsp;在讲解这部分代码之前，我们首先要简单了解一下TensorFlow的多线程概念。&emsp;&emsp;TensorFlow的Session对象是支持多线程的，可以在同一个会话(Session)中创建多个线程，并行执行。在Session中的所有线程都必须能被同步终止，异常必须能被正确捕获并报告，会话终止的时候， 队列必须能被正确地关闭。&emsp;&emsp;TensorFlow提供了两个类来实现对Session中多线程的管理：tf.Coordinator和tf.QueueRunner，这两个类往往一起使用。Coordinator类用来管理在Session中的多个线程，可以用来同时停止多个工作线程并且向那个在等待所有工作线程终止的程序报告异常，该线程捕获到这个异常之后就会终止所有线程。使用tf.train.Coordinator()来创建一个线程管理器(协调器)对象。&emsp;&emsp;QueueRunner类用来启动tensor的入队线程，可以用来启动多个工作线程同时将多个tensor(训练数据)推送入文件名称队列中，具体执行函数是tf.train.start_queue_runners，只有调用 tf.train.start_queue_runners之后，才会真正把tensor推入内存序列中，供计算单元调用，否则会由于内存序列为空，数据流图会处于一直等待状态。 &emsp;&emsp;结合上述理论，我们再次了解一下TensorFlow的数据读取机制： 调用 tf.train.slice_input_producer，从本地文件里抽取tensor，准备放入Filename Queue（文件名队列）中; 调用 tf.train.batch，从文件名队列中提取tensor，使用单个或多个线程，准备放入文件队列; 调用 tf.train.Coordinator() 来创建一个线程协调器，用来管理之后在Session中启动的所有线程; 调用tf.train.start_queue_runners, 启动入队线程，由多个或单个线程，按照设定规则，把文件读入Filename Queue中。函数返回线程ID的列表，一般情况下，系统有多少个核，就会启动多少个入队线程（入队具体使用多少个线程在tf.train.batch中定义）; 文件从 Filename Queue中读入内存队列的操作不用手动执行，由tf自动完成; 调用sess.run 来启动数据出列和执行计算; 使用 coord.should_stop()来查询是否应该终止所有线程，当文件队列（queue）中的所有文件都已经读取出列的时候，会抛出一个OutofRangeError 的异常，这时候就应该停止Sesson中的所有线程了; 使用coord.request_stop()来发出终止所有线程的命令，使用coord.join(threads)把线程加入主线程，等待threads结束。 &emsp;&emsp;有了上述2个理论基础，我们就可以理解之前的代码了，首先启动会话，然后启动线程管理器，然后将之前的训练数据依次放入队列中，这里加了一个图片显示，主要是利用matplotlib这个库，最后在依次终止所有的线程和会话。&emsp;&emsp;这是其中的一张图片的显示结果。 模型搭建&emsp;&emsp;有了数据集后，我们就可以正式搭建卷积神经网络的结构了。这里主要是仿照TensorFlow的官方例程cifar-10网络结构来写的。就是两个卷积层（每个卷积层后加一个池化层），两个全连接层，最后一个softmax输出分类结果。 import tensorflow as tf def inference(images, batch_size, n_classes): # conv1, shape = [kernel_size, kernel_size, channels, kernel_numbers] with tf.variable_scope(&quot;conv1&quot;) as scope: weights = tf.get_variable(&quot;weights&quot;, shape=[3, 3, 3, 16], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32)) biases = tf.get_variable(&quot;biases&quot;, shape=[16], dtype=tf.float32, initializer=tf.constant_initializer(0.1)) conv = tf.nn.conv2d(images, weights, strides=[1, 1, 1, 1], padding=&quot;SAME&quot;) pre_activation = tf.nn.bias_add(conv, biases) conv1 = tf.nn.relu(pre_activation, name=&quot;conv1&quot;) # pool1 &amp;&amp; norm1 with tf.variable_scope(&quot;pooling1_lrn&quot;) as scope: pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=&quot;SAME&quot;, name=&quot;pooling1&quot;) norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=&#39;norm1&#39;) # conv2 with tf.variable_scope(&quot;conv2&quot;) as scope: weights = tf.get_variable(&quot;weights&quot;, shape=[3, 3, 16, 16], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32)) biases = tf.get_variable(&quot;biases&quot;, shape=[16], dtype=tf.float32, initializer=tf.constant_initializer(0.1)) conv = tf.nn.conv2d(norm1, weights, strides=[1, 1, 1, 1], padding=&quot;SAME&quot;) pre_activation = tf.nn.bias_add(conv, biases) conv2 = tf.nn.relu(pre_activation, name=&quot;conv2&quot;) # pool2 &amp;&amp; norm2 with tf.variable_scope(&quot;pooling2_lrn&quot;) as scope: pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=&quot;SAME&quot;, name=&quot;pooling2&quot;) norm2 = tf.nn.lrn(pool2, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=&#39;norm2&#39;) # full-connect1 with tf.variable_scope(&quot;fc1&quot;) as scope: reshape = tf.reshape(norm2, shape=[batch_size, -1]) dim = reshape.get_shape()[1].value weights = tf.get_variable(&quot;weights&quot;, shape=[dim, 128], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32)) biases = tf.get_variable(&quot;biases&quot;, shape=[128], dtype=tf.float32, initializer=tf.constant_initializer(0.1)) fc1 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=&quot;fc1&quot;) # full_connect2 with tf.variable_scope(&quot;fc2&quot;) as scope: weights = tf.get_variable(&quot;weights&quot;, shape=[128, 128], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32)) biases = tf.get_variable(&quot;biases&quot;, shape=[128], dtype=tf.float32, initializer=tf.constant_initializer(0.1)) fc2 = tf.nn.relu(tf.matmul(fc1, weights) + biases, name=&quot;fc2&quot;) # softmax with tf.variable_scope(&quot;softmax_linear&quot;) as scope: weights = tf.get_variable(&quot;weights&quot;, shape=[128, n_classes], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32)) biases = tf.get_variable(&quot;biases&quot;, shape=[n_classes], dtype=tf.float32, initializer=tf.constant_initializer(0.1)) softmax_linear = tf.add(tf.matmul(fc2, weights), biases, name=&quot;softmax_linear&quot;) softmax_linear = tf.nn.softmax(softmax_linear) return softmax_linear &emsp;&emsp;整体主要分为三个部分，即卷积+池化层，全连接层，softmax输出。&emsp;&emsp;在正式讲解前，我们先了解几个比较重要的函数。 tf.nn.conv2d()：卷积函数函数头为：tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)input：输入图像，形式为：[batch, in_height, in_width, in_channels]，即训练一个batch的图片数量，图片高度，图片宽度，图像通道数。filter：相当于卷积核，形式为：[filter_height, filter_width, in_channels, out_channels]，具体含义为：[卷积核的高度，滤波器的宽度，图像通道数，滤波器个数]，这里的第三维in_channels就是参数input的第四维。strides：卷积时在图像每一维的步长，是一个一维的向量，长度为4，其中strides[0]=strides[3]=1，strides[1]表示输入图像in_height的滑动步长，strides[2]表示输入图像in_weight的滑动步长。padding：当其值为VALID时，表示边缘不填充，当其值为SAME时，表示填充。 tf.nn.max_pool()：池化函数函数头为：tf.nn.max_pool(input, ksize, strides, padding, name=None)这里面的input,strides和padding和之前的卷积函数里面的几乎一样，唯一有点不同的是ksize，这个表示池化窗口的大小，一般是[1,height,width,1]，因为一般我们不在batch和channels上做池化，所以这两个维度都设为1。 tf.nn.lrn()：局部响应归一化函数函数头为：tf.nn.lrn(input,depth_radius=None,bias=None,alpha=None,beta=None,name=None)局部响应归一化原理是仿造生物学上活跃的神经元对相邻神经元的抑制现象(侧抑制)，其公式如下：计算方法如下：sqr_sum[a, b, c, d] = sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2),output = input / (bias + alpha * sqr_sum) ** beta。N表示通道数(channel)。a,n/2,k,α,β分别表示函数中的input,depth_radius,bias,alpha,beta。具体含义可以参考这篇博客：网址。 卷积+池化层&emsp;&emsp;这里一共有2层，首先利用tf.variable_scope()定义变量的作用域并重命名，下同。然后利用tf.get_variable()分别创建weights变量和biases变量，下同。truncated_normal_initializer()和tf.constant_initializer()都是参数初始化函数，读者可以自己查阅。然后利用tf.nn.conv2d()函数进行卷积，其中input为图像，也就是函数的输入，filter为weights，这里的取值为[3,3,3,16]，前面的2个3表示滤波器的大小为3*3，后面的3是因为图像的通道数为3，最后的16表示一共使用16个3*3的滤波器。stride取值为1，即步长为1，且边缘填充。最后将其和偏差相加并输入到Rule激活函数中，下同。&emsp;&emsp;接下来就是一个池化层，相比较卷积层要简单了很多，这里的input就是刚才的池化层1，即conv1，ksize取值为[1,3,3,1]，即滤波器的大小为3*3，这里的stride取值为2，且边缘填充。最后使用tf.nn.lrn()进行局部响应归一化。&emsp;&emsp;第二个的卷积和池化层和第一个类似，这里就不再叙述了。值得注意的是第二个卷积层中的[3,3,16,16]里面的第1个16表示的是上一层卷积层的第四个维度16，即16个滤波器的16。 全连接层&emsp;&emsp;全连接层是将所有的元素平整化为一个一维向量。每个全连接层的权重长度为2，分别为上一层的长度和该层的长度，从代码可以看出，两个全连接层的长度都是128，其中第一个全连接层的上一层长度是根据之前的池化层算出来的，也就是将所有的特征图的参数相乘，具体见下文分析。 softmax&emsp;&emsp;最后一个是softmax层，也就是最后的分类输出，其最后的网络输出长度也就是分类的个数，也就是函数的输入n_classes。 损失函数及评估&emsp;&emsp;搭建好网络结构后，后面的损失函数及评估就十分简单了，这里使用的是交叉熵损失函数，tf.nn.in_top_k()的使用方法也比较简单，读者可以自己查阅。 def losses(logits, labels): with tf.variable_scope(&#39;loss&#39;): cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) loss = tf.reduce_mean(cross_entropy) return loss def evaluation(logits, labels): with tf.variable_scope(&quot;accuracy&quot;): correct = tf.nn.in_top_k(logits, labels, 1) correct = tf.cast(correct, tf.float16) accuracy = tf.reduce_mean(correct) return accuracy 网络结构再分析&emsp;&emsp;我们以208*208*3的输入图像为例，将卷积网络结构的形状、大小及参数汇总到如下的表格中：&emsp;&emsp;表中的第一列为每一层维度大小，第一个卷积层的16也就是该层使用的滤波器个数，而池化层会将之前的维度减半，后面的全连接层是一个一维列向量。表中的第二列为每一层的激活值尺寸，即将之前的维度全部相乘得到的值，第三列是每一层的参数个数。卷积层的计算公式为：(滤波器参数+1)*滤波器个数，其中1表示偏差。池化层没有参数，全连接层的计算公式为：(上一层维度+1)*这一层维度，这里的1也表示偏差。&emsp;&emsp;我们也可以用TensorFlow里面的tf.trainable_variables()将训练的变量找到并计算其数量： if __name__ == &#39;__main__&#39;: image_dir = r&#39;D:\\\\TensorFlow\\\\dog_and_cat\\\\data\\\\train&#39; sess = tf.Session() train_list = get_all_files(image_dir, True) image_train_batch, label_train_batch = get_batch(train_list, 208, 8, 200, True) train_logits = inference(image_train_batch, 2) var_list = tf.trainable_variables() for v in var_list: print(v, end=&#39;\\n&#39;) paras_count = tf.reduce_sum([tf.reduce_prod(v.shape) for v in var_list]) print(&#39;The number of parameters are :%d&#39; % sess.run(paras_count), end=&#39;\\n\\n&#39;) &emsp;&emsp;结果显示为：&emsp;&emsp;可以看出，其结果和之前我们计算的一致。注：paras_count计算步骤为：先找到每个变量的维度，然后再计算各个维度相乘的积，最后再求和。&emsp;&emsp;由表可见，随着卷积网络的加深，激活值尺寸由开始的692224，慢慢地减少到43264，最后减少到softmax层的2，当然如果激活尺寸下降太快，也会影响神经网络的性能。我们还能观察到其大部分的参数都是集中在全连接层。&emsp;&emsp;当然了，这个卷积网络的参数也可以取其他的值。读者可以在代码中自行修改。 模型训练&emsp;&emsp;下面就是将之前的函数综合起来进行模型训练。 import time from load_data import * from model import * import matplotlib.pyplot as plt # 训练模型 def training(): N_CLASSES = 2 IMG_SIZE = 208 BATCH_SIZE = 8 CAPACITY = 200 MAX_STEP = 10000 LEARNING_RATE = 1e-4 # 测试图片读取 image_dir = r&#39;D:\\\\TensorFlow\\\\dog_and_cat\\\\data\\\\train_2&#39; sess = tf.Session() train_list = get_all_files(image_dir, True) image_train_batch, label_train_batch = get_batch(train_list, IMG_SIZE, BATCH_SIZE, CAPACITY, True) train_logits = inference(image_train_batch, N_CLASSES) train_loss = losses(train_logits, label_train_batch) train_acc = evaluation(train_logits, label_train_batch) train_op = tf.train.AdamOptimizer(LEARNING_RATE).minimize(train_loss) saver = tf.train.Saver() sess.run(tf.global_variables_initializer()) coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) s_t = time.time() try: for step in range(MAX_STEP): if coord.should_stop(): break _, loss, acc = sess.run([train_op, train_loss, train_acc]) if step % 100 == 0: # 实时记录训练过程并显示 runtime = time.time() - s_t print(&#39;Step: %6d, loss: %.8f, accuracy: %.2f%%, time:%.2fs, time left: %.2fhours&#39; % (step, loss, acc * 100, runtime, (MAX_STEP - step) * runtime / 360000)) s_t = time.time() saver.save(sess, r&#39;D:\\\\TensorFlow\\\\dog_and_cat\\\\log\\\\model.cpkt&#39;) except tf.errors.OutOfRangeError: print(&#39;Done.&#39;) finally: coord.request_stop() coord.join(threads=threads) sess.close() &emsp;&emsp;首先初始化一些参数，其中分类类别为2，图像大小为208*208，batch_size为8，即一次训练8张图片，容量为200，迭代次数为10000，学习率为1e-4。然后初始化图片存放的位置。&emsp;&emsp;下面就正式启动会话，开始训练。先用get_all_files()函数将图片全部读入到train_list列表中，然后将该列表放到get_batch中获取训练批次image_train_batch和label_train_batch，其次放入到之前设计好的卷积网络模型中，得到模型的输出train_logits，并依次进行损失函数和评估处理以得到正确率。最后使用自适应矩估计算法AdamOptimizer()进行反向传播的参数优化。&emsp;&emsp;接下来是用Saver()将训练好的模型保存，因为卷积网络的计算量很大，每次运行都耗费很长时间，所以很有必要将训练好的模型保存以便下次处理。&emsp;&emsp;再往下就是调用run()函数实际运行了，首先初始化所有变量，然后调用线程(见上文分析)，最后就是迭代训练，每100次显示训练的正确率。&emsp;&emsp;注：如果设置迭代次数为10000次，一次训练大概需要2个小时左右。 模型测试&emsp;&emsp;训练好模型后，就可以拿测试集数据来检验模型的正确性了。 def eval(): N_CLASSES = 2 IMG_SIZE = 208 BATCH_SIZE = 1 CAPACITY = 200 MAX_STEP = 10 test_dir = r&#39;D:\\\\TensorFlow\\\\dog_and_cat\\\\data\\\\test&#39; sess = tf.Session() train_list = get_all_files(test_dir, is_random=True) image_train_batch, label_train_batch = get_batch(train_list, IMG_SIZE, BATCH_SIZE, CAPACITY, True) train_logits = inference(image_train_batch, N_CLASSES) train_logits = tf.nn.softmax(train_logits) # 用softmax转化为百分比数值 # 载入模型 saver = tf.train.Saver() saver.restore(sess, &#39;log\\\\model.cpkt&#39;) coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) try: for step in range(MAX_STEP): if coord.should_stop(): break image, prediction = sess.run([image_train_batch, train_logits]) print(&#39;prediction&#39;, prediction) max_index = np.argmax(prediction) if max_index == 0: label = &#39;%.2f%% is a cat.&#39; % (prediction[0][0] * 100) else: label = &#39;%.2f%% is a dog.&#39; % (prediction[0][1] * 100) plt.imshow(image[0]) plt.title(label) plt.show() except tf.errors.OutOfRangeError: print(&#39;Done.&#39;) finally: coord.request_stop() coord.join(threads=threads) sess.close() &emsp;&emsp;程序和训练模型的程序几乎一致，唯一的区别在于，这里放入run()函数运行的部分是image_train_batch和train_logits，即图像批次和训练结果。这里的prediction也就是每个类别的置信度，即是猫或是狗的概率。接下来就是分析该概率，接近于0则表示是猫，否则为狗。最后用matplotlib库将图像显示出来：&emsp;&emsp;这是某一张图的结果，从结果可以看出，该模型的输出结果为[0.0494897,0.9505103]，即认为4%的概率是猫，95%的概率是狗。 结论&emsp;&emsp;本博客利用TensorFlow搭建了一个简单的两层卷积神经网络结构，基本实现了猫狗分类器，正确率和置信度总体上还可以，基本上能达到要求，毕竟我们只是使用了一个非常简单的卷积网络，但正确率和置信度仍有提高的空间，读者可以适当的增加网络层数，并设置好相应的参数，以改变模型的正确率。&emsp;&emsp;由于本人也是初学深度学习，有写的不好或写错的地方还望读者多多留言指出，以便后续的改进。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://cxx0822.github.io/categories/深度学习/"}],"tags":[]},{"title":"NAO比赛视觉系统设计（python高级版）","slug":"NAO比赛视觉系统设计（python高级版）","date":"2019-04-27T07:18:47.000Z","updated":"2019-05-23T05:30:29.687Z","comments":true,"path":"2019/04/27/NAO比赛视觉系统设计（python高级版）/","link":"","permalink":"http://cxx0822.github.io/2019/04/27/NAO比赛视觉系统设计（python高级版）/","excerpt":"","text":"概述&emsp;&emsp;之前初级版的视觉系统设计只是通过opnecv简单处理了NAO获得的图像，然后再加上一些判断条件（主要是颜色条件），最后如果满足条件则认为是该目标。虽然总体上可以实现目标的识别，但很容易受到现场环境的影响，不是特别稳定。&emsp;&emsp;在高级版的设计中，我们采用了机器学习中的分类算法。首先根据opencv中的检测算法得到候选区域，然后通过对候选区域的特征提取获得其特征向量，最后通过这些特征向量离线训练分类器模型，从而得到一个较好的分类器。而且该方法适用于任何目标的检测。 候选区域&emsp;&emsp;候选区域的提取是传统图像处理的第一步，能否正确提取到候选区域直接决定着能否检测到目标，现在的机器学习分类算法在分类结果上基本上都能达到很好效果，正确率也能满足要求，但往往在特征区域的提取上会出现偏差甚至检测不到。所以特征提取这一步至关重要。&emsp;&emsp;下面创建一个通用的目标检测类TargetDetection，想用什么方法获得候选区域，就在类中封装成一个方法即可，最后统一返回候选区域(矩形)的左上角和右下角坐标，以便后续的处理。 预处理&emsp;&emsp;之前初级版并没有对原图进行过多的预处理，虽然在比赛时，背景颜色比较单一，不加预处理也可以得到理想的效果，但是为了提高检测的精确度和应用的广泛性，这里稍微加了几个图像处理的方法。&emsp;&emsp;首先介绍几个常见的预处理方法。 灰度化&emsp;&emsp;将彩色图像转化成为灰度图像的过程称为图像的灰度化处理。彩色图像中的每个像素的颜色有R、G、B三个分量决定，而每个分量的取值为[0, 255]，每个像素一共有255*255*255种情况。而灰度图像是R、G、B三个分量相同的一种特殊的彩色图像，所以每个像素一共只有255种情况，所以在数字图像处理种一般先将各种格式的图像转变成灰度图像以使后续的图像的计算量变得少一些。&emsp;&emsp;灰度图像的描述与彩色图像一样仍然反映了整幅图像的整体和局部的色度和亮度等级的分布和特征。图像的灰度化处理可用两种方法来实现。&emsp;&emsp;第一种方法是求出每个像素点的R、G、B三个分量的平均值，然后将这个平均值赋予给这个像素的三个分量。第二种方法是根据YUV的颜色空间中，Y的分量的物理意义是点的亮度，由该值反映亮度等级，根据RGB和YUV颜色空间的变化关系可建立亮度Y与R、G、B三个颜色分量的对应：Y=0.3R+0.59G+0.11B，以这个亮度值表达图像的灰度值。(一般采用第二种方法。)&emsp;&emsp;opencv中实现：cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 二值化&emsp;&emsp;将图像上点的灰度置为0或255的过程称为二值化处理，也就是将整个图像呈现出明显的黑白效果(非黑即白)。&emsp;&emsp;所有灰度大于或等于阀值的像素被判定为属于特定物体，其灰度值为255表示，否则这些像素点被排除在物体区域以外，灰度值为0，表示背景或者例外的物体区域。&emsp;&emsp;opencv中实现：cv2.threshold(src, threshold, maxValue, method) 图像滤波&emsp;&emsp;图像滤波，即在尽量保留图像细节特征的条件下对目标图像的噪声进行抑制，是图像预处理中不可缺少的操作，其处理效果的好坏将直接影响到后续图像处理和分析的有效性和可靠性。&emsp;&emsp;高斯滤波是一种线性平滑滤波，可以消除高斯噪声。每一个像素点的值，都由其本身和领域内的其他像素值经过加权平均(高斯函数)后得到。&emsp;&emsp;opencv中实现：cv2.GaussianBlur(img, ksize, sigmaX) HSV空间的二值化&emsp;&emsp;预处理的主要思想是，先将颜色通道转换为HSV空间，当然也可以转到其他的颜色空间，只是实际测试下来发现，HSV空间更加稳定，适合比赛的环境。其次根据HSV空间颜色分布表，设置相应的阈值，将符合的颜色区间二值化，得到一个只有目标区域的图像分布，最后加上几个简单的滤波算法去除噪声。&emsp;&emsp;上表为HSV颜色空间表，即每个颜色对应的三个通道的范围。HSV即色相(Hue)、饱和度(Saturation)和明度(Value)。根据其范围，我们就可以将我们需要的颜色提取出来，并二值化处理。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# coding: utf-8import cv2import cv2.cv as cvimport numpy as npclass TargetDetection(object): ''' Target Detection：目标检测基类，主要用于图像的预处理，以便后续检测更加精确 ''' def __init__(self, img): self.img = img def preProcess(self, img, object): ''' Pre Process：预处理 Arguments: img：图像 object：红球(redball)/足球(football)/黄杆(stick) Return: binImg：二值化后的图像 ''' if object == \"redball\": HSVImg = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # 转到HSV空间 # HSV空间颜色判断，具体参见表格 smin1, vmin1, hmax1, hmin2 = 9, 21, 39, 153 # 调用滑动条函数（sliderObjectHSV）得到理想值 minHSV1 = np.array([0, smin1, vmin1]) maxHSV1 = np.array([hmax1, 255, 255]) minHSV2 = np.array([hmin2, smin1, vmin1]) maxHSV2 = np.array([180, 255, 255]) # 二值化处理 binImg1 = cv2.inRange(HSVImg, minHSV1, maxHSV1) binImg2 = cv2.inRange(HSVImg, minHSV2, maxHSV2) binImg = np.maximum(binImg1, binImg2) # 图像滤波处理（腐蚀，膨胀，高斯） binImg = self.filter(binImg)) else: print('''Please input \"redball\" or \"football\" or \"stick\" in preProcess()''') return binImg def filter(self, img): ''' 图像滤波处理（腐蚀，膨胀，高斯） Arguments: img：图像 Return: resImg：处理后的图像 ''' kernelErosion = np.ones((3, 3), np.uint8) kernelDilation = np.ones((3, 3), np.uint8) resImg = cv2.erode(img, kernelErosion, iterations=2) resImg = cv2.dilate(resImg, kernelDilation, iterations=3) resImg = cv2.GaussianBlur(resImg, (9, 9), 1.5) return resImg &emsp;&emsp;首先利用cvtColor()将其转换为HSV空间，然后由表可知目标的颜色范围，从而得到其上限和下限(红色有2个区间)，最后利用inRange()将其二值化。inRange()：将在两个阈值内的像素值设置为白色(255)，而不在阈值区间内的像素值设置为黑色(0)。 最后加个几个简单的滤波处理算法。这里给出了红球的二值化代码，足球和黄杆的代码读者可以先自行考虑。 123456789if __name__ == '__main__': srcImg = cv2.imread(\"./redball_1/5.jpg\") # 红球 tarDet = TargetDetection(srcImg) binImg = tarDet.preProcess(srcImg, \"redball\") # 红球 cv2.imshow(\"srcImg\", srcImg) cv2.imshow(\"binImg\", binImg) cv2.waitKey(0) cv2.destroyAllWindows() &emsp;&emsp;为了克服比赛时场地和光线的干扰，这里的阈值参数可以通过滑动条函数来获得。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def sliderObjectHSV(self, object): ''' HSV滑动条函数，为了获得理想的HSV阈值 Arguments: object：红球(redball)/足球(football)/黄杆(stick) ''' if object == \"redball\": cv2.namedWindow(\"redball\") # 创建滑动条 cv2.createTrackbar(\"hmax1\", \"redball\", 1, 20, self.nothing) cv2.createTrackbar(\"smin1\", \"redball\", 30, 60, self.nothing) cv2.createTrackbar(\"vmin1\", \"redball\", 30, 60, self.nothing) cv2.createTrackbar(\"hmin2\", \"redball\", 156, 175, self.nothing) img = self.img.copy() HSVImg = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) while True: srcImg = img.copy() # 获取滑动条的值 hmax1 = cv2.getTrackbarPos(\"hmax1\", \"redball\") smin1 = cv2.getTrackbarPos(\"smin1\", \"redball\") vmin1 = cv2.getTrackbarPos(\"vmin1\", \"redball\") hmin2 = cv2.getTrackbarPos(\"hmin2\", \"redball\") # HSV空间颜色判断 minHSV1 = np.array([0, smin1, vmin1]) maxHSV1 = np.array([hmax1, 255, 255]) minHSV2 = np.array([hmin2, smin1, vmin1]) maxHSV2 = np.array([180, 255, 255]) binImg1 = cv2.inRange(HSVImg, minHSV1, maxHSV1) binImg2 = cv2.inRange(HSVImg, minHSV2, maxHSV2) binImg = np.maximum(binImg1, binImg2) # 图像滤波处理 binImg = self.filter(binImg) cv2.imshow(\"srcImg\", img) cv2.imshow(\"redball\", binImg) cv2.waitKey(1) cv2.destroyAllWindows() else: print('''Please input \"redball\" or \"football\" or \"stick\" in sliderObjectHSV()''')def nothing(self, x): pass &emsp;&emsp;将其HSV空间的临界值设置为滑动条参数即可。现场调试时，根据实际情况选择一个最优的参数。这里同样只给出红球的代码实例。 霍夫圆检测&emsp;&emsp;针对NAO比赛中的红球和足球，我们可以采用opencv中的霍夫圆检测技术将其检测出来。其函数声明为：HoughCircles(img, method, dp, minDist, param1=100, param2=100, minRadius=0, maxRadius=0)，其中method一般为霍夫梯度法，即cv2.cv.CV_HOUGH_GRADIENT，dp=1, param1=100, param2=20，比较重要的参数是圆之间的距离minDist，圆的最小和最大半径minRadius和maxRadius。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class HoughDetection(TargetDetection): ''' Hough Detection：霍夫圆检测 ''' def __init__(self, img): super(HoughDetection, self).__init__(img) def houghDetection(self, img, minDist=100, minRadius=25, maxRadius=80, isShow=False): ''' 霍夫圆检测 Arguments: img：图像 minDist：两圆之间最小间距 minRadius：圆的最小半径 maxRadius：圆的最大半径 isShow：是否显示结果 Return: circles：检测出来的圆 ''' srcImg = self.img.copy() circles = cv2.HoughCircles(img, cv.CV_HOUGH_GRADIENT, 1, minDist, param1=100, param2=20, minRadius=minRadius, maxRadius=maxRadius) if circles is None: circles = [] print(\"no circle\") else: circles = circles[0, ] if isShow is True: self.showHoughResult(srcImg, circles) return circles def circle2Rect(self, circle, k=1): ''' 圆的信息转换为矩阵信息，以便后续处理 Arguments: circle：圆的信息：圆心坐标，半径 k：放缩因子 Return: rect：矩阵信息：左上角和右下角的坐标 ''' rect = [] x, y, r = int(circle[0]), int(circle[1]), int(circle[2]) initX, initY = x - k * r, y - k * r endX, endY = x + k * r, y + k * r rect = [initX, initY, endX, endY] return rect def showHoughResult(self, img, circles, timeMs=0): ''' 显示霍夫圆检测结果 Arguments: img：图像 circles：圆 timeMs：延迟时间，0表示一直显示 ''' for circle in circles: rect = self.circle2Rect(circle) initX, initY = rect[0], rect[1] endX, endY = rect[2], rect[3] cv2.rectangle(img, (initX, initY), (endX, endY), (0, 0, 255), 2) # 画矩形 x, y, r = int(circle[0]), int(circle[1]), int(circle[2]) cv2.circle(img, (x, y), r, (0, 0, 255), 2) # 画圆 cv2.imshow(\"Hough Result\", img) cv2.waitKey(timeMs) cv2.destroyAllWindows() &emsp;&emsp;在调用霍夫圆检测时，首先要将图片进行预处理。这里将比较重要的3个参数作为函数的参数，以便后续的红球和足球的处理。circle2Rect函数的作用是将圆的信息转换为矩阵的信息，并提供一个比例的参数接口，以便后续的矩形区域的调整，showHoughResult函数的作用是在原图中画出圆和矩阵。&emsp;&emsp;下面进行简单的测试，首选读取一张带有红球/足球的图片，然后创建对象并调用预处理方法和霍夫圆检测方法，并将isShow的参数设置为True将其结果显示出来。 12345678if __name__ == '__main__': srcImg = cv2.imread(\"./redball_1/5.jpg\") # 红球 # srcImg = cv2.imread(\"./img_1/5.jpg\") # 足球 hogDet = HoughDetection(srcImg) binImg = hogDet.preProcess(srcImg, \"redball\") # 红球 # binImg = hogDet.preProcess(srcImg, \"football\") # 足球 hogDet.houghDetection(binImg, minDist=100, minRadius=10, maxRadius=50, isShow=True) # 红球 # hogDet.houghDetection(binImg, minDist=100, minRadius=25, maxRadius=80, isShow=True) # 足球 &emsp;&emsp;实际测试发现，经过二值化处理后再用霍夫圆检测的效果要比之前直接霍夫圆检测好很多，基本上每次都能选中目标区域。&emsp;&emsp;注：实际测试发现，足球用霍夫圆检测效果并不是太好，轮廓检测(见下文分析)对于足球效果更好。可能因为红球是纯色，二值化后圆的特征比较明显，而足球呈黑白色，圆的特征不是太明显。 轮廓检测&emsp;&emsp;而对于NAO比赛中的黄杆，我们可以采用opencv中的轮廓检测算法。其函数声明为：cv2.findContours(image, mode, method[, contours[, hierarchy[, offset ]]])，其中mode表示轮廓的检索模式，这里选择cv2.RETR_EXTERNAL，即只检测外轮廓，method表示轮廓的近似办法，这里选择cv2.CHAIN_APPROX_NONE，即存储所有的轮廓点。其返回值有2个，contours和hierarchy，分别表示轮廓本身和每条轮廓对应的属性。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class ContoursDetection(TargetDetection): def __init__(self, img): super(ContoursDetection, self).__init__(img) def contoursDetection(self, img, minPerimeter=300, mink=2, isShow=False): ''' 轮廓检测 Arguments: img：图像 minPerimeter：轮廓最小周长 isShow：是否显示结果 Return: resultContours：检测出来的轮廓 ''' srcImg = self.img.copy() rects = [] contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) resultContours = [] # 简单的轮廓周长及长宽比判断 for contour in contours: x, y, w, h = cv2.boundingRect(contour) k = h / w perimeter = cv2.arcLength(contour, True) if perimeter &gt; minPerimeter and k &gt;= mink: resultContours.append(contour) if resultContours == []: print(\"no contours\") else: if isShow is True: self.showContourResult(srcImg, resultContours) return resultContours def showContourResult(self, img, contours, timeMs=0): ''' 显示轮廓检测结果 Arguments: img：图像 contours：轮廓 timeMs：延迟时间，0表示一直显示 ''' cv2.drawContours(img, contours, -1, (0, 0, 255), 2) # 画出轮廓的外接矩阵 for contour in contours: rect = self.contour2Rect(contour) cv2.rectangle(img, (rect[0], rect[1]), (rect[2], rect[3]), (0, 0, 255), 2) cv2.imshow(\"Contour_result\", img) cv2.waitKey(timeMs) cv2.destroyAllWindows() def contour2Rect(self, contour): ''' 轮廓的信息转换为矩阵信息，以便后续处理 Arguments: contour：轮廓的信息：若干个点组成的轮廓 Return: rect：矩阵信息：左上角和右下角的坐标 ''' rect = [] x, y, w, h = cv2.boundingRect(contour) # 返回值为外接矩阵的顶点坐标和长宽 rect = [x, y, x + w, y + h] return rect &emsp;&emsp;该轮廓算法是将所有的轮廓都检测出来，但其中有些结果并不是我们所需要的，所以我们可以采用一些简单的判断条件进行筛选，比如轮廓的周长和轮廓矩形的长宽比等，并将其作为函数的参数以便实际比赛时候的调整。contour2Rect函数的作用是将轮廓的信息转换为矩阵的信息，showContourResult函数的作用是在原图中画出轮廓和矩阵。&emsp;&emsp;下面进行简单的测试，首选读取一张带有黄杆的图片，然后创建对象并调用预处理方法和轮廓检测方法，并将isShow的参数设置为True将其结果显示出来。 特征提取&emsp;&emsp;检测到目标后，下面就要对其进行特征的提取，从而获得分类器的输入向量。和之前候选区域的提取一样，将所有的特征提取方法封装成一个类，并统一返回向量(列表)的形式。 球类目标颜色特征&emsp;&emsp;首先对于足球和红球最容易想到的就是其颜色特征，之前初级版的设计只是简单的判断颜色所占比例，这并不是一个非常好的标准，因为受到光线等其他因素的影响，在图像中实际的颜色并非我们想象的那样，所以我们采用另一种判断标准。&emsp;&emsp;将每个通道的颜色区间分为若干份，即将[0, 255]区间分成若干个子区间，一般取16比较适中，然后分别统计每个通道的每个像素点属于哪一个子区间，最后统计每个子区间有多少个像素点。例如，R通道的第1个像素点的值为46，属于[32, 47]这个区间，则该区间就+1。这样一共有3*16=48个特征向量，3表示3个通道，16表示16个区间，当然也可以取其他值。 12345678910111213141516171819202122232425class ColorFeature(object): def __init__(self, img, number=16): self.img = img self.number = number assert type(self.number % 16) == int, \"number should be divided by 16.\" def splitChannal(self, img): # 分离通道 Channel1, Channel2, Channel3 = cv2.split(img) Channel1, Channel2, Channel3 = Channel1.flatten(), Channel2.flatten(), Channel3.flatten() Channels = [Channel1, Channel2, Channel3] return Channels def colorExtract(self, img): # 颜色提取 Channels = self.splitChannal(img) size = Channels[0].shape[0] colorVector = [] for Channel in Channels: Channel = Channel / self.number # 商即为分类类别 Channel = np.append(Channel, 15) colorVector.extend(np.round((1.0 * np.bincount(Channel) / size), 4)) # 统计个数，并计算概率 return colorVector &emsp;&emsp;首先通过opencv中的split()方法将图像分成3个通道，然后对每个通道进行颜色特征提取。Channel是一个列表，包含了该通道的每个像素点的值，将其除以总区间个数就可以知道属于第几个区间。例如刚才的像素值46，因为46/16=2，所以属于第2个区间(区间从0开始计)，也就是[32, 47]这个区间，即之前的像素点46转换为现在的区间类别2。然后在利用numpy中的bincount()统计方法计算每个区间有多少个像素，注意这里最好在Channel中添加一个元素15，因为图像中不一定存在第16个区间内的像素值，如果不存在，则特征向量的个数就会不同，影响后续的分类器训练，最后在除以总像素个数得到归一化的结果并保留4位小数。&emsp;&emsp;注：python3中需要用//。12345if __name__ == \"__main__\": img = cv2.imread(\"./img_3/1.jpg\") colorFeature = ColorFeature(img) colorVector = colorFeature.colorExtract(img) print(len(colorVector), colorVector) &emsp;&emsp;输出结果为：1(48, [0.0158, 0.0035, 0.0037, 0.0083, 0.0254, 0.1115, 0.226, 0.2418, 0.1169, 0.065, 0.0429, 0.0272, 0.0223, 0.026, 0.0227, 0.0411, 0.0186, 0.0036, 0.0065, 0.0177, 0.0486, 0.1404, 0.1927, 0.2202, 0.1531, 0.0649, 0.028, 0.0242, 0.0265, 0.0203, 0.0125, 0.0223, 0.0201, 0.0038, 0.0057, 0.0158, 0.0486, 0.1376, 0.1863, 0.2099, 0.172, 0.0964, 0.0419, 0.0261, 0.0145, 0.0107, 0.0067, 0.0039]) HOG特征&emsp;&emsp;HOG特征指的是梯度方向直方图，顾名思义，就是选用梯度方向的分布作为特征。一张图像的梯度（x和y方向的导数）在边缘和拐角（强度变化剧烈的区域）处的梯度幅值很大，而且边缘和拐角比其他平坦的区域包含更多关于物体形状的信息。&emsp;&emsp;首先需要将图像分成小的连通区域，称之为细胞单元。然后采集细胞单元中各像素点的梯度或边缘的方向直方图。最后把这些直方图组合起来就可以构成特征描述器。&emsp;&emsp;这里主要参考了以下两篇博客的：梯度方向直方图，80行Python实现-HOG梯度特征提取。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586class HogFeature(): def __init__(self, img, cell_size=4, bin_size=8): self.img = img self.img = np.sqrt(img / float(np.max(img))) self.img = self.img * 255 self.cell_size = cell_size self.bin_size = bin_size self.angle_unit = 360 / self.bin_size assert type(self.bin_size) == int, \"bin_size should be integer,\" assert type(self.cell_size) == int, \"cell_size should be integer,\" assert type(self.angle_unit) == int, \"bin_size should be divisible by 360\" def hog_extract(self): height, width = self.img.shape gradient_magnitude, gradient_angle = self.global_gradient() gradient_magnitude = abs(gradient_magnitude) cell_gradient_vector = np.zeros((int(round(1.0 * height / self.cell_size)), int(round(1.0 * width / self.cell_size)), self.bin_size)) for i in range(cell_gradient_vector.shape[0]): for j in range(cell_gradient_vector.shape[1]): cell_magnitude = gradient_magnitude[i * self.cell_size:(i + 1) * self.cell_size, j * self.cell_size:(j + 1) * self.cell_size] cell_angle = gradient_angle[i * self.cell_size:(i + 1) * self.cell_size, j * self.cell_size:(j + 1) * self.cell_size] cell_gradient_vector[i][j] = self.cell_gradient(cell_magnitude, cell_angle) hog_image = self.render_gradient(np.zeros([height, width]), cell_gradient_vector) hog_vector = [] for i in range(cell_gradient_vector.shape[0] - 1): for j in range(cell_gradient_vector.shape[1] - 1): block_vector = [] block_vector.extend(cell_gradient_vector[i][j]) block_vector.extend(cell_gradient_vector[i][j + 1]) block_vector.extend(cell_gradient_vector[i + 1][j]) block_vector.extend(cell_gradient_vector[i + 1][j + 1]) mag = lambda vector: math.sqrt(sum(i ** 2 for i in vector)) magnitude = mag(block_vector) if magnitude != 0: normalize = lambda block_vector, magnitude: [element / magnitude for element in block_vector] block_vector = normalize(block_vector, magnitude) hog_vector.append(block_vector) return hog_vector, hog_image def global_gradient(self): gradient_values_x = cv2.Sobel(self.img, cv2.CV_64F, 1, 0, ksize=5) gradient_values_y = cv2.Sobel(self.img, cv2.CV_64F, 0, 1, ksize=5) gradient_magnitude = cv2.addWeighted(gradient_values_x, 0.5, gradient_values_y, 0.5, 0) gradient_angle = cv2.phase(gradient_values_x, gradient_values_y, angleInDegrees=True) return gradient_magnitude, gradient_angle def cell_gradient(self, cell_magnitude, cell_angle): orientation_centers = [0] * self.bin_size for i in range(cell_magnitude.shape[0]): for j in range(cell_magnitude.shape[1]): gradient_strength = cell_magnitude[i][j] gradient_angle = cell_angle[i][j] min_angle, max_angle, mod = self.get_closest_bins(gradient_angle) orientation_centers[min_angle] += (gradient_strength * (1 - (mod / self.angle_unit))) orientation_centers[max_angle] += (gradient_strength * (mod / self.angle_unit)) return orientation_centers def get_closest_bins(self, gradient_angle): idx = int(gradient_angle / self.angle_unit) mod = gradient_angle % self.angle_unit if idx == self.bin_size: return idx - 1, (idx) % self.bin_size, mod return idx, (idx + 1) % self.bin_size, mod def render_gradient(self, image, cell_gradient): cell_width = self.cell_size / 2 max_mag = np.array(cell_gradient).max() for x in range(cell_gradient.shape[0]): for y in range(cell_gradient.shape[1]): cell_grad = cell_gradient[x][y] cell_grad /= max_mag angle = 0 angle_gap = self.angle_unit for magnitude in cell_grad: angle_radian = math.radians(angle) x1 = int(x * self.cell_size + magnitude * cell_width * math.cos(angle_radian)) y1 = int(y * self.cell_size + magnitude * cell_width * math.sin(angle_radian)) x2 = int(x * self.cell_size - magnitude * cell_width * math.cos(angle_radian)) y2 = int(y * self.cell_size - magnitude * cell_width * math.sin(angle_radian)) cv2.line(image, (y1, x1), (y2, x2), int(255 * math.sqrt(magnitude))) angle += angle_gap return image &emsp;&emsp;首先在一幅图中选取若干像素组合成一个cell，例如8*8个像素，然后选取2*2个cell，即16*16个像素，组合成一个block。然后将0°-360°划分为若干个区间，比如8个区间，在这8个区间内，统计每个block中的每个cell中的梯度方向直方图(具体见博客)，所以一个block有4*8=32个特征向量(因为一个block有4个cell，1个cell有8个直方图特征)，然后按照step的大小在图像中移动block，通常step设置为一个cell的大小，即下次移动的时候会和之前的有重复。最后统计一幅图中有多少个block即可算出所有的特征向量个数。例如一个640*480的原始图，其横向有640/8-1=79个block，纵向有480/8-1=59个block,那么其特征向量一个有79*59=4661个，每一个都是一个32维的向量。 123456if __name__ == \"__main__\": img = cv2.imread(\"./img_3/1.jpg\") img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) hogFeature = HogFeature(img) hogVector, _ = hogFeature.hog_extract() print(np.array(hogVector).shape) &emsp;&emsp;输出结果为(4661, 32)。当然了，这是整幅图的特征向量，实际的特征提取中，只是针对候选区域的(不会太大)，所以我们可以将整个候选区域当作一个block，这样特征向量的个数就缩减为32个。 分类器&emsp;&emsp;有了特征向量后，我们就可以选择分类器来训练模型。 Logistic回归&emsp;&emsp;Logistic回归是一个简单的线性二分类的分类算法，其基本原理就是在每个特征上都乘以一个回归系数，然后把所有的结果相加，将这个总和代入到Sigmoid函数中，进而得到一个范围在0~1之间的数值，最后判断当大于0.5的时候就被分为1类，小于0.5就被分为0类。&emsp;&emsp;x即输入的特征向量，w即回归系数。下面的问题就是如何找到w的最优解。这里我们采用的优化算法是梯度上升法。其w的更新公式为：&emsp;&emsp;程序实现：123456789101112131415161718192021222324252627282930313233343536# coding uft-8import numpy as npimport mathclass Logistic(object): def __init__(self, filename, maxCycle): self.filename = filename self.maxCycle = maxCycle def loadDateSet(self): data = np.loadtxt(self.filename) dataMat = data[:, 0: -1] classLabels = data[:, -1] # dataMat = np.insert(dataMat, 0, 1, axis=1) return dataMat, classLabels def sigmoid(self, z): return 1 / (1 + np.exp(-z)) def gradDescent(self, dataMat, classLabels): dataMatrix = np.mat(dataMat) labelMat = np.mat(classLabels).transpose() m, n = np.shape(dataMatrix) weights = np.ones((n, 1)) alpha = 0.001 for i in range(self.maxCycle): h = self.sigmoid(dataMatrix * weights) error = labelMat - h weights = weights + alpha * dataMatrix.transpose() * error return np.round(weights, 4) def classifyVector(self, inX, weights): prob = self.sigmoid(sum(np.dot(inX, weights))) return prob 模型训练&emsp;&emsp;有了特征向量和分类器，我们就可以训练模型了。模型中分为正样本和负样本，其训练方法也是不同的。 球类目标正样本&emsp;&emsp;对于正样本来说，通常的做法是利用数据标注来获得候选区域。常用的数据标注软件有labelImg。该软件不用安装，且使用方法十分简单(自行百度下载)。&emsp;&emsp;打开软件后，首先选择Open Dir打开图像文件夹，然后选择Change Save Dir选择要保存的xml文件的文件路径（标注完成后会生成一个xml文件存放标注信息），然后点击Create RectBox创建矩形，&emsp;&emsp;在需要的区域拖拽鼠标即可，然后在弹出的对话框中选择类别的名称，最后点击save保存，并选择Next Image切换到下一张。&emsp;&emsp;下面就要从生成的xml文件中提取出我们需要的信息。1234567891011121314151617181920212223242526&lt;annotation&gt; &lt;folder&gt;img_2&lt;/folder&gt; &lt;filename&gt;0.jpg&lt;/filename&gt; &lt;path&gt;D:\\NAO\\visual_cxx\\img_2\\0.jpg&lt;/path&gt; &lt;source&gt; &lt;database&gt;Unknown&lt;/database&gt; &lt;/source&gt; &lt;size&gt; &lt;width&gt;640&lt;/width&gt; &lt;height&gt;480&lt;/height&gt; &lt;depth&gt;3&lt;/depth&gt; &lt;/size&gt; &lt;segmented&gt;0&lt;/segmented&gt; &lt;object&gt; &lt;name&gt;ball&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;47&lt;/xmin&gt; &lt;ymin&gt;315&lt;/ymin&gt; &lt;xmax&gt;189&lt;/xmax&gt; &lt;ymax&gt;454&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt;&lt;/annotation&gt; &emsp;&emsp;这是其中的一个xml文件，可以看出我们想要的信息在object项中的第1个和第4个中，即name和bndbox中。利用python内置的ElementTree库就可以解析出我们想要的结果。1234567891011121314151617181920# coding uft-8import xml.etree.ElementTree as ETdef parseXml(xml_file): classes_num = &#123;\"ball\": 1, \"noball\": 0&#125; labels = [] tree = ET.parse(xml_file) root = tree.getroot() for item in root: if item.tag == \"object\": obj_name = item[0].text obj_num = classes_num[obj_name] xmin = int(item[4][0].text) ymin = int(item[4][1].text) xmax = int(item[4][2].text) ymax = int(item[4][3].text) labels.append([xmin, ymin, xmax, ymax, obj_num]) return labels &emsp;&emsp;返回的labels列表就包括了候选矩形区域的2个顶点坐标及类别。 &emsp;&emsp;接下来就是使用之前的特征提取和分类器进行离线训练，在训练之前，我们针对模型进行小小的改进。将整个候选区域分成若干份，比如分成4份，每份都执行同样的特征提取，这样既增加了特征向量的个数，也提高了模型的鲁棒性。&emsp;&emsp;候选区域一般为矩形，所以直接上下等分即可：1234567891011121314151617181920def reshapeBallRect(rawRect, numbers): n = int(math.sqrt(numbers)) + 1 newPoint = np.zeros((n, n, 2)) newRect = np.zeros((n - 1, n - 1, 4)) initX, initY, endX, endY = rawRect[0], rawRect[1], rawRect[2], rawRect[3] # 初始化参数 # 找出每个小矩阵的顶点坐标 for i in range(n): for j in range(n): newPoint[i][j][0] = int(initX + ((endX - initX) / (n - 1) * j)) newPoint[i][j][1] = int(initY + ((endY - initY) / (n - 1) * i)) # 根据坐标构造新矩阵 for i in range(n - 1): for j in range(n - 1): newInitX, newInitY = int(newPoint[0 + i][0 + j][0]), int(newPoint[0 + i][0 + j][1]) newEndX, newEndY = int(newPoint[1 + i][1 + j][0]), int(newPoint[1 + i][1 + j][1]) newRect[i][j][0], newRect[i][j][1], newRect[i][j][2], newRect[i][j][3] = newInitX, newInitY, newEndX, newEndY return newRect &emsp;&emsp;针对球类目标，我们选择颜色和HOG特征提取：123456789101112131415def calColorFeature(img, number=16): # 计算颜色特征 color = ColorFeature(img, number) result = color.colorExtract(img) return np.round(result, 4)def calHOGFeature(img, cell_size): # 计算HOG特征 rectBallArea = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) hog = HogFeature(rectBallArea, cell_size) vector, image = hog.hog_extract() return np.round(vector[0], 4) &emsp;&emsp;最后统一保留4位小数。 &emsp;&emsp;然后就是采集大量的正样本图片(越多越好，至少上百张)，最后将这些特征向量全部合并到一个列表中，并保存到txt文件里，以便后续分类器的处理。1234567891011121314151617181920212223242526272829303132def calPosVector(writeFilename, labelNumbers): with open(writeFilename, 'w') as f: for i in range(labelNumbers): print('test ' + str(i)) resultTotal = [] xmlFile = \"./label_2/\" + str(i) + \".xml\" labels = parseXml(xmlFile) img = cv2.imread(\"./img_2/\" + str(i) + \".jpg\") initX, initY, endX, endY = labels[0][0], labels[0][1], labels[0][2], labels[0][3] Rect = [initX, initY, endX, endY] newRects = reshapeHoughRect(Rect, 4) for newRectRow in newRects: for newRect in newRectRow: newInitX, newInitY = int(newRect[0]), int(newRect[1]) newEndX, newEndY = int(newRect[2]), int(newRect[3]) rectBallArea = img[newInitY:newEndY, newInitX:newEndX, :] # 矩形区域(宽，高，通道) cv2.rectangle(img, (newInitX, newInitY), (newEndX, newEndY), (0, 0, 255), 2) # 画矩形 resultColor = calColorFeature(rectBallArea, 16) cellSize = min(newEndX - newInitX, newEndY - newInitY) result_HOG = calHOGFeature(rectBallArea, cellSize / 2) resultTotal.extend(resultColor) resultTotal.extend(result_HOG) print('resultTotal', len(resultTotal)) cv2.imshow(\"Original\", img) cv2.waitKey(0) row = ' '.join(list(map(str, resultTotal))) + ' ' + str(labels[0][4]) + '\\n' f.write(row) &emsp;&emsp;首先从标注文件xml中获得lables信息，然后依次打开正样本图片，并根据信息获得其中的候选区域，再对其4等分处理，对每一个等分矩阵进行颜色和HOG特征的提取，注意此时的HOG特征中的block区域为整个等分矩阵区域，所以其cell_size大小的选取应为长宽中的最小值的一半。（cell_size一般为block的一半）。所以单个等分矩阵的HOG特征的特征向量一共有4*8=32个，颜色特征的特征向量个数为16*3=48个，即总的候选区域的特征向量个数为80*4=320个。&emsp;&emsp;单张图片的实验结果，特征向量太多了，这里就不显示了。 负样本&emsp;&emsp;而对于负样本的训练，就不能采用标注的方法了，而是直接调用之前的霍夫圆检测算法检测原图，当然了，原图不应该包含我们的球类目标，而且二值化的阈值范围要放大一点，以防止检测不出足球/红球。这里我们直接只使用灰度化处理，不进行二值化处理。其算法检测出来多少个圆就当作多少个负样本，最后在对其特征向量的提取并存放在txt文件中，以便后续的分类器训练。123456789101112131415161718192021222324252627282930313233def calNegVector(writeFilename, labelNumbers): with open(writeFilename, 'w') as f: for i in range(labelNumbers): print('test ' + str(i)) srcImg = cv2.imread(\"./img_3/\" + str(i) + \".jpg\") hogDec = HoughDetection(srcImg) preImg = cv2.cvtColor(srcImg, cv2.COLOR_BGR2GRAY) circles = hogDec.houghDetection(preImg, minDist=100, minRadius=25, maxRadius=80) for circle in circles: resultTotal = [] rect = hogDec.circle2Rect(circle) if rect[0] &lt; 0 or rect[1] &lt; 0 or rect[2] &gt; 640 or rect[3] &gt; 480: continue newRects = reshapeHoughRect(rect, 4) for newRectRow in newRects: for newRect in newRectRow: newInitX, newInitY = int(newRect[0]), int(newRect[1]) newEndX, newEndY = int(newRect[2]), int(newRect[3]) rectBallArea = srcImg[newInitY:newEndY, newInitX:newEndX, :] # 矩形区域(宽，高，通道) cv2.rectangle(srcImg, (newInitX, newInitY), (newEndX, newEndY), (0, 0, 255), 2) # 画矩形 resultColor = calColorFeature(rectBallArea, 16) cellSize = min(newEndX - newInitX, newEndY - newInitY) result_HOG = calHOGFeature(rectBallArea, cellSize / 2) resultTotal.extend(resultColor) resultTotal.extend(result_HOG) print('resultTotal', len(resultTotal)) cv2.imshow(\"Original\", srcImg) cv2.waitKey(0) row = ' '.join(list(map(str, resultTotal))) + ' ' + str(0) + '\\n' f.write(row) &emsp;&emsp;最后一行的label标签需要更改为0。 结果测试&emsp;&emsp;有了正负样本的特征向量后，就可以放入到分类器里面去训练参数了，这里将所有的特征向量都合并到一个txt文件里面。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def resultTest(method): trainingSet = [] trainingLabels = [] with open(\"data.txt\", 'r') as f: for line in f.readlines(): currLine = line.strip().split() lineArr = [] for i in range(320): lineArr.append(float(currLine[i])) trainingSet.append(lineArr) trainingLabels.append(float(currLine[320])) if method == \"Logistic\": log = Logistic(\"data.txt\", 500) trainingWeights = log.gradDescent(trainingSet, trainingLabels) numbers = 39 for i in range(numbers): print(\"test\" + str(i)) srcImg = cv2.imread(\"./images/\" + str(i) + \".jpg\") conDet = ContoursDetection(srcImg) preImg = conDet.preProcess(srcImg, \"football\") rects = conDet.contoursDetection(preImg, minPerimeter=200, minK=0) if rects == []: print(\"test\" + str(i) + \" no rects\") for rect in rects: resultTotal = [] rect = conDet.contour2Rect(rect) if rect[0] &lt; 0 or rect[1] &lt; 0 or rect[2] &gt; 640 or rect[3] &gt; 480: print(\"out of bound\") continue newRects = reshapeHoughRect(rect, 4) for newRectRow in newRects: for newRect in newRectRow: newInitX, newInitY = int(newRect[0]), int(newRect[1]) newEndX, newEndY = int(newRect[2]), int(newRect[3]) rectBallArea = srcImg[newInitY:newEndY, newInitX:newEndX, :] resultColor = calColorFeature(rectBallArea, 16) cellSize = min(newEndX - newInitX, newEndY - newInitY) resultHOG = calHOGFeature(rectBallArea, cellSize / 2) resultTotal.extend(resultColor) resultTotal.extend(resultHOG) resultTotal = np.array(resultTotal).reshape(1, -1) classify = log.classifyVector(resultTotal, trainingWeights) if classify &gt; 0.5: classifyResult = \"yes\" cv2.rectangle(srcImg, (rect[0], rect[1]), (rect[2], rect[3]), (0, 0, 255), 2) else: classifyResult = \"no\" cv2.rectangle(srcImg, (rect[0], rect[1]), (rect[2], rect[3]), (0, 255, 255), 2) print('classify', classifyResult) cv2.imshow(\"test \" + str(i), srcImg) cv2.waitKey(0) &emsp;&emsp;首先打开特征向量的文本文件，将所有的特征向量和标签分布放到2个列表中，然后放到分类器里面得到权重参数，然后对每一张图片进行目标检测，并获得该目标区域的特征向量，在放到分类器里面得到预测的结果，最后如果概率大于0.5，则认为是，否则为不是。&emsp;&emsp;下图为其中一张的测试结果：&emsp;&emsp;其中红色为正确的目标，黄色为错误的目标。实际测试下来，几乎可以达到实时检测，正确率也达到了比赛的标准。 黄杆类目标&emsp;&emsp;黄杆类目标的模型训练和球类目标基本上一致，重复性的工作这里就不再叙述，读者可以自行考虑代码设计。&emsp;&emsp;这里同样对模型进行一点改进，由于黄杆是细条型的矩形框，所以不能采用之前的均分方法。这里采用一种新的等分方法：首先找到黄杆的矩形框，然后将其左右延申，将landmark标记也框在矩形框内，然后为了克服随机性，将矩形框上下延申至图像的顶部和底部，最后再将其四等分。如图所示：123456789101112def reshapeStickRect(rawRect, numbers): newRect = np.zeros((numbers, 4)) initX, initY, endX, endY = rawRect[0], rawRect[1], rawRect[2], rawRect[3] # 初始化参数 # 找出每个小矩阵的顶点坐标 for i in range(numbers): newRect[i][0] = initX newRect[i][1] = initY + ((endY - initY) / numbers) * i newRect[i][2] = endX newRect[i][3] = initY + ((endY - initY) / numbers) * (i + 1) return newRect &emsp;&emsp;特征向量的提取和之前的球类目标类似，这样总的特征向量仍然为320个。 总结&emsp;&emsp;高级版的设计其实也就是传统的目标检测方法，一般分为三个阶段：首先在给定的图像上选择一些候选的区域，然后对这些区域提取特征，最后使用训练的分类器进行分类。&emsp;&emsp;随着技术的发展，现在目标检测的普遍做法是利用深度学习来训练模型，得到的分类器更具体一般性，效果也更好。这里就不再过多的叙述了，以后有机会也会发布深度学习版的NAO比赛目标检测。","categories":[{"name":"NAO高尔夫比赛","slug":"NAO高尔夫比赛","permalink":"http://cxx0822.github.io/categories/NAO高尔夫比赛/"}],"tags":[]},{"title":"python高级编程","slug":"python高级编程","date":"2019-04-25T13:56:20.000Z","updated":"2020-01-09T07:20:40.578Z","comments":true,"path":"2019/04/25/python高级编程/","link":"","permalink":"http://cxx0822.github.io/2019/04/25/python高级编程/","excerpt":"","text":"推导式&emsp;&emsp;推导式comprehensions（又称解析式），是Python的一种独有特性。推导式是可以从一个数据序列构建另一个新的数据序列的结构体。 列表推导式1[expression for item in iterable if condition] &emsp;&emsp;例：1num = [num + 1 for num in range(0, 5) if num % 2 == 1] &emsp;&emsp;程序输出：1[2, 4] 字典推导式1&#123;key_expression:value_expression for expression in iterable if condition&#125; &emsp;&emsp;例：12A = &#123;\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4&#125;A = &#123;v: k for k, v in A.items()&#125; &emsp;&emsp;程序输出：1&#123;1: 'a', 2: 'b', 3: 'c', 4: 'd'&#125; &emsp;&emsp;例：12A = [\"apple\", \"orange\", \"banana\", \"pear\"]A = &#123;key: value for key, value in enumerate(A) if value == \"apple\"&#125;&#125; &emsp;&emsp;程序输出：1&#123;0: 'apple'&#125; 集合推导式1&#123;expression for expression in iterable if condition&#125; &emsp;&emsp;例：12A = [\"apple\", \"orange\", \"banana\", \"pear\"]A = &#123;len(s) for s in A&#125; &emsp;&emsp;程序输出：1[5, 6, 6, 4] 可迭代对象定义&emsp;&emsp;可迭代对象：实现了__iter__()方法的对象都是可迭代对象。如果没有实现__iter__而实现了__getitem__方法，并且其参数是从零开始的索引，这种对象，如序列，也是可迭代的。&emsp;&emsp;迭代器：实现了__iter__和__next__()方法的对象都是迭代器。&emsp;&emsp;由定义可知，除了内置的序列和字典之外，我们还可以自己定义可迭代对象。 iter()&emsp;&emsp;该方法返回的是当前对象的迭代器类的实例。因为可迭代对象与迭代器都要实现这个方法，因此有以下两种写法。 用于可迭代对象类的写法，返回该可迭代对象的迭代器类的实例。 用于迭代器类的写法，直接返回self（即自己本身），表示自身即是自己的迭代器。 next()&emsp;&emsp;表示获取迭代器对象中下一个值。 创建可迭代对象1234567891011121314151617181920212223from collections import Iterable # 自定义可迭代对象class MyList(object): def __init__(self): # 定义一个空列表，保存用户添加的数据 self.my_list = list() # 在列表中添加元素 def append_data(self, data): self.my_list.append(data) # 在类里面提供__iter__方法,使该对象是可迭代对象 def __iter__(self): pass # 通过自定义可迭代类型创建自定义可迭代对象my_iterable = MyList() # 查看my_iterable是不是指定类型：Iterableresult = isinstance(my_iterable, Iterable)print(result) &emsp;&emsp;在创建的类中，由于定义了__iter__()方法，即使该方法为空，但已经满足了可迭代对象的定义，所以用isinstance()检测是否为可迭代对象时，输出的结果为true。&emsp;&emsp;可迭代对象的本质是通过迭代器帮助可迭代对象依次迭代对象中的每一个数据，所以真正完成获取数据的操作是通过迭代器完成的。 创建迭代器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class MyList(object): def __init__(self): self.my_list = list() def append_data(self, data): self.my_list.append(data) # 使用__iter__()的第一种写法 def __iter__(self): my_iterator = MyIterator(self.my_list) return my_iterator # 自定义迭代器类class MyIterator(object): def __init__(self, current_list): # 保存外界传入过来的数据对象 self.current_list = current_list # 记录获取数据的下标 self.current_index = 0 # 使用__iter__()的第二种写法 def __iter__(self): return self # 使用__next__方法获取指定对象中的数据 def __next__(self): # 判断下标是否越界 if self.current_index &lt; len(self.current_list): # 根据下标获取数据对象中的指定数据 value = self.current_list[self.current_index] # 获取数据成功对下标加上1 self.current_index += 1 return value else: # 代码执行到此说明下标越界，停止取值操作，抛出停止迭代异常 raise StopIterationif __name__ == \"__main__\": # 通过自定义可迭代类创建出自定义可迭代对象，添加数据 my_iterable = MyList() my_iterable.append_data(1) my_iterable.append_data(2) for value in my_iterable: print(value, end=',') &emsp;&emsp;输出结果为：1,2。迭代器的标配就是__iter__和__next__方法，每个迭代器都有，这种方法是自从它被创建出来的时候就有。真正的迭代是由__next__来执行的，也就是__next__每次从迭代器中取出一个元素来操作，直到所有的元素被取完。&emsp;&emsp;当然也可以用python内置的迭代器函数生成迭代器。iter()函数表示获取可迭代对象的迭代器，会调用可迭代对象身上的__iter__方法，next()函数表示获取迭代器对象中下一个值，会调用迭代器对象身上的__next__方法。例：1234567891011if __name__ == \"__main__\": # 创建迭代器对象 it = iter([1, 2, 3, 4, 5]) while True: try: # 获得下一个值: x = next(it) print(x, end=',') except StopIteration: # 遇到StopIteration就退出循环 break &emsp;&emsp;输出结果为：1,2,3,4,5。首先使用iter()使列表成为迭代器（列表是可迭代对象，但不是迭代器），然后一直调用next()函数不停地获取迭代器里面的值。注意，一定要加异常处理，因为一旦获取不到迭代器里面的值就会报错。 迭代器应用&emsp;&emsp;使用迭代器生成斐波那契数列。123456789101112131415161718192021222324252627282930# 迭代器完成斐波那契数列class Fibonacci(object): def __init__(self, num): # num:表示根据个数生成数列 self.num = num # 保存斐波那契数列前两个值 self.first = 0 self.second = 1 # 记录生成斐波那契数列的下标 self.current_index = 0 def __iter__(self): return self def __next__(self): if self.current_index &lt; self.num: result = self.first self.first, self.second = self.second, self.first + self.second # 生成数据完成以后对下标加上1 self.current_index += 1 return result else: # 表示数列生成完成 raise StopIterationif __name__ == \"__main__\": # 创建生成斐波那契数列的对象 fib = Fibonacci(5) for i in fib: print(i, end=',') &emsp;&emsp;输出结果为0,1,1,2,3,。使用迭代器可以节省空间，没有上限控制。迭代器不会把每次生成的值保存起来，只会根据你的算法生成一个值。即可以用多少创建多少，而且可以无限创建。 创建生成器&emsp;&emsp;生成器属于轻装版的迭代器，但生成器仍然是迭代器，不过是改进的。可以实现一边循环一边计算。1234567891011121314151617# 生成器完成斐波那契数列def fibonacci(num): # num：表示生成器根据个数创建指定个数的fibonacci数列 first = 0 second = 1 current_index = 0 # 循环判断条件是否成立，表示是否生成斐波那契数列 while current_index &lt; num: result = first first, second = second, first + second current_index += 1 yield resultif __name__ == \"__main__\": generator = fibonacci(5) for a in generator: print(a, end=',') &emsp;&emsp;输出结果为0,1,1,2,3,。生成器函数的定义方式和普通函数几乎一样，只不过它有一个yield语句。当函数执行到yield的时候，该函数被挂起，等待下次被next()激活。带有yield的函数不再是一个普通函数，而是一个生成器generator, yield相当于return返回一个值，并且记住这个返回的位置，下次迭代时，代码从yield的下一条语句开始执行。&emsp;&emsp;生成器相比较迭代器，更加简洁，且功能完全相同，相当于内置了next()函数。","categories":[{"name":"Python","slug":"Python","permalink":"http://cxx0822.github.io/categories/Python/"}],"tags":[]},{"title":"NAO足球/高尔夫比赛视觉系统设计（python初级版）","slug":"NAO比赛视觉系统设计（python初级版）","date":"2019-03-20T10:12:26.000Z","updated":"2019-05-22T12:54:13.912Z","comments":true,"path":"2019/03/20/NAO比赛视觉系统设计（python初级版）/","link":"","permalink":"http://cxx0822.github.io/2019/03/20/NAO比赛视觉系统设计（python初级版）/","excerpt":"","text":"概述&emsp;&emsp;初级版的内容主要分为两部分，第一部分是如何利用NAO的视觉传感器，即上下摄像头，来获取图片及如何利用opencv显示获得的图片。第二部分是如何利用opencv里面的视觉算法从NAO获取的图片中找到所需目标，并返回需要的目标信息。 视觉系统框架设计&emsp;&emsp;首先要搭建好视觉系统的程序框架，python是一种面向对象的编程语言，而面向对象最重要的特征就是类的封装，所以我们可以将整个视觉系统分为若干类，每个类实现相应的功能。 ConfigureNAO类12345678910111213141516from naoqi import ALProxyclass ConfigureNao(object): def __init__(self, IP=\"169.254.67.213\", PORT=9559): self.IP = IP self.PORT = PORT self.cameraProxy = ALProxy(\"ALVideoDevice\", self.IP, self.PORT) self.landMarkProxy = ALProxy(\"ALLandMarkDetection\", self.IP, self.PORT) self.motionProxy = ALProxy(\"ALMotion\", self.IP, self.PORT) self.postureProxy = ALProxy(\"ALRobotPosture\", self.IP, self.PORT) self.tts = ALProxy(\"ALTextToSpeech\", self.IP, self.PORT) self.memoryProxy = ALProxy(\"ALMemory\", self.IP, self.PORT) &emsp;&emsp;首先定义NAO机器人的IP和端口号，并将此作为类的参数，其次声明一些常用的类，如视觉、运动、姿势和存储类等。只有实例化后的类才可以使用。 VisualBasis类123456789101112131415161718192021222324252627282930313233# coding: utf-8from configureNao import ConfigureNaofrom naoqi import ALProxyimport numpy as npimport almath import mathimport timeimport sysimport osimport cv2import cv2.cv as cvimport vision_definitions as vdclass VisualBasis(ConfigureNao): def __init__(self, IP, PORT, cameraID=vd.kBottomCamera, resolution=vd.kVGA): super(VisualBasis, self).__init__(IP, PORT) self.cameraID = cameraID self.resolution = resolution self.colorSpace = vd.kBGRColorSpace self.fps = 20 self.frameHeight = 320 self.frameWidth = 640 self.frameChannels = 0 self.frameArray = None self.cameraPitchRange = 47.64 / 180 * np.pi self.cameraYawRange = 60.97 / 180 * np.pi &emsp;&emsp;这里使用了naoqi系统的宏定义vision_definitions，为了书写方便，将其简写为vd，其中这里使用了摄像头ID的宏定义：vd.kBottomCamera(下摄像头)和vd.kTopCamera(上摄像头)、分辨率宏定义vd.kVGA和颜色空间宏定义vd.kBGRColorSpace。(具体含义见下文分析)。另外将图像本身(frameArray)及其高度、宽度、通道，摄像头的俯仰角作为类的属性，以便后续方法的调用。&emsp;&emsp;因为NAO读取球目标和黄杆目标分别使用的是上面和下面的摄像头，所以将摄像头ID作为类的参数以便后续调整。基类的第二个参数为分辨率，方便后续图像的清晰度调整。IP和PORT是为了继承父类所需的参数。 获取图片12345678910111213141516171819def updateFrame(self): if self.cameraProxy.getActiveCamera() != self.cameraID: self.cameraProxy.setActiveCamera(self.cameraID) videoClient = self.cameraProxy.subscribe(\"cxx\", self.resolution, self.colorSpace, self.fps) frame = self.cameraProxy.getImageRemote(videoClient) self.cameraProxy.unsubscribe(videoClient) self.frameWidth = frame[0] self.frameHeight = frame[1] self.frameChannels = frame[2] self.frameArray = np.frombuffer(frame[6], dtype=np.uint8).reshape([frame[1], frame[0], frame[2]]) # frombuffer将data以流的形式读入转化成ndarray对象, 第一参数为stream,第二参数为返回值的数据类型 # frame[6]: binary array of size height * width * nblayers containing image data. if self.frameArray is None: print(\"no frame find\") return np.array([]) return self.frameArray &emsp;&emsp;让NAO读取一张图片大致可以分为3个步骤。1.激活摄像头。2.订阅。3.读取图像。&emsp;&emsp;调用setActiveCamera()函数即可激活摄像头，其次就是调用subscribe()函数订阅该摄像头，该函数的函数头为std::string ALVideoDeviceProxy::subscribe(const std::string&amp; Name, const int&amp; resolution, const int&amp; colorSpace, const int&amp; fps)，一共需要4个参数。第1个为订阅者的名字，随便写一个字符串即可，第2个是分辨率，NAO最高支持1280*960的分辨率，但是越高的分辨率意味着越大的数据量，所以我们选择的是640*480的分辨率，其宏定义为vd.kVGA，第3个参数为颜色空间，这里选择的是一般的BGR空间，其宏定义为vd.kBGRColorSpace，最后一个为帧数，这里选择20即可。&emsp;&emsp;订阅完之后，就可以调用getImageRemote()函数来获得图像了。其参数为之前的订阅者，返回值为图像的容器，具体内容如下：&emsp;&emsp;其中0-5都是图像的基本信息，即宽、高、通道、颜色空间和时间戳。第6个索引值存放的内容就是我们需要的图像信息，使用numpy里面的reshape()函数将其简单的处理成我们需要的宽*高*通道类型的数组。&emsp;&emsp;最后使用unsubscribe()取消订阅以释放内存即可。 显示图片12345678def showFrame(self, frameArray, timeMs=1000, isSave=False): if frameArray is None: print(\"please get an image from Nao with the method updateFrame()\") else: cv2.imshow(\"current frame\", frameArray) cv2.waitKey(timeMs) if isSave is True: cv2.imwrite(\"test.jpg\", frameArray) &emsp;&emsp;opencv中常用的显示图片的函数为imshow()，该函数需要传入2个参数，即显示图像窗口的名称和图像内容。然后调用waitKey()函数延迟显示一段时间，单位为ms。最后提供了一个是否保存的参数接口，以便后续需要保存获得的图像。 FootBallDetect&emsp;&emsp;针对足球和红球，主要的流程为检测-&gt;筛选-&gt;定位。首先对NAO获得的图像进行检测，从而获得足球/红球或类似的目标，其次对其进行简单的判断筛选，以确保是我们需要的目标，最后进行定位，返回需要的目标信息。 初始化12345678class FootBallDetect(VisualBasis): def __init__(self, IP, PORT, cameraID=vd.kBottomCamera, resolution=vd.kVGA): super(FootBallDetect, self).__init__(IP, PORT, cameraID, resolution) self.ballData = &#123;\"centerX\": 0, \"centerY\": 0, \"radius\": 0&#125; self.ballPosition = [0, 0, 0] self.minDist = 100 # int(self._frameHeight/30.0) self.minRadius = 25 self.maxRadius = 80 # int(self._frameHeight/10.0) &emsp;&emsp;首先将足球的基本信息和位置信息作为类的属性，然后将霍夫圆检测算法(具体见下文分析)中的参数也作为类的属性。 检测1234567891011def findCirclesV0(self, img, minDist=100, minRadius=25, maxRadius=80): grayImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) circles = cv2.HoughCircles(grayImg, cv.CV_HOUGH_GRADIENT, 1, minDist, param1=100, param2=20, minRadius=minRadius, maxRadius=maxRadius) if circles is None: circles = [] print(\"no circle\") else: circles = circles[0, ] return circles &emsp;&emsp;初级版采用的检测球的算法是霍夫变换圆检测算法，该算法比较简单，而且opencv有现成的函数，直接调用即可。函数头为：1cv2.HoughCircles(image, method, dp, minDist, circles, param1, param2, minRadius, maxRadius) &emsp;&emsp;image：输入图像，必须为灰度图像，这里使用opencv中的cvtColor()函数将其从BGR转为GRAY即可。method为检测方法，常用的是霍夫梯度法HOUGH_GRADIENT（opencv2的写法为：cv2.cv.CV_HOUGH_GRADIENT）。dp为检测内侧圆心的累加器图像的分辨率于输入图像之比的倒数，如dp=1，累加器和输入图像具有相同的分辨率，如果dp=2，累计器便有输入图像一半那么大的宽度和高度（取1即可）。minDist表示两个圆之间圆心的最小距离。param1是method设置的检测方法的对应的参数，对于霍夫梯度法，它表示传递给canny边缘检测算子的高阈值，而低阈值为高阈值的一半。param2是method设置的检测方法的对应的参数，对于霍夫梯度法，它表示在检测阶段圆心的累加器阈值，它越小，就越可以检测到更多根本不存在的圆，而它越大的话，能通过检测的圆就更加接近完美的圆形了。minRadius表示圆半径的最小值，maxRadius表示圆半径的最大值。&emsp;&emsp;由上述分析可知，霍夫圆检测算法的参数十分重要，如果设置的不合理，很难达到预期的要求。 筛选&emsp;&emsp;将球检测出来后，下一步就是进行筛选并找到我们需要的球。初级版采用的策略是颜色概率法。如下图所示：&emsp;&emsp;检测出圆形后，外接一个正方形，其边长d为2(k*r)，r为圆的半径，k为比例。这样就可以通过各个颜色出现的概率判断是否是绿地毯上的红球或足球（黑白球）。&emsp;&emsp;opencv中有两种常用的颜色通道空间：HSV空间和BGR空间。实际测试下来发现HSV空间比较稳定。 12345678910111213141516171819202122232425262728293031323334353637383940def selectCircleV0(self, circles): img = self.frameArray.copy() HSV = cv2.cvtColor(self.frameArray, cv2.COLOR_BGR2HSV) circleSelected = [] BWRatiomin = 1.0 k = 1.5 for circle in circles: centerX, centerY, radius = circle[0], circle[1], circle[2] initX, initY = int(centerX - int(k * radius)), int(centerY - int(k * radius)) endX, endY = int(centerX + int(k * radius)), int(centerY + int(k * radius)) if initX &lt; 0 or initY &lt; 0 or endX &gt; img.shape[1] or endY &gt; img.shape[0] or radius &lt; 1: continue rectBallArea = HSV[initY:endY, initX:endX, :] HFlat, SFlat, VFlat = rectBallArea[:, :, 0].flatten(), rectBallArea[:, :, 1].flatten(), rectBallArea[:, :, 2].flatten() size = HFlat.shape[0] onesArray = np.ones((size,)) # 参考HSV颜色分布表 GScoreHL = np.uint8(35 * onesArray &lt;= HFlat) GScoreHR = np.uint8(HFlat &lt;= 77 * onesArray) GScoreSL = np.uint8(43 * onesArray &lt;= SFlat) GScoreVL = np.uint8(46 * onesArray &lt;= VFlat) GScore = float(np.sum(GScoreHL * GScoreHR * GScoreSL * GScoreVL)) WScoreSR = np.uint8(SFlat &lt;= 35 * onesArray) WScoreVL = np.uint8(221 * onesArray &lt;= VFlat) WScore = float(np.sum(WScoreSR * WScoreVL)) BScoreVR = np.uint8(VFlat &lt;= 46 * onesArray) BScore = float(np.sum(BScoreVR)) GRatio, BRatio, WRatio = GScore * 1.0 / size, BScore * 1.0 / size, WScore * 1.0 / size WhiteBlackRatio = BScore * 1.0 / size + WScore * 1.0 / size if WhiteBlackRatio &gt; 0.1 and np.abs(WhiteBlackRatio - 0.34) &lt; BWRatiomin and GRatio &gt; 0.1: BWRatiomin = np.abs(WhiteBlackRatio - 0.34) circleSelected = circle circleSelected = np.array(circleSelected) return circleSelected &emsp;&emsp;首先将图像空间转换为HSV空间，然后对于检测出来的圆利用for循环依次遍历筛选。对于每个圆，首先根据比例k和圆心、半径求出矩形的左上角和右下角，对于超出图像边界的矩形框不予讨论，其次将HSV空间根据通道索引将其分解成3个部分，然后将每个部分根据HSV空间颜色分布表求出每个部分的颜色得分，全部设置为1，然后求和，算出每个颜色的总得分，然后除以总数，即可得到概率，最后根据概率即可判断是否是我们需要的目标。 &emsp;&emsp;可以利用opencv的circle()函数将结果显示出来：123456def drawCircle(self, img, circle): x, y, r = int(circle[0]), int(circle[1]), int(circle[2]) cv2.circle(img, (x, y), r, (0, 0, 255), 2) cv2.imshow(\"result\", img) cv2.waitKey(0) cv2.destroyAllWindows() &emsp;&emsp;下面进行一个简单的测试，将之前的函数综合调用一下，并画出检测的结果。1234567891011if __name__ == \"__main__\": footBallDet = FootBallDetect(\"192.168.1.101\", 9559) footBallDet.postureProxy.goToPosture(\"StandInit\", 0.5) img = footBallDet.updateFrame() circles = footBallDet.findCirclesV0(img) if circles == []: print(\"no ball\") else: circleSelected = footBallDet.selectCircleV0(circles) footBallDet.drawCircle(img, circleSelected) &emsp;&emsp;结果显示在图中可以大致框出足球范围。 定位&emsp;&emsp;灰色矩形区域是摄像头拍摄的图像区域，其中图像原点位于矩形的左上角，坐标轴方向如图所示。蓝点表示摄像头，红点表示球的位置。摄像头的pitch（张角）和yaw（仰角）的正方向如图所示。红线表示摄像头的张角范围（左右视角范围），紫线表示仰角范围（上下视角范围）。球的位置坐标为(x,y)。&emsp;&emsp;如果球正好位于图像的中心位置(320,240)，那么摄像头-球-机器人双脚中心正好可以构成一个平面的直角三角形，如下图所示：&emsp;&emsp;摄像头离地面的 &emsp;&emsp;注：初级版只是为了NAO足球比赛的，完整的NAO比赛视觉系统设计参见高级版的博客。","categories":[{"name":"NAO高尔夫比赛","slug":"NAO高尔夫比赛","permalink":"http://cxx0822.github.io/categories/NAO高尔夫比赛/"}],"tags":[]},{"title":"C++重难点：重载与模板","slug":"C++重难点：重载与模板","date":"2019-03-15T01:51:24.000Z","updated":"2019-04-16T07:11:38.274Z","comments":true,"path":"2019/03/15/C++重难点：重载与模板/","link":"","permalink":"http://cxx0822.github.io/2019/03/15/C++重难点：重载与模板/","excerpt":"","text":"&emsp;&emsp;在之前的C语言编程中，一个函数实现一个功能，但有时候我们需要实现几个功能类似的函数，只是有些细节不同，如果按照C语言的编程方式，我们需要重新定义函数，这会使得代码十分不美观。但在C++中，我们可以使用重载或模板很好的解决这个问题。 函数重载定义&emsp;&emsp;在同一作用域类中，一组函数名相同，参数列表（函数特征标）不同（参数个数不同/参数类型不同/参数排列顺序不同），返回值可同可不同的函数。&emsp;&emsp;重载函数通常用来命名一组功能相似的函数，这样做减少了函数名的数量，对于程序的可读性有很大的好处。 示例&emsp;&emsp;先看一个简单的加法函数重载：12345678910111213141516171819202122232425262728#include &lt;iostream&gt;using namespace std;int fun(int, int);int fun(int *, int *);int fun(int a, int b)&#123; cout &lt;&lt; \"int a + int b : \"; return a + b;&#125;int fun(int * a, int * b)&#123; cout &lt;&lt; \"int *a + int *b : \"; return *a + *b;&#125;int main(void)&#123; int a1 = 1, b1 = 2; int a2 = 2, b2 = 3; int *p = &amp;a2, *q = &amp;b2; cout &lt;&lt; fun(a1, b1) &lt;&lt; endl; cout &lt;&lt; fun(p, q) &lt;&lt; endl; return 0;&#125; &emsp;&emsp;定义了2个fun函数，其参数类型分别为int和int *类型，返回值都是int类型，为了更好的说明，在每个函数中都加入输出提示。最终输出结果为： 12int a int b : 3int *a + int *b : 5 &emsp;&emsp;这个例子很好理解，但如果此时在定义一个double类型变量，并调用fun函数，会显示什么？如下所示： 12double a3 = 1.2, b3 = 2.9;cout &lt;&lt; fun(a3, b3) &lt;&lt; endl; 输出结果： 1int a int b: 3 &emsp;&emsp;为什么此时没有报错呢？明明没有定义参数列表是double类型的fun函数。如果再定义个fun函数的重载，将参数类型设置为int &amp;，即引用参数，如下所示： 123456int fun(int &amp;, int &amp;);int fun(int &amp; a, int &amp; b)&#123; cout &lt;&lt; \"&amp; a + &amp; b : \"; return a + b;&#125; &emsp;&emsp;此时再调用刚才的cout &lt;&lt; fun(a1, b1) &lt;&lt; endl;程序会报错：error C2668: “fun”: 对重载函数的调用不明确。这又有什么会报错呢？见下文解析。 原理如何解决命名冲突&emsp;&emsp;编译器在编译当前作用域里的同名函数时，会根据函数形参的类型和顺序会对函数进行重命名（不同的编译器在编译时对函数的重命名标准不一样）。在visual studio编译器中，根据返回值类型（不起决定性作用）+形参类型和顺序（起决定性作用）的规则重命名并记录在map文件中。&emsp;&emsp;右击工程名，然后选择属性，在依次选择配置属性-&gt;链接器–&gt;调试，将其中的生成映射文件和映射导出都设置为是，映射文件名为生成的map文件名，可以自己命名也可以用默认的。点击确定，并运行程序后，会在工程文件夹中的Debug文件夹下生成一个.map的文件夹，将其拖拽至编译器内即可打开。 &emsp;&emsp;从图中可以看到，虽然函数名相同，但在map中的生成的名称去不一样。？表示名称开始，？后边是函数名，@@YA表示参数表开始，后边的3个字符分别表示返回值类型，参数类型。@Z表示名称结束。由上述分析可知，函数重载仅仅是语法层面的，本质上它们还是不同的函数，占用不同的内存，入口地址也不一样。 如何解决调用匹配&emsp;&emsp;除了利用函数重载可以实现相同的函数名实现不同的功能，函数模板同样的也可以实现。为了更好的解释其中的调用匹配问题，在讲完函数模板后，再重新解释（刚才提到的问题属于调度匹配问题）。 作用及意义&emsp;&emsp;函数重载是属于多态中的静态多态，即在编译时的多态，而虚函数与虚继承属于动态多态，即在运行时的多态，其两者都是为了减少函数名的数量，避免名字空间的污染，提高程序的可读性。 模板定义&emsp;&emsp;模板也是一种C++支持参数化多态的工具，使用模板可以为类或函数声明一种一般模式，使得类中的某些数据成员、成员函数的参数、返回值取得任意类型。&emsp;&emsp;模板通常有两种形式：函数模板和类模板，函数模板针对仅参数类型不同的函数；类模板针对仅数据成员和成员函数类型不同的类。（本博客只讨论函数模板。）&emsp;&emsp;模板的声明或定义只能在全局，命名空间或类范围内进行。即不能在局部范围，函数内进行，比如不能在main函数中声明或定义一个模板。 函数模板&emsp;&emsp;函数模板是通用的函数描述，也就是说，它们使用泛型来定义函数，其中的泛型可用具体的类型(如int或double)替换。通过将类型作为参数传递给模板，可使编译器生成该类型的函数。由于模板允许以泛型(而不是具体类型)的方式编写程序，因此有时也被称为通用编程。由于类型是用参数表示的，因此模板特性有时也被称为参数化类型。 示例&emsp;&emsp;先看一个简单的函数模板定义：12345678910111213141516#include &lt;iostream&gt;using namespace std;template &lt;typename T&gt;T fun(T a, T b)&#123; return a * b;&#125;int main(void)&#123; int a1 = 1, b1 = 2; cout &lt;&lt; fun(a1, b1) &lt;&lt; endl; return 0;&#125; &emsp;&emsp;程序输出：2&emsp;&emsp;函数模板的一般定义格式如下：template &lt;typename T&gt;返回值 函数名(T 参数){}，其中typename可以替换为class。&emsp;&emsp;函数模板有两种类型的参数，第一种是模板参数，位于函数模板名称的前面，在一对尖括号内部进行声明；第二种是调用参数，位于函数模板名称之后，在一对圆括号内部进行声明。&emsp;&emsp;如果可以由调用参数来决定模板参数，则模板函数调用是不需要指明模板参数，但如果不能则必须指明，例如以下情况：12double a2 = 1.2, b2 = 2.3;cout &lt;&lt; fun(a1, b2) &lt;&lt; endl; &emsp;&emsp;此时再调用的话，就会报错：C2782 “double fun(T,T)”: 模板 参数“T”不明确，因为此时变量a1和b2不是同一个类型，而函数模板定义中并没有说明这一点，所以正确的定义和调用应该如下所示：1234567891011121314151617#include &lt;iostream&gt;using namespace std;template &lt;typename T1, typename T2, typename T3&gt;T3 fun(T1 a, T2 b)&#123; return a * b;&#125;int main(void)&#123; int a1 = 1, b1 = 2; double a2 = 1.2, b2 = 2.3; cout &lt;&lt; fun&lt;int, double, double&gt;(a1, b2) &lt;&lt; endl; return 0;&#125; &emsp;&emsp;总之，调用和定义时的类型必须保持一致，当编译器无法判断时，需要显示地指明参数类型。&emsp;&emsp;当然模板也是可以重载的，比如再定义一个可以将2个数组中的各个元素相乘的函数。1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;using namespace std;template &lt;typename T1, typename T2&gt;void fun(T1 a, T2 b)&#123; cout &lt;&lt; a * b &lt;&lt; endl;&#125;template &lt;typename T1, typename T2&gt;void fun(T1 *a, T2 *b, int n)&#123; for (int i = 0; i &lt; n; i++) &#123; cout &lt;&lt; a[i] * b[i] &lt;&lt; ' '; &#125; cout &lt;&lt; endl;&#125;int main(void)&#123; int a1 = 1, b1 = 2; double a2 = 1.2, b2 = 2.3; double a3[2] = &#123; 1.2, 2.3 &#125;, b3[2] = &#123; 2.3, 3.4 &#125;; fun&lt;int, double&gt;(a1, b2); fun&lt;double, double&gt;(a3, b3, 2); return 0;&#125; &emsp;&emsp;程序输出结果为：122.32.76 7.82 &emsp;&emsp;那么面对如此多的相同名字的函数，编译器到底是如何选择的呢？","categories":[{"name":"C++","slug":"C","permalink":"http://cxx0822.github.io/categories/C/"}],"tags":[]},{"title":"C++重难点：友元","slug":"C++重难点：友元","date":"2019-03-12T01:09:44.000Z","updated":"2019-04-16T02:23:15.396Z","comments":true,"path":"2019/03/12/C++重难点：友元/","link":"","permalink":"http://cxx0822.github.io/2019/03/12/C++重难点：友元/","excerpt":"","text":"&emsp;&emsp;C++由于其封装的特性，通常使用类对数据进行了隐藏和封装，类的数据成员一般都定义为私有成员。虽然在有些时候这对类中的数据是一种很好的保护措施，但是有时候也需要定义一些函数或类来访问类中的非公有成员，比如运算符重载或两个类不是继承关系，但需要共享数据的时候，这时通常我们令其他类或函数成为它的友元来解决这类问题。 友元概念&emsp;&emsp;友元机制允许一个类将对其非公有成员的访问权授予指定的函数或者类，友元的声明以friend开始，它只能出现在类定义的内部，友元声明可以出现在类中的任何地方：友元不是授予友元关系的那个类的成员（友元不属于该类），所以它们不受其声明出现部分的访问控制影响（可以是公有，也可以是私有）。通常，友元声明成组地放在类定义的开始或结尾。 友元分类&emsp;&emsp;C++中一共有3种友元，即友元函数，友元类和友元成员函数。 友元函数概念&emsp;&emsp;友元函数是指虽然不是类成员函数却能够访问类的所有成员的函数。本身是一个类外的普通函数，但函数的声明只能在类的内部，定义在类的外部。&emsp;&emsp;格式：friend 类型 函数名(形参); 实现123456789101112131415161718192021222324#include &lt;iostream&gt;using namespace std;class A&#123;public: friend void fun(A &amp;a); //友元函数声明 private: int data;&#125;;void fun(A &amp;a) //友元函数定义&#123; a.data = 0; cout &lt;&lt; a.data &lt;&lt; endl;&#125;int main()&#123; class A a; fun(a); return 0;&#125; &emsp;&emsp;程序结果：10 注意 友元函数的声明可以放在类的私有部分，也可以放在公有部分，它们是没有区别的，都说明是该类的一个友元函数。 一个函数可以是多个类的友元函数，只需要在各个类中分别声明。 友元函数的调用与一般函数的调用方式和原理一致。 友元函数能定义在类的内部，这样的函数是隐式内联的。 友元类概念&emsp;&emsp;如果希望一个类可以访问另一个类的非公有成员在内的所有成员，可以将一个类指定为另一类的友元类。友元类的所有成员函数都是另一个类的友元函数。&emsp;&emsp;格式：friend class 类名; 实现1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;using namespace std;class A&#123;public: friend class B; //友元类声明 private: int data;&#125;;class B &#123;public: void fun(A &amp;a) &#123; a.data = 0; cout &lt;&lt; a.data &lt;&lt; endl; &#125;&#125;;int main(void)&#123; class A a; class B b; b.fun(a); return 0;&#125; &emsp;&emsp;程序结果：10 注意 友元关系不能被继承，就像父亲的朋友未必是儿子的朋友。 友元关系是单向的，不具有交换性。若类B是类A的友元，类A不一定是类B的友元，要看在类中是否有相应的声明。 友元关系不具有传递性。若类B是类A的友元，类C是B的友元，类C不一定是类A的友元，同样要看类中是否有相应的申明。 友元成员函数概念&emsp;&emsp;有时候我们仅需要将特定的类成员函数成为另一个类的友元，而不是整个类。我们可以这样做：使类B中的特定成员函数成为类A的友元函数，这样类B的该成员函数就可以访问类A的所有成员了。&emsp;&emsp;但是这样做会比较麻烦，必须要排列好各种声明和定义的顺序。&emsp;&emsp;比如在上例的友元类中，将类B中的函数fun定义为A的友元成员函数：friend void B::fun(A &amp;a) {a.data = 0; cout &lt;&lt; a.data &lt;&lt; endl;}。但是编译器在处理这条语句时，必须要知道类B的定义，否则无法判断B是一个类，所以需要将B的定义放在A的前面。但B中的fun函数方法又提到了A对象，所以A的定义应该在B的前面，这样就陷入了循环中，为此需要利用前向声明来避免这种问题。所以最终的排序顺序如下：123class A; //前向声明class B &#123;...&#125;;class A &#123;...&#125;; &emsp;&emsp;但此时又会出现另外一个问题，即B中的fun函数是内联函数，用到了A中的数据，但A的定义在后面，所以通常将fun的声明放在前面，fun的定义使用外部定义的方法放在后面。 实现&emsp;&emsp;最终的实现如下：123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;using namespace std;class A; //前向声明class B &#123;public: void fun(A &amp;a) ;&#125;;class A&#123;public: friend void B::fun(A &amp;a); //友元成员函数声明 private: int data;&#125;;void B::fun(A &amp;a) //友元函数的定义 &#123; a.data = 0; cout &lt;&lt; a.data &lt;&lt; endl; &#125;int main(void)&#123; class A a; class B b; b.fun(a); return 0;&#125; &emsp;&emsp;程序结果：10 小结优点： 可以灵活地实现需要访问若干类的私有或受保护的成员才能完成的任务； 便于与其他不支持类概念的语言(如C语言、汇编等)进行混合编程； 通过使用友元函数重载可以更自然地使用C++语言的IO流库。 缺点： 一个类将对其非公有成员的访问权限授予其他函数或者类，会破坏该类的封装性，降低该类的可靠性和可维护性。","categories":[{"name":"C++","slug":"C","permalink":"http://cxx0822.github.io/categories/C/"}],"tags":[]},{"title":"C++重难点：虚函数与虚继承","slug":"C++重难点：虚函数与虚继承","date":"2019-03-07T00:49:19.000Z","updated":"2019-03-12T01:11:29.862Z","comments":true,"path":"2019/03/07/C++重难点：虚函数与虚继承/","link":"","permalink":"http://cxx0822.github.io/2019/03/07/C++重难点：虚函数与虚继承/","excerpt":"","text":"&emsp;&emsp;C++是一种面向对象的编程语言，其主要的特点是封装、继承和多态。其中继承指的是可以将一个类作为基类，并将另一个类继承于它，作为它的派生类。但在多重继承或存在一些复杂的继承关系时，可能会出现一些二义性，通常我们可以用虚函数与虚继承来避免这些问题。 虚函数&emsp;&emsp;在某基类中声明为virtual并在一个或多个派生类中被重新定义的成员函数称为虚函数。主要用于多继承时，由于成员函数名称相同，调用出现二义性的问题。&emsp;&emsp;声明格式：virtual 函数返回类型 函数名(参数表){函数体}; 问题出现&emsp;&emsp;假设现在有1个基类，2个派生类都继承该基类，且每个类中都有1个名称相同的成员函数。如：123456789101112131415161718192021#include &lt;iostream&gt;using namespace std;class A&#123;public: void fun1() &#123;cout &lt;&lt; \"A::fun1\" &lt;&lt; endl;&#125;&#125;;class B :public A&#123;public: void fun1() &#123;cout &lt;&lt; \"B::fun1\" &lt;&lt; endl;&#125;&#125;;class C :public A&#123;public: void fun1() &#123;cout &lt;&lt; \"C::fun1\" &lt;&lt; endl;&#125;&#125;; &emsp;&emsp;A是基类，B，C是派生类，都继承于A类，且都有1个函数名为fun1的函数，如果我们使用各自的类类型实例化各自的对象，则调用fun1函数完全没问题，如：12345678910int main()&#123; A a1; B *b1 = new B; C c1; a1.fun1(); b1-&gt;fun1(); c1.fun1(); return 0;&#125; &emsp;&emsp;程序输出为：123A::fun1B::fun1C::fun1 &emsp;&emsp;但当我们想用基类A去声明2个派生类B，C的对象时（B，C都是继承于A，当然可以用基类去声明派生类），如：12345678int main()&#123; A *a2 = new B; a2-&gt;fun1(); A *a3 = new C; a3-&gt;fun1(); cout &lt;&lt; endl;&#125; &emsp;&emsp;此时，我们预期的结果是输出B::fun1 C::fun1但实际输出的结果是A::fun1 A::fun1。这就是因为3个类中的fun1函数名都相同，出现了二义性，都调用了基类的fun1函数。 解决办法&emsp;&emsp;解决的办法就是在基类中的fun1成员函数最前面的修饰符中加上虚函数的关键字virtual（当然派生类的成员函数前面也可以加上），这样fun1就是一个虚函数，类在调用的时候则会根据实际情况调用。如：1234567891011virtual void fun1() &#123;cout &lt;&lt; \"A::fun1\" &lt;&lt; endl;&#125;int main()&#123; A *a2 = new B; a2-&gt;fun1(); A *a3 = new C; a3-&gt;fun1(); cout &lt;&lt; endl; return 0;&#125; &emsp;&emsp;程序输出为：12B::fun1C::fun1 应用场景&emsp;&emsp;当然，我们可能会觉得这样不是更复杂么？直接声明各自的对象不就可以了么？如果是那样的话，C++就体现不会多态的特性了。比如，我们声明一个对象数组，该数组中的每个元素都是一个对象，但是数组的数据格式必须统一，不能既是A的对象又是B的对象，那这样我们就不能调用各自类中的函数，那么也就体现不出C++多态的特性了。&emsp;&emsp;但我们可以这样做，声明一个指向基类A的对象数组，即数组里面都是指向A类的指针，然后通过加入虚函数特性，这样在调用各自的成员函数时，就不会出现二义性问题，数组的数据格式也是一样的。这也反映了多态的思想。如：12345678910int main()&#123; A *a[2]; a[0] = new B; a[1] = new C; a[0]-&gt;fun1(); a[1]-&gt;fun1(); return 0;&#125; &emsp;&emsp;程序结果：12B::fun1C::fun1 基本原理——动态联编&emsp;&emsp;由于C++的函数可以重载，再加上指针和引用，使得程序在调用函数时，特别是多个重名的函数时，使用哪个可执行的代码块是一个非常复杂的问题。&emsp;&emsp;将源代码中的函数调用解释为执行特定的函数代码块称为函数名联编。C++一共有2种方式，一种是静态联编，即编译器可以在编译过程中完成联编，另一种是动态联编，即由于编译器不知道该选择哪种类型的对象，必须生成能够在程序运行时选择正确的虚方法的代码。&emsp;&emsp;动态联编主要与指针和引用的调用方法有关。C++不允许将一种类型的地址赋值给另一种类型的指针，比如12double x = 1.2;int *p = &amp;x; long &amp;r = x; &emsp;&emsp;但是指向基类的引用或指针可以引用派生类对象，虽然基类和派生类并不是同一种数据类型（类也是一种数据类型，即用户自定义数据类型），但派生类是由基类继承而来，所以这种引用被称为向上强制转换。但向下是不可以的。&emsp;&emsp; 向上强制转换使基类指针或引用可以指向基类对象或派生类对象，因此需要动态联编，C++使用虚成员函数来满足这种要求。 &emsp;&emsp;例如在上例中，如果fun1函数没有声明为虚的，当利用指针创建A *a2 = new B;和A *a3 = new C;对象并调用fun1()函数时，a2和a3将根据指针类型A *来调用A::fun1()，指针类型在编译时已知，因此编译器对非虚方法使用静态联编。&emsp;&emsp;但是，当fun1函数声明为虚函数时，a2和a3将根据对象类型来确定，其中a2为B类，a3为C类，由此可见，编译器生成的代码是在程序执行时才根据对象类型将fun1关联到B::fun1()或C::fun1()，所以编译器对虚方法使用的是动态联编。 &emsp;&emsp;在大多数情况下，动态联编很好，因为程序能够选择为特定类型设计的方法，但是静态联编的效率更高，因为动态联编需要额外的内存开销（见深层原理分析），所以一般我们可以这样设计：如果要在派生类中重新定义基类的方法，则将它设置为虚方法，否则设置为非虚方法。 深层原理——虚函数表&emsp;&emsp;编译器处理虚函数的方法是：给每个对象添加一个隐藏成员，该隐藏成员中保存了一个指向函数地址数组的指针，这种数组称为虚函数表。虚函数表中存储了为类对象进行声明的虚函数的地址。&emsp;&emsp;虚函数表的变化：如下图所示，类A是基类，类B是派生类，每个对象都有1个针对虚函数的虚函数表，其中基类A有2个虚函数，地址分别为4064和6400，类B有3个虚函数，其中第一个虚函数是继承于基类A的并且未重新定义，则类B的虚函数表直接将基类A对应的虚函数地址复制下来；第二个虚函数也是继承于基类A的，但是已经重新定义了，则会产生一个新的虚函数地址；第三个虚函数是类B本身的，所以该虚函数地址也是新的。&emsp;&emsp;每个类只有1个虚函数表，每次只需要在表中添加1个地址，只是表的大小不同而已。&emsp;&emsp;当调用虚函数，首先会找到该虚函数表（该表也是有地址的），然后在表中找到相应的函数地址，最后根据地址调用函数。所以这也就是为什么虚函数需要额外的开销。因为首先要占用一定的存储空间来存放虚函数地址表，其次根据在表中寻找合适的函数地址也需要一定的运行时间。 注意事项 构造函数构造函数不能是虚函数。因为在创建派类对象时，将调用派生类的构造函数，而不是基类的构造函数。 析构函数析构函数应该是虚函数，除非不用做基类。 12A *p = new B;delete p; &emsp;&emsp;在上例中，当delete对象时，如果不是虚函数，将调用基类A的析构函数，这将释放基类指向的内存，但不会释放派生类的内存，但如是虚函数，则会先释放派生类的内存，在释放基类的内存。所以通常给基类提供一个虚析构函数。 友元友元不能是虚函数，因为友元不是类成员，只有成员才能是虚函数。 虚继承&emsp;&emsp;在继承定义中包含了virtual关键字的继承关系被称为虚继承，在虚继承体系中的通过virtual继承而来的基类被称为虚基类。主要用于多重继承(如菱形继承)时，函数不知归属于哪个类的问题。&emsp;&emsp;声明格式：class 派生类类名： virtual [继承方式] 基类类名。 问题出现&emsp;&emsp;假设现在有一种复杂的多重继承方式，如有1个基类A，2个派生类B，C都继承于基类A，派生类D又继承于B和C。如下图所示：12345678910111213141516171819202122#include &lt;iostream&gt;using namespace std;class A&#123;public: void fun1() &#123;cout &lt;&lt; \"A::fun1\" &lt;&lt; endl;&#125;&#125;;class B :public A&#123;&#125;;class C :public A&#123;&#125;;class D :public B, public C&#123;&#125;; &emsp;&emsp;当我们利用派生类D去声明一个对象，并调用fun1函数时，会出现问题。123456int main()&#123; D d; d.fun1(); return 0;&#125; &emsp;&emsp;程序会报错，即error C2385: 对“fun1”的访问不明确，因为派生类B和C都继承于A，所以派生类D中会有2份fun1函数，这样编译器就不知道该选择哪个函数了。 解决办法&emsp;&emsp;有一种解决办法就是将fun1函数在派生类D重写或者调用时指明用哪个类的fun1，如D.B::fun1();，但这种并不是最好的解决办法，因为这样会有2个副本，占用额外的内存空间。&emsp;&emsp;还有一种比较好的解决办法是将继承方式声明为虚继承。12345678910111213141516171819class A&#123;public: void fun1() &#123;cout &lt;&lt; \"A::fun1\" &lt;&lt; endl;&#125;&#125;;class B :virtual public A&#123;&#125;;class C :virtual public A&#123;&#125;;class D :public B, public C&#123;&#125;; &emsp;&emsp;public和virtual的位置无所谓。这样的话，继承关系不变，但它们只会保留一个副本(这个副本既不来自于B，也不来自于C，是从A中单独拷贝出来的)，在调用的时候也不会产生错误。 实现原理&emsp;&emsp;为了更好的分析其实现原理，我们可以调用visual studio的内存布局管理。&emsp;&emsp;在解决方案管理器中选择.cpp文件，然后右击选择属性，在打开的窗口中选择命令行，然后在其他选项中输入查看内存布局的命令：12/d1 reportSingleClassLayout[className] //查看单个类/d1 reportAllClassLayout //查看所有类 &emsp;&emsp;确认后，按下F7即可查看内存布局情况。1234567891011121314151617181920212223class A&#123;public: int a;&#125;; class B : public A&#123;public: int b;&#125;; class C : public A&#123;public: int c;&#125;; class D : public B, public C&#123;public: int d;&#125;; &emsp;&emsp;B，C声明为普通继承时，D的内存布局：&emsp;&emsp;左边的数字表示类中成员在类中排列的起始地址。由图易知，类D一共有20个字节，1个int占4个，一共有5个，为什么有5个int？因为类A的int a在类B和类C中都复制了一份，所以在调用的时候当然不知道该调用哪一个了。&emsp;&emsp;我们再将B，C声明为虚继承，查看D的内存布局：&emsp;&emsp;可以看出，和之前的内存分布还是很不一样的，现在大概分成了3块，两块是B和C的数据加上一个vbptr的指针，该指针是虚基类表指针，指向一个虚表（和上文提到的虚函数表类似），表的内容在下面有显示，第二项表示vbptr到共有基类元素之间的偏移量，比如类B中的vbptr指向了虚表D::$vbtable@B@，可以看出，公共基类A的成员变量a距离类B开始处的位移为20（为什么一个是20一个是12查了很多资料也没搞明白，还望有会的大佬留言指教），这样根据这个虚表就可以找到基类中的数据了。&emsp;&emsp;还有一块是基类A中的数据，这也就是为什么利用虚继承只会出现一个副本。 纯虚函数&emsp;&emsp;除了虚函数和虚继承，关于virtual关键字还有一种用法：纯虚函数。一般用于声明一个函数但不实现它，让派生类去实现。&emsp;&emsp;声明格式：virtual 函数返回类型 函数名(参数表)=0;&emsp;&emsp;至少有1个虚函数是纯虚函数的基类称为抽象类。抽象类不可实例化，相当于一个接口。1234567891011121314#include &lt;iostream&gt;using namespace std;class A&#123;public: virtual void fun1() = 0;&#125;;int main()&#123; A a; return 0;&#125; &emsp;&emsp;程序会报错：error C2259: “A”: 不能实例化抽象类。 实例1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;using namespace std;class A&#123;public: virtual void fun1() = 0;&#125;;class B :virtual public A&#123;public: void fun1() &#123;cout &lt;&lt; \"B::fun1()\" &lt;&lt; endl;&#125;&#125;;class C :virtual public A&#123;public: void fun1() &#123;cout &lt;&lt; \"C::fun1()\" &lt;&lt; endl;&#125;&#125;;int main()&#123; B b; C *c = new C; b.fun1(); c-&gt;fun1(); return 0;&#125; &emsp;&emsp;程序结果： B::fun1() C::fun1() 总结&emsp;&emsp;从一个大佬的博客上拷贝下来的，有些地方仍然不是太理解(学无止境啊)。 虚基类1. 一个类可以在一个类族中既被用作虚基类，也被用作非虚基类。 2.在派生类的对象中，同名的虚基类只产生一个虚基类子对象，而某个非虚基类产生各自的子对象。 3.虚基类子对象是由最派生类的构造函数通过调用虚基类的构造函数进行初始化的。 4.最派生类是指在继承结构中建立对象时所指定的类。 5.派生类的构造函数的成员初始化列表中必须列出对虚基类构造函数的调用；如果未列出，则表示使用该虚基类的缺省构造函数。 6.从虚基类直接或间接派生的派生类中的构造函数的成员初始化列表中都要列出对虚基类构造函数的调用。但只有用于建立对象的最派生 类的构造函数调用虚基类的构造函数，而该派生类的所有基类中列出的对虚基类的构造函数的调用在执行中被忽略，从而保证对虚基类子对象 只初始化一次。 7.在一个成员初始化列表中同时出现对虚基类和非虚基类构造函数的调用时，虚基类的构造函数先于非虚基类的构造函数执行。 虚函数1.虚函数是非静态的、非内联的成员函数，而不能是友元函数，但虚函数可以在另一个类中被声明为友元函数。 2.虚函数声明只能出现在类定义的函数原型声明中，而不能在成员函数的函数体实现的时候声明。 3.一个虚函数无论被公有继承多少次，它仍然保持其虚函数的特性。 4.若类中一个成员函数被说明为虚函数，则该成员函数在派生类中可能有不同的实现。当使用该成员函数操作指针或引用所标识的对象时 ，对该成员函数调用可采用动态联编。 5.定义了虚函数后，程序中声明的指向基类的指针就可以指向其派生类。在执行过程中，该函数可以不断改变它所指向的对象，调用不同 版本的成员函数，而且这些动作都是在运行时动态实现的。虚函数充分体现了面向对象程序设计的动态多态性。 纯虚函数 版本的成员函数，而且这些动作都是在运行时动态实现的。虚函数充分体现了面向对象程序设计的动态多态性。 纯虚函数1.当在基类中不能为虚函数给出一个有意义的实现时，可以将其声明为纯虚函数，其实现留待派生类完成。 2.纯虚函数的作用是为派生类提供一个一致的接口。 3.纯虚函数不能实化化，但可以声明指针。","categories":[{"name":"C++","slug":"C","permalink":"http://cxx0822.github.io/categories/C/"}],"tags":[]},{"title":"Linux的文件与目录","slug":"Linux的文件与目录","date":"2019-03-05T09:02:56.000Z","updated":"2020-01-09T07:20:10.388Z","comments":true,"path":"2019/03/05/Linux的文件与目录/","link":"","permalink":"http://cxx0822.github.io/2019/03/05/Linux的文件与目录/","excerpt":"","text":"&emsp;&emsp;在Linux系统中，所有的软件和硬件都是以文件的形式存在的。 文件&emsp;&emsp;在Linux中的每一个文件或目录都包含有访问权限，这些访问权限决定了谁能访问和如何访问这些文件和目录。 用户与用户组&emsp;&emsp;Linux是一个多用户、多任务的环境，一般将用户分为3个类别：owner，group和others，每个类别各有read，write和execute等权限。此外，还有1个超级用户root，可以访问任何类别的任何文件。&emsp;&emsp;一般身份和root的相关信息记录在/etc/passwd文件内，个人的密码记录在/etc/shadow文件内，组名的信息记录在etc/group文件内。&emsp;&emsp;切换用户命令：su [-] ownername。不加-表示只能获得执行权限，加了表示获得环境变量及执行权限。ownername默认为root。普通用户切换到root需要密码，root切换到普通用户不需要密码，root输入exit可以退出root。注：如果进入root权限时显示su:Authentication failure则是因为之前没有设置过root的密码，输入命令sudo passwd并按照提示即可设置密码。 文件属性（图）输入命令查看文件 &emsp;&emsp;第一列：文件的类型和权限&emsp;&emsp;第一列一共有10个字符，第1个字符表示的是文件是目录或文件。| d | - | l | b | c || :———: | :———: | :———: | :———: | :———: ||目录|文件|链接文件|可供存储的接口设备|串行端口设备|&emsp;&emsp;接下来的9个字符，3个为1组，一共有3组，分别为r(可读)，w(可写)和x(可执行)权限，如果没有权限则用-表示。3组分别为文件所有者的权限，同用户组的权限和其他非本用户组的权限。&emsp;&emsp;read：可以读取文件的内容，write：可以编辑、增加或修改文件的内容（但不能删除），execute：可以被系统执行。 &emsp;&emsp;第二列：有多少个文件名连接到此节点&emsp;&emsp;第三列：所有者账号&emsp;&emsp;第四列：所属用户组&emsp;&emsp;第五列：容量大小，单位为B&emsp;&emsp;第六列：创建日期或最近的修改日期&emsp;&emsp;第七列：文件名 改变文件属性与权限&emsp;&emsp;常用的有3个命名：chgrp，chown和chmod。 chgrp&emsp;&emsp;命令格式：chgrp groupame dirname/filename chown&emsp;&emsp;命令格式：chgrp ownerame dirname/filename chmod&emsp;&emsp;设置权限的方式有两种，分别是数字和符号，比较常用的是数字设置。其中read是4，write是2，execute是1。每个类别的权限为这3种权限的和。例有一个文件的权限为rwxrwx---，则owner的数字为：4+2+1=7，group的数字为：4+2+1=7，others的数字为0+0+0=0，总的数字为774。命令格式为：chmod xyz dirname/filename。（xyz为各个类别的数字。） 文件种类&emsp;&emsp;常见的Linux文件类型有：普通文件、目录、连接文件、设备文件、套接字(sockets,s)和管道(pipe,p)。文件类型可以在文件属性的第一个字符中查到。 普通文件(-)&emsp;&emsp;普通文件又可以分为纯文本文件、二进制文件和数据格式文件。 目录(directory,d)连接文件(link,l)&emsp;&emsp;类似于Windows系统的快捷方式。 设备与设备文件(device)&emsp;&emsp;与系统外设及存储等相关的文件，集中在/dev目录下。又分为两种，一种是块设备文件(block,b)，如硬盘，软盘等，另一种是字符设备(character,c)，如键盘，鼠标等。","categories":[{"name":"Linux系统","slug":"Linux系统","permalink":"http://cxx0822.github.io/categories/Linux系统/"}],"tags":[]},{"title":"基于Ubuntu系统的NAO开发01：安装系统并配置python环境","slug":"基于Ubuntu系统的NAO开发01：安装系统并配置python环境","date":"2019-02-26T05:14:15.000Z","updated":"2020-01-09T07:37:57.204Z","comments":true,"path":"2019/02/26/基于Ubuntu系统的NAO开发01：安装系统并配置python环境/","link":"","permalink":"http://cxx0822.github.io/2019/02/26/基于Ubuntu系统的NAO开发01：安装系统并配置python环境/","excerpt":"","text":"安装Ubuntu系统&emsp;&emsp;Ubuntu是基于linux的免费开源桌面PC操作系统。本博客是在Windows10系统基础上，安装Ubuntu双系统，为了配合NAO机器人的使用，选择安装Ubuntu14.04版本。 制作系统盘&emsp;&emsp;首先准备一个空u盘（格式化后的）制作系统盘。可以选择UltraISO软件制作，制作过程也十分简单。先下载Ubuntu14.04的镜像文件(选择桌面版desktop-amd64)。&emsp;&emsp;然后打开UltraISO软件，选择文件-&gt;打开，选择下载好的镜像文件，然后选择启动-&gt;写入硬盘镜像，最后在弹出的对话框中依次选择格式化、写入即可。 硬盘分区&emsp;&emsp;建议先分区，再安装双系统。利用Win10自带的磁盘管理功能分区。&emsp;&emsp;右击电脑，选择管理-&gt;磁盘管理，选择任意一个空间较大的盘符，比如选择D盘符，然后右击选择压缩卷，输入压缩空间量即可实现硬盘的分区。比如从D盘分100G的空间给Ubuntu，则输入1000*1024MB = 102400MB即可。&emsp;&emsp;压缩完后不要进行任何操作，以便安装Ubuntu系统的时候可以直接找到该空间。 正式安装Ubuntu系统&emsp;&emsp;在安装前，先检查一下BIOS是否关闭安全启动（禁止安装其他没有经过微软验证的系统）：在BIOS中找到Security-&gt;Secure Boot，选择Disabled。&emsp;&emsp;然后插入制作好的系统盘，重启电脑并选择u盘启动，即可进入Ubuntu的安装界面。&emsp;&emsp;在启动界面中选择第一个Try Ubuntu without installing或第二个Install Ubuntu。&emsp;&emsp;语言环境建议选择英文，方面后面的文件路径处理。&emsp;&emsp;这个可以勾选也可以不勾选，安装完系统后再装也行。&emsp;&emsp;这一步是最重要的分区界面，建议选择最后一个自定义分区。&emsp;&emsp;进入后可以看到整个硬盘的空间分布，在Linux系统中，所有的软件和硬件都是以文件的形式存在的，硬盘也是文件，在根目录下的dev文件夹下，其中SCSI/SATA/USB硬盘都是以sd+序号开头的，如图可以看到并没有C,D,E,F盘的概念，只有sd1-sd6。其中白色区域的free space就是刚才压缩的盘符。（这里为50G。）在列表中找到该区域，左下角会出现+按钮，点击即可进行分区。 Ubuntu分区：对于整个linux系统来说，至少要有两个分区，一个是/分区，就是根分区，一个是swap分区，就是交换分区。 根分区/根目录：是所有目录的绝对路径的起始点，Ubuntu 中的所有文件和目录都在跟目录下。相当于Windows的C盘（假设只有1个盘）。 Swap分区：在系统的物理内存不够用的时候，把硬盘空间中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间被临时保存到Swap分区中，等到那些程序要运行时，再从Swap分区中恢复保存的数据到内存中。相当于Windows的虚拟内存。&emsp;&emsp;在Red Hat Enterprise Linux中建议的Swap分区大小。 物理内存 Swap分区 &lt;=4 GB 至少4 G 4~16 GB 至少8 G 16~64 GB 至少16 G 64~256 GB 至少32 G &emsp;&emsp;但一般不建议只分2个区，Linux系统没有盘符的概念，所有的东西都存储在根目录下面的文件夹中，如果只有2个区，则相当于Windows系统中只有C盘，没有program files，documents and settings这种文件夹，这样用户数据和Windows都在一个分区里，这是十分危险的。所以在Ubuntu系统中，至少要划出/分区，/home分区和swap三个分区。其中/home分区就是存放用户数据和应用程序设置的文件夹，相当于Windows下的documents and settings文件夹。重新安装系统，会格式化/分区，这样/home分区的内容就可以保留了。&emsp;&emsp;一般只需划分这3个分区即可，其余默认即可。&emsp;&emsp;以下是我的分区方案，也可以自己选择。 分区名称 含义 / 根目录，是所有目录的绝对路径的起始点，Ubuntu 中的所有文件和目录都在跟目录下 /etc 此目录非常重要，绝大多数系统和相关服务的配置文件都保存在这里，这个目录的内容一般只能由管理员进行修改。像密码文件、设置网卡信息、环境变量的设置等都在此目录中 /home 系统默认的用户家目录，新增用户账号时，用户的家目录都存放在此目录下，~表示当前用户的家目录，~test表示用户test的家目录。建议单独分区，并设置较大的磁盘空间，方便用户存放数据 /bin 此目录中放置了所有用户能够执行的命令 /sbin 此目录中放置了一般是只有系统管理有才能执行的命令 /dev 存放linux系统下的设备文件，访问该目录下某个文件，相当于访问某个设备，常用的是挂载光驱mount/dev/cdrom/mnt /mnt 此目录主要是作为挂载点使用 /usr 此目录包含了所有的命令、说明文件、程序库等，此目录下有很多重要的目录，常见的有：/usr/local 这个目录包含管理员自己安装的程序；/usr/share 包含文件的帮助文件；/usr/bin 和/usr/sbin 包含了所有的命令 /var 包含了日志文件、计划性任务和邮件等内容 /lib 包含了系统的函数库文件 /lost+found 包含了系统修复时的回复文件 /tmp 包含了临时的文件。一般用户或正在执行的程序临时存放文件的目录,任何人都可以访问,重要数据不可放置在此目录下 /boot 系统的内核所在地，也是启动分区。放置linux系统启动时用到的一些文件。/boot/vmlinuz为linux的内核文件，以及/boot/gurb /media 主要用于挂载多媒体设备 /root 系统管理员的宿主目录 &emsp;&emsp;注：安装系统后，需要更换国内源和软件更新，具体可以搜索其他博客学习。 配置NAO环境&emsp;&emsp;配置方法可以参考NAO的官方文档。&emsp;&emsp;注：下载的python-linux-SDK版本（32/64）要和Ubuntu系统一致。 安装python&emsp;&emsp;Ubuntu系统自带python2，所以无需安装。 安装python SDK&emsp;&emsp;根据官网的提示，进入NAO社区下载。我安装的系统是Ubuntu64位，所以选择64位的python SDK。百度云链接 提取码：8acy。&emsp;&emsp;先压缩下载好的文件，可以选择命令行压缩也可以直接右击文件选择Extract Here。压缩命令为：tar -xvf soft.tar.gz.tar，要先进入目标文件夹，如：12cd /home/cxx/NAO/NAOUbuntutar -xvf pynaoqi-python2.7-2.1.4.13-linux64.tar.gz.tar （按TAB键盘可自动补全） &emsp;&emsp;解压后将文件夹名改为python-sdk。 添加环境变量&emsp;&emsp;常用的方法是修改Ubuntu系统中存储环境变量的文件，如/etc/profile或/etc/bashrc，修改需要root权限。&emsp;&emsp;打开终端，进入文件夹后，使用Ubuntu自带的gedit打开文件，如：12cd /etcsudu gedit profile &emsp;&emsp;打开文件后，在最后一行输入官方文档给的环境变量名+文件路径：export PYTHONPATH=${PYTHONPATH}:/home/cxx/NAO/NAOUbuntu/python-sdk。保存并重启，然后在终端依次输入：python和import naoqi，如果没有报错则安装成功。 安装Choregraphe软件&emsp;&emsp;和python SDK一样，进入社区下载软件，百度云链接提取码：i2s7。然后用命令行或右击解压文件，解压命令tar -xzvf soft.tar.gz，如：12cd /home/cxx/NAO/NAOUbuntutar -xzvf choregraphe-suite-2.1.4.13-linux64.tar.gz &emsp;&emsp;解压完成后会进入文件夹，并打开bin文件夹中的choregraphe-bin即可。 NAO测试&emsp;&emsp;python环境的NAO测试，可以参考另一篇博客：基于python的NAO机器人开发01","categories":[{"name":"NAO开发与应用","slug":"NAO开发与应用","permalink":"http://cxx0822.github.io/categories/NAO开发与应用/"}],"tags":[]},{"title":"基于C++的NAO机器人开发01：配置C++环境","slug":"基于C++的NAO机器人开发01：配置C++环境","date":"2019-01-10T05:40:43.000Z","updated":"2020-01-09T07:21:30.447Z","comments":true,"path":"2019/01/10/基于C++的NAO机器人开发01：配置C++环境/","link":"","permalink":"http://cxx0822.github.io/2019/01/10/基于C++的NAO机器人开发01：配置C++环境/","excerpt":"","text":"基于C++的NAO机器人开发01：配置C++环境&emsp;&emsp;NAOdocument提供了官方的配置说明文档，官方文档，本博客也是基于这篇文档配置的。本博客基于Windows平台建立，其他平台可参考官方文档。 所需软件 操作系统：windows10 64位 编译器：Visual Studio 2010 qiBuild：生成交叉编译工程(具体见后面解释) CMake：跨平台编译软件，生成qiBuild所需的工程 python2&emsp;32位 naoqi C++ SDK 安装Visual Studio 2010&emsp;&emsp;百度云：链接：https://pan.baidu.com/s/1W58DQagcEN-AWlgNchaVUw提取码：0gey。下载解压后正常安装即可。软件大小约6.8G。&emsp;&emsp;只能是2010版的！官方文档说的2013版本装了编译不了，太坑了，可能是还没有C++SDK的vs2013版的。 为什么要装qibuild和CMake&emsp;&emsp;编译一共分2种：本地编译和交叉编译。本地编译：在当前编译平台下，编译出来的程序放到当前平台下运行。交叉编译：在当前编译平台下，编译出来的程序运行在体系结构不同的另一种目标平台上。因为我们是在本地计算机上面编译代码，然后要运行在NAO的操作系统里面，两者系统结构并不一样，所以需要使用交叉编译。&emsp;&emsp;qiBuild就是一个可以产生交叉编译工程文件的工具。&emsp;&emsp;CMake是一个跨平台的编译工具，可以用简单的语句来描述所有平台的编译过程。他能够输出各种各样的makefile或者project文件，包括Windows, Mac, Linux 和NAOqi OS。所以我们使用CMake来编译qiBuild产生的交叉编译工程文件，并写入NAO操作系统中。&emsp;&emsp;也就是说我们首先使用qiBuild生成一个可以交叉编译的文件，然后用CMake去编译，从而生成visual studio的工程文件，这样我们下次再去编写代码时，只需要在visual studio里面修改了，而且CMake编译的文件可以跨平台使用，可以直接传给NAO的操作系统使用。 (以上均个人理解) 安装CMake和python2&emsp;&emsp;CMake需要2.8.3以上的版本，网址。 尽量选择32位版本的，且是.msi的文件。&emsp;&emsp;注：安装时选择添加环境变量至所有用户，安装路径不要有中文。&emsp;&emsp;检查是否安装成功：进入cmd命令行：输入cmake，显示信息则安装正确。&emsp;&emsp;python的安装参考另一篇博客：基于python的NAO机器人开发01。 qiBuild的安装与配置&emsp;&emsp;qiBuild使用python的pip包管理来安装，进入cmd命令行，输入pip install qibuild即可。(如果安装了python2和3，一定要先切换到python2再安装。)&emsp;&emsp;安装完首先初始化设置，继续输入qibuild config --wizard并按照提示完成相应配置(选择visual studio 2010)，当然也可以再次输入重新配置。这一步相当于指定编译器类型。&emsp;&emsp;新建一个文件夹myWorktree(可以是其他名字)作为工作路径(不能放在中文文件夹下)，然后用cd命令切换到该文件夹下，初始化工作路径，输入命令：qibuild init。完成后会生成一个.qi的文件夹。 安装C++ SDK并配置&emsp;&emsp;进入NAO社区网站并下载windows版本的naoqi C++ SDK，网址。链接：https://pan.baidu.com/s/1Cic_8saHFx_uo_ge_sGDag提取码：o2qe，下载并解压重命名为naoqi-sdk，和之前的工作路径文件夹myWorktree放在同级目录。&emsp;&emsp;创建一个工具链，即建立一个交叉编译环境，输入命令：qitoolchain create mytoolchain /.../naoqi-sdk/toolchain.xml。mytoolchain为工具链的名字，/…/naoqi-sdk/toolchain.xml为naoqi的文件路径(可以先进入naoqi所在盘符名，在输入该命令，如qitoolchain create mytoolchain /NAO/naoqi-sdk/toolchain.xml)，最后cd进入工作路径文件夹myWorktree，生成配置文件qibuild add-config myconfig -t mytoolchain --default。&emsp;&emsp;注：我之前配置过，所以显示already exists。 新建qiBuild工程并测试hello world&emsp;&emsp;首先建立一个不涉及NAOqi库的工程。具体步骤见官方文档：官方文档。&emsp;&emsp;cd命令进入工作路径文件夹myWorktree，然后新建一个工程文件：1qisrc create myFirstExample &emsp;&emsp;cd命令进入该工程文件：1cd myFirstExample &emsp;&emsp;然后使用qibuild配置该文件1qibuild configure &emsp;&emsp;最后使用qibuild生成编译文件1qibuild make &emsp;&emsp;在生成的build-myconfig文件夹中可以找到visual studio的工程文件(后缀为.sln的文件)。&emsp;&emsp;打开后会发现一个有5个工程文件，其中只有my_first_example文件包含main.cpp，其余4个可以移除。并右击设置该文件夹为启动项。&emsp;&emsp;打开并编译运行main.cpp，可以看到输出的hello world。当然也可以用cd命令行找到其exe可执行文件直接运行，在build-myconfig/sdk/bin中。12cd build-myconfig/sdk/binmyfirstexample_d.exe moveHead&emsp;&emsp;下面建立一个包含naoqi库的一个案例，可以参考旧版的官方文档。官方文档。&emsp;&emsp;首先要建立一个工程文件movehead(即qisrc create movehead)，此时会生成一些文件，具体含义见官方文档说明。&emsp;&emsp;其次就是修改其中的main.cpp和CMakeLists.txt文件。我们采用官方文档的一个例子：movehead。&emsp;&emsp;将main.cpp里面的内容替换成movehead.cpp的内容(文件名也要修改为movehead.cpp)，CMakeLists.txt的内容替换成该例子中CMakeLists的内容。 CMakeLists.txt文件说明&emsp;&emsp;该文件一共有5行命令。具体含义见官方文档注释。比较重要的一行是最后一行，其含义为编译文件所需要包含的naoqi库文件，且均大写，也就是你编写的NAO代码需要用到哪些库，比如movehead这个例子，其头文件为12#include &lt;alerror/alerror.h&gt;#include &lt;alproxies/almotionproxy.h&gt; &emsp;&emsp;那么就需要将相应的的库放在最后一行。 编译工程文件&emsp;&emsp;编译方法有2种，一种是之前的命令行编译，还有一种是使用CMake软件编译，其原理是一样的。下面介绍CMake编译(编译之前要先建立工程文件夹)：&emsp;&emsp;打开CMake软件，在最上面的Source中选择工程文件夹，Build中选择工程文件夹中的Build文件夹(需要自己新建，和main.cpp位于同级目录)。&emsp;&emsp;然后点击下面的configure，选择VS2010编译器和交叉编译，在选择toolchain，在之前的naoqi-sdk文件夹下的toolchain-pc.cmake。&emsp;&emsp;等待一会，会全部红色高亮，再次点击Congfigure和Generate。此时打开build文件夹，找到vs2010的工程文件，打开编译执行即可。&emsp;&emsp;如果报错检查CMakeLists.txt中的项目文件夹名，.cpp文件名和相应的库名是否正确。&emsp;&emsp;注：如果不是第一次configure，需要清除缓存。file-&gt;Delete Cache。 Choregraphe调试&emsp;&emsp;打开Choregraphe软件，连接一个虚拟机器人，并在movehead.cpp中输入相应的ip地址和端口号，(删除main()中的if判断，并修改其中一行改为：AL::ALMotionProxy motion(&quot;127.0.0.1&quot;, 60770);，具体IP和端口号查看Choregraphe软件。)编译执行即可在机器人视图中看到效果。其实现方法和python类似，可参考另一篇博客基于python的NAO机器人开发01。","categories":[{"name":"NAO开发与应用","slug":"NAO开发与应用","permalink":"http://cxx0822.github.io/categories/NAO开发与应用/"}],"tags":[]},{"title":"K均值算法","slug":"K均值算法","date":"2019-01-08T05:17:54.000Z","updated":"2019-01-14T02:56:07.519Z","comments":true,"path":"2019/01/08/K均值算法/","link":"","permalink":"http://cxx0822.github.io/2019/01/08/K均值算法/","excerpt":"","text":"K均值算法一、基本原理&emsp;&emsp;K-均值算法属于无监督的聚类算法，所谓无监督就是原始数据集中的分类标签是未知的，聚类的意思为将相似的对象归到同一个簇中。k指的是将数据集分成几个簇，而均值的意思是，判断是否为相同类别的标准为数据的均值。&emsp;&emsp;K-均值算法首先随机确定k个初始点作为质心(均值)，然后计算数据集中每个点和每个质心的距离，并将其分配给距离最近的质心的类别中，这样就会产生了k个类别，然后再更新每个类别的质心，重复计算，直到每个类别的质心不在变化或满足迭代次数。&emsp;&emsp;伪代码为：1234567创建k个点作为起始质心(随机选择)当任意一个点的簇分配结果发生改变时 对数据集的每个数据点 对每个质心 计算质心与数据点之间的距离 将数据点分配到距离最近的簇 对每一个簇，计算簇中所有点的均值并将均值作为质心 二、算法实现准备数据集123456783.275154 2.957587 -3.344465 2.603513 0.355083 -3.376585 1.852435 3.547351...2.960769 3.079555-3.275518 1.5770680.639276 -3.412840 &emsp;&emsp;数据集由一系列的二维坐标构成，用制表符隔开，首先用open函数打开文本文件，然后将其放入python列表中。程序实现：12345678910from numpy import *def loadDataSet(fileName): dataMat = [] fr = open(fileName) for line in fr.readlines(): curLine = line.strip().split('\\t') fltLine = list(map(float, curLine)) #将字符串转为浮点型 dataMat.append(fltLine) return dataMat 程序说明：&emsp;&emsp;map(function, iterable, ...)：将可迭代对象iterable执行函数function，即对可迭代对象中的每一个元素都进行一次函数调用，得到新的可迭代对象。python3中map()函数返回的是迭代器，所以要在前面调用list()函数将其转换为列表。&emsp;&emsp;函数首先打开原始数据文件，然后用制表符隔开数据并添加到列表中，因为后面需要对其进行数值计算，而readlines()函数返回的列表元素是字符串，所以要利用map()函数将其全部转换为浮点数类型。最后添加至返回值列表中。 随机生成质心&emsp;&emsp;k均值的核心就是不断的更新质心，但最开始的质心通常是随机生成的，且必须要在原始数据集的范围内，即要保证在最小值和最大值之间。程序实现：123456789def randCent(dataSet, k): n = shape(dataSet)[1] # 返回数据集的列数 centroids = mat(zeros((k, n))) # 初始化质心为0 for j in range(n): minJ = min(dataSet[:, j]) rangeJ = float(max(dataSet[:, j]) - minJ) centroids[:, j] = mat(minJ + rangeJ * random.rand(k, 1)) # 保证生成的质心坐标在边界内 return centroids 程序说明：&emsp;&emsp;mat()：将数据转为矩阵形式。array()：将数据转为数组形式。矩阵：二维数据，数组：多维数据。numpy中默认的数据形式为数组形式。&emsp;&emsp;首先计算出数据每列的最小和最大值，然后得到取值范围，从而确定随机数。k为质心的个数。12range = max - mini = min + range * (0,1) # 在(min,max)范围内 距离计算&emsp;&emsp;K均值算法需要将其数据集中的数据分配给距离最近的质心的类别中，常用的距离计算公式为平面内点之间的距离公式。图程序实现：12def distEclud(vecA, vecB): return sqrt(sum(power(vecA - vecB, 2))) &emsp;&emsp;当然也可以用其他的距离计算公式。 k均值算法程序实现：12345678910111213141516171819202122def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent): m = shape(dataSet)[0] clusterAssment = mat(zeros((m, 2))) centroids = createCent(dataSet, k) clusterChanged = True while clusterChanged: clusterChanged = False for i in range(m): # 遍历数据集的每一行数据 minDist = inf minIndex = -1 for j in range(k): # 确定每一行数据的类别标签 distJI = distMeas(centroids[j, :], dataSet[i, :]) if distJI &lt; minDist: minDist = distJI minIndex = j if clusterAssment[i, 0] != minIndex: clusterChanged = True clusterAssment[i, :] = minIndex, minDist**2 for cent in range(k): ptsInClust = dataSet[nonzero(clusterAssment[:, 0].A == cent)[0]] centroids[cent, :] = mean(ptsInClust, axis=0) # 更新每个类别的质心 return centroids, clusterAssment 程序说明：&emsp;&emsp;该函数一共有4个输入参数，第一个为数据集dataSet，第二个为分类类别数目，第三个为距离计算公式，第四个为创建初始质心的函数。返回值为质心坐标和簇分配结果矩阵(类别标签结果和与质心的误差)。&emsp;&emsp;首先初始化簇分配结果矩阵为0并随机产生k个质心。函数主体一共有3层循环，第一层循环为while循环，其循环条件为：clusterChanged，即任一点的簇分配结果的类别标签是否改变，也就是每个数据的前后2次分类结果都是同一类就跳出while循环，否则需要继续分类。首先将其设为false，然后进入第二层的for循环判断，该层for循环会遍历数据集中的每一行数据，首先初始化距离和类别标签，然后进入第三层的for循环判断，该层for循环会判断每一行数据属于哪一类的类别标签，首先计算质心坐标和数据集中每一行数据的距离，并和minDist判断大小，一共判断k次，即有多少个类别就判断多少次，从而找到哪个类别和质心的距离最小，并设置为该类别。每做完一次第三层循环，都会判断一次终止条件是否满足，即如果之前判断的类别标签和现在的结果不是一致的，则仍需要继续分类，也就是现在的质心还不稳定，数据集中还存在不稳定的分类结果。&emsp;&emsp;每遍历完一次完整的数据集都需要重新计算每个类别的质心。 clusterAssment[:, 0]表示簇分配结果的第一列，即每行数据的类别标签，.A表示将结果转换为矩阵形式，然后判断是否等于相应的类别(k表示分类数目，也相当于类别标签，比如k为3，表示一共3类，类别为0，1，2)，nonzero()返回非0元素的索引，也就是返回是该类别的数据的索引，然后根据索引找到dateSet数据中的数据。也就是分别找到簇分配结果矩阵中的每个类别对应的原始数据集的数据。这里[0]表示的是nonzero()函数有很多返回值，我们需要的是其索引值，也就是第1个返回值。得到每一类的数据后，在进行均值计算，得到每一类的新的质心。 程序结果&emsp;&emsp;质心坐标：123[[ 2.93386365 3.12782785] [-2.94737575 3.3263781 ] [-0.45965615 -2.7782156 ]] # 每一类质心的坐标 &emsp;&emsp;簇分配结果矩阵：1234567[[0.00000000e+00 1.45461050e-01] [1.00000000e+00 6.80213825e-01] [2.00000000e+00 1.02184582e+00]... [0.00000000e+00 3.05416591e-03] [1.00000000e+00 3.16776316e+00] [2.00000000e+00 1.61040000e+00]] &emsp;&emsp;第一列为分类结果，0表示第一类，以此类推，第二列为每类数据与该类质心的误差。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://cxx0822.github.io/categories/机器学习/"}],"tags":[]},{"title":"将hexo博客转移到另一台电脑","slug":"将hexo博客转移到另一台电脑","date":"2019-01-05T08:41:23.000Z","updated":"2019-12-16T03:14:41.568Z","comments":true,"path":"2019/01/05/将hexo博客转移到另一台电脑/","link":"","permalink":"http://cxx0822.github.io/2019/01/05/将hexo博客转移到另一台电脑/","excerpt":"","text":"安装必要的软件&emsp;&emsp;有2个必须安装的软件：Git客户端和node JS。百度下载安装即可。 在Github官网添加新电脑产生的密钥&emsp;&emsp;安装完git客户端后，在桌面或者文件夹中右击选择git bash并输入以下命令：1ssh-keygen -t rsa -C \"邮箱地址\" &emsp;&emsp;邮箱为注册Github的邮箱地址，输入命令后一直按回车，然后会生成一个.ssh的文件夹，打开里面的.pub文件，就是新产生的密钥。然后登陆GitHub网站，找到setting设置，然后选择SSH and GPG keys，点击New SSH key，然后给密钥命名并将.pub文件中的&#39;ssh-ras....&#39;后面的内容复制到key中。 复制之前电脑的博客文件夹&emsp;&emsp;没有必要全部复制，只需要部分文件夹复制即可：12345_config.ymlpackage,jsonscaffolds/source/themes/ 安装hexo&emsp;&emsp;打开cmd，输入下面指令安装hexo：1npm install hexo-cli -g 配置博客相关模块&emsp;&emsp;进入到博客文件夹中，右击选择git bash1234npm config set registry https://registry.npm.taobao.org //换源npm info underscore (输出正常反馈信息则说明换源成功)npm installnpm install hexo-deployer-git --save // 文章部署到 git 的模块 &emsp;&emsp;如果博客中需要上传本地图片，首先将_config.yml文件中的 post_asset_folder字段设置为true，然后再安装图片模块：1npm install https://github.com/CodeFalling/hexo-asset-image --save 登陆账号&emsp;&emsp;第一次上传博客到服务器时，输入hexo d，会出现错误，并提示需要输入邮箱和用户名，指令在提示窗口中，重新输入指令并输入邮箱和用户名即可。 常用命令123hexo n \"文件名\" //新建一个md文件hexo g //生成网站静态文件到默认设置的 public 文件夹hexo d //自动生成网站静态文件，并部署到设定的仓库,即上传至服务器","categories":[],"tags":[]},{"title":"基于python的NAO机器人开发01：安装python和naoqi库及有线和无线连接","slug":"基于python的NAO机器人开发01：安装python和naoqi库及有线和无线连接","date":"2019-01-02T06:26:10.000Z","updated":"2020-01-09T07:21:41.234Z","comments":true,"path":"2019/01/02/基于python的NAO机器人开发01：安装python和naoqi库及有线和无线连接/","link":"","permalink":"http://cxx0822.github.io/2019/01/02/基于python的NAO机器人开发01：安装python和naoqi库及有线和无线连接/","excerpt":"","text":"一、NAOqi APIs&emsp;&emsp;NAOqi OS是NAO机器人的核心操作系统，NAOqi API提供了访问机器人的各种传感器设备接口以及应用接口。通过NAOqi，可以在动作、视觉、音频等不同模块之间相互传递信息，也可以通过编程实现各种功能。 二、安装python和NAOqi库&emsp;&emsp;Windows环境下，NAO支持的python版本为32位的python2，可以在python官网中下载。选择Windows x86 MSI installer，并配置python2的环境变量，将C:\\Python27和C:\\Python27\\Scripts添加至PATH中。然后在软银机器人社区SoftBank Robotics Community中下载NAOqi库，选择Resources中的Software，找到Python 2.7 SDK 2.1.4 Win 32 Setup下载即可。&emsp;&emsp;下载链接：NAOqi python SDK &emsp;&emsp;检查python是否安装正确：同时按下win+R，并输入cmd，进入命令行程序，输入python，如果有提示信息则安装正确，否则检查环境变量是否添加正确。&emsp;&emsp;检查naoqi是否安装正确：接上面的命令行(输入python的前提下)，继续输入import naoqi，无任何显示则安装正确。 三、安装Choregraphe&emsp;&emsp;Choregraphe是一个图形化的多平台软件，编写完程序后可以在模拟机器人上测试，也可以运行在真实的机器人上。在软银机器人社区SoftBank Robotics Community中可以下载。选择Resources中的Software，找到Choregraphe 2.1.4 Win 32 Setup下载安装即可。&emsp;&emsp;注：安装路径不要出现在中文。&emsp;&emsp;下载链接：Choregraphe 四、NAO机器人的有线连接4.1 连接步骤&emsp;&emsp;首先将NAO和计算机通过网线连接，将计算机的网络设置为有线连接，然后打开浏览器，按下NAO机器人的胸部按钮，在浏览器中输入NAO报的IP地址。 &emsp;&emsp;输入用户名nao，密码nao(默认)即可登陆至nao机器人网页，在网络设置里面可以看到已经连接至有线。 4.2 测试&emsp;&emsp;打开Choregraph，选择连接-&gt;连接至，在弹出的对话框中选择IP为刚才的NAO机器人，并选择连接。 &emsp;&emsp;在Choregraph界面的左下角的指令盒内依次选择Motions中的Wake Up，Stand Up和Rest，并用信号线将其连接，即可实现简单的站立、休息的动作。在Choregraph界面的最上面单击绿色三角箭头按钮即可运行程序。 &emsp;&emsp; 当然也可以通过python编译器在里面输入相应的代码实现此功能。 12345678910111213141516171819202122232425262728# -*- encoding: UTF-8 -*-import argparsefrom naoqi import ALProxydef main(robotIP, PORT=9559): motionProxy = ALProxy(\"ALMotion\", robotIP, PORT) postureProxy = ALProxy(\"ALRobotPosture\", robotIP, PORT) # Wake up robot motionProxy.wakeUp() # Send robot to Stand Init postureProxy.goToPosture(\"StandInit\", 0.5) # Go to rest position motionProxy.rest()if __name__ == \"__main__\": parser = argparse.ArgumentParser() parser.add_argument(\"--ip\", type=str, default=\"169.254.67.213\", help=\"Robot ip address\") parser.add_argument(\"--port\", type=int, default=9559, help=\"Robot port number\") args = parser.parse_args() main(args.ip, args.port) &emsp;&emsp;注意，IP和端口号(port)一定要对，IP为按NAO胸口按钮后报出的IP，真实NAO的port默认为9559。&emsp;&emsp;最终效果如图所示： 五、NAO机器人的无线连接及测试5.1 连接步骤&emsp;&emsp;打开计算机的无线网络连接,连接一个无线网，然后在NAO网页界面的网络设置中选择和计算机相同的无线网络，并拔掉网线，即可实现无线连接。按下NAO胸口按钮，此时会报出无线网的IP地址。此时需要重新输入IP地址进入NAO网页界面。 5.2 测试&emsp;&emsp;此时，在Choregraph中的连接选项中，选择无线网的IP地址的NAO机器人，并执行刚才的程序。同理在python代码中更改相应的IP地址和端口号即可。 六、虚拟NAO机器人的连接及测试6.2 连接步骤&emsp;&emsp;Choregraph软件提供了可连接虚拟机器人的设置，也可以实现一些真实机器人的功能。先断开所有的真实机器人的连接，在连接选项中选择连接虚拟机器人即可连接至一个虚拟机器人。同时可以查看它的IP地址和端口号。（虚拟机器人默认的IP地址都为127.0.0.1）在视图选项中勾选上机器人视图即可查看虚拟机器人。 6.3 测试&emsp;&emsp;同理在python代码中更改相应的IP地址和端口号即可。","categories":[{"name":"NAO开发与应用","slug":"NAO开发与应用","permalink":"http://cxx0822.github.io/categories/NAO开发与应用/"}],"tags":[]},{"title":"基于python的NAO机器人开发02：多线程实现边唱歌边跳舞","slug":"基于python的NAO机器人开发02：多线程实现边跳舞边唱歌","date":"2019-01-02T06:19:47.000Z","updated":"2020-01-09T07:21:47.388Z","comments":true,"path":"2019/01/02/基于python的NAO机器人开发02：多线程实现边跳舞边唱歌/","link":"","permalink":"http://cxx0822.github.io/2019/01/02/基于python的NAO机器人开发02：多线程实现边跳舞边唱歌/","excerpt":"","text":"在NAO上播放音频&emsp;&emsp;在NAO机器人上播放音频文件常用的库为ALAudioPlayer，里面有2个可以播放音乐的函数：play()和playFile()。 play()&emsp;&emsp;该函数的C++函数头为：1void play(const int&amp; taskId, const float&amp; volume, const float&amp; pan) &emsp;&emsp;该函数有3个参数，其中第一个为必须的，后面2个是可选的。第一个参数为任务ID，即文件名，通常由ALAudioPlayer库里面的loadFile(&quot;文件名&quot;)函数产生的。第二个为音频的音量，其值为0.0-1.0，第三个为Stereo panorama requested (-1.0 : left / 1.0 : right)d。 123456from naoqi import ALProxy audio = ALProxy(\"ALAudioPlayer\", '169.254.67.213', 9559)fileId = audio.loadFile(\"文件名绝对路径\")audio.play(fileId) &emsp;&emsp;首先建立一个ALAudioPlayer库的实例，然后用loadFile()加载音频文件，最后使用play()函数进行播放。 playFile()&emsp;&emsp;该函数的C++函数头为：1void playFile(const std::string&amp; fileName, const float&amp; volume, const float&amp; pan) &emsp;&emsp;该函数基本和play()类似，只是第一个参数为文件名，即不需要在用loadFile()函数来转换。123456from naoqi import ALProxy audio = ALProxy(\"ALAudioPlayer\", '169.254.67.213', 9559)songfile = \"文件名绝对路径\" audio.playFile(songfile) 将音频文件传入到NAO中&emsp;&emsp;NAO机器人存储器中自带一些音频文件，同时也支持将本地音频文件上传至NAO中。NAO机器人本身相当于一个服务器，本地计算机相当于主机，只要输入正确的IP地址和端口号即可访问和文件操作。常用的方式为自带的Choregraph软件和其他文件传输软件。 Choregraph软件&emsp;&emsp;NAO机器人自带的Choregraph软件可以将本地文件上传到NAO机器人的存储内存中，其文件传输协议为FTP模式。&emsp;&emsp;首先将NAO机器人连接到Choregraph软件中，具体可以参考另一篇博客：基于python的NAO机器人开发01。然后在连接选项中，选择高级-&gt;文件传送，此时必须是连接真机，虚拟机器人没有此功能。&emsp;&emsp;然后输入用户名(nao)和密码(初始为nao)。&emsp;&emsp;在空白处右击，选择创建文件夹即可创建自己的文件夹，点击上传功能并选择相应的本地文件即可将文件上传至NAO机器人中，此时文件存放的路径为：/home/nao/...。&emsp;&emsp;例如新建一个test的文件夹，并将test_wav.wav音频文件上传至NAO机器人中，可以使用下面的代码播放音频。12345678from naoqi import ALProxydef playMusic(): audio = ALProxy(\"ALAudioPlayer\", '169.254.67.213', 9559) songfile = \"/home/nao/test/test_wav.wav\" audio.playFile(songfile)playMusic() &emsp;&emsp;注：NAO机器人中，只能识别wav格式的音频文件，不支持mp3等其他格式的。 WinSCP软件&emsp;&emsp;除了使用Choregraph软件可以传输文件外，还可以使用其他的文件传输软件，比如WinSCP。输入其对应的IP地址和端口号，即可访问其内部文件夹。&emsp;&emsp;打开WinSCP软件后，首先选择FTP文件传输模式，然后输入相应的NAO机器人IP地址，用户名和密码同Choregraph软件。输入正确后，即可进入到机器人的内部存储文件夹。&emsp;&emsp;在WinSCP中上传本地文件，直接选中文件拖拽到相应位置即可，右击选择属性也可以直接更改其文件夹或文件的访问权限。 将python文件传入NAO机器人中，并直接运行&emsp;&emsp;除了可以上传音频文件，其他任何文件都可以上传至NAO机器人中，而NAO机器人中自带python的解释器，所以可以将py文件上传到NAO中，并直接运行python文件。&emsp;&emsp;NAO机器人内部使用的是linux操作系统，使用PuTTY软件可以进入到该系统中。linux系统的命令也都适用于此，输入相应的命令即可直接运行python文件。 登陆NAO系统&emsp;&emsp;打开PuTTY软件后，选择SSH登陆方式，然后输入正确的IP地址和端口号即可登陆NAO系统，用户名和密码同之前。 NAO系统中常用的Linux命令 cd ~ :返回根目录 cd /.../...：进入某个文件夹，例：cd /home/nao/test/ （TAB键可以自动补全） cd .. ：返回上一级文件夹 pwd：查看当前所在目录 ls： 查看当前文件夹的内容（ls：list） ls -l：查看当前文件夹的内容和详细信息 rm file：删除名为file的文件（rm：remove） rmdir dir：删除名为dir的目录（dir：dictionary） chmod \\*** file：修改文件权限&emsp;&emsp;文件权限共分为3种，r：读权限，用数字4表示；w：写权限，用数字2表示；x：执行权限，用数字1表示。一共有3类用户：文件所有者，同组用户和其他用户。每种用户用1个数字表示。&emsp;&emsp;比如将文件所有者的权限改为可读可写可执行：chmod 700 file。将所有用户都改为可读可写可执行：chmod 777 file。&emsp;&emsp;7表示4+2+1，即3个权限之和，file为完整的文件路径名。 ifconfig：显示以太网信息 su：登陆超级用户，密码为root reboot：重启 shutdown -h now：现在关机（需要root权限） 在NAO中执行运行python文件&emsp;&emsp;NAO支持多种编程，其内部带有python的解释器，使用python \\*.py文件可以直接运行。&emsp;&emsp;例如在/home/nao/目录下新建一个test的文件夹，并将之前的python文本文件和音频文件放入该文件夹中。首先利用cd /home/nao/test/命令进入该文件夹，然后输入python test.py命令，就可以实现之前的功能。&emsp;&emsp;注：如果音频不能正确播放，可能需要权限设置，将文件权限改为可读可写可执行即可。NAO有2块网卡，即有线网卡和无线网卡。分别对应一个IP地址，其本身也有一个本地IP地址，即127.0.0.1。当运行NAO本地的python文件时，可以将IP地址设置为有线连接或无线连接的IP地址，也可以直接设置为本地IP地址。 NAO机器人跳舞&emsp;&emsp;NAO机器人全身有25个自由度，通过控制每个自由度的变化可以实现多种不同的运动动作。&emsp;&emsp;ALMotionProxy库中的angleInterpolation()函数可以控制每个关节的变化，其函数头为：1void angleInterpolation(const AL::ALValue&amp; names, const AL::ALValue&amp; angleLists, const AL::ALValue&amp; timeLists, const bool&amp; isAbsolute) &emsp;&emsp;函数的第一个参数为关节的名字，第二个参数为关节的角度(弧度制)，第三个参数为关节变化的时间(s)，第四个参数为是否为相对变化，即变化的角度是否跟随上一个变化。&emsp;&emsp;NAO中的每个关节的名称可以参考NAO的官方文档，通过给每个关节不同的角度值就可以实现不同的动作，所谓舞蹈就是一连串动作的结合。&emsp;&emsp;这里参考了别人的跳舞代码：&emsp;&emsp;Dance模板库dance.py：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class DancePart(object): \"\"\" DancePart for Nao dance includes lists of names, times, and keys for Python simplified dances exported from Choreographe\"\"\" def __init__(self, part_length, pause_length, names, times, keys): super(DancePart, self).__init__() self.part_length = part_length self.pause_length = pause_length self.names = names self.times = times self.keys = keys def get_part_length(self): return self.part_length def get_pause_length(self): return self.pause_length def get_names(self): return self.names def get_times(self): return self.times def get_keys(self): return self.keysclass Dance(object): \"\"\" Nao robot dance. The attribute num_parts is the number of parts in the dance. The parts attribute is a list of DanceParts\"\"\" def __init__(self, song_name, num_parts, parts): super(Dance, self).__init__() self.song_name = song_name self.num_parts = num_parts self.parts = parts def get_part(self, part_num): return self.parts[part_num] def get_song_name(self): return self.song_name def get_num_parts(self): return self.num_parts def get_parts(self): return self.parts &emsp;&emsp;waltzDance舞蹈代码waltzDance.py： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444# -*- encoding: UTF-8 -*-from dance import DancePart, Dancenames1 = list()times1 = list()keys1 = list()names1.append(\"HeadPitch\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([0.136484, -0.130432, -0.147306, 0.032172, 0.0720561, 0.0720561, 0.113474, 0.00455999])names1.append(\"HeadYaw\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([-0.00464392, 0.274544, 0.0122299, -0.316046, 0.00609398, 0.332836, -0.0153821, -0.43263])names1.append(\"LAnklePitch\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([0.0935321, 0.0935321, 0.0843279, 0.082794, 0.082794, 0.082794, 0.082794, 0.082794])names1.append(\"LAnkleRoll\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([-0.125746, -0.125746, -0.125746, -0.125746, -0.125746, -0.125746, -0.125746, -0.125746])names1.append(\"LElbowRoll\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([-1.38363, -1.32533, -1.41584, -1.36062, -1.38669, -1.3192, -1.32687, -1.33914])names1.append(\"LElbowYaw\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([-1.52484, -2.06634, -1.52944, -1.53711, -1.53711, -2.07248, -1.53864, -1.53864])names1.append(\"LHand\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([0.2932, 0.2932, 0.2852, 0.2932, 0.2932, 0.2932, 0.2932, 0.2932])names1.append(\"LHipPitch\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([0.139636, 0.139636, 0.131966, 0.139636, 0.139636, 0.139636, 0.139636, 0.139636])names1.append(\"LHipRoll\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([0.093616, 0.093616, 0.10282, 0.093616, 0.093616, 0.093616, 0.093616, 0.093616])names1.append(\"LHipYawPitch\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([-0.171766, -0.171766, -0.174835, -0.171766, -0.171766, -0.171766, -0.171766, -0.171766])names1.append(\"LKneePitch\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([-0.0782759, -0.0782759, -0.0890141, -0.0890141, -0.0890141, -0.0890141, -0.0890141, -0.0890141])names1.append(\"LShoulderPitch\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([1.50328, 1.59532, 1.50481, 1.55083, 1.54316, 1.59532, 1.47567, 1.54776])names1.append(\"LShoulderRoll\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([-0.0383921, 0.338972, -0.0982179, -0.04913, -0.0890141, 0.337438, -0.107422, -0.0521979])names1.append(\"LWristYaw\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([-1.49876, -1.68591, -1.5233, -1.53558, -1.53558, -1.67517, -1.52177, -1.52177])names1.append(\"RAnklePitch\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([0.093616, 0.093616, 0.0874801, 0.093616, 0.093616, 0.093616, 0.093616, 0.093616])names1.append(\"RAnkleRoll\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([0.135034, 0.135034, 0.127364, 0.135034, 0.135034, 0.135034, 0.135034, 0.135034])names1.append(\"RElbowRoll\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([1.47575, 1.45427, 1.5095, 1.27019, 1.48189, 1.45427, 1.48189, 1.27019])names1.append(\"RElbowYaw\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([1.54776, 1.56004, 1.5585, 1.33147, 1.54623, 1.5585, 1.5585, 1.32227])names1.append(\"RHand\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([0.2864, 0.2864, 0.288, 0.2864, 0.2864, 0.2864, 0.2864, 0.2864])names1.append(\"RHipPitch\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([0.122678, 0.122678, 0.131882, 0.133416, 0.133416, 0.133416, 0.133416, 0.133416])names1.append(\"RHipRoll\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([-0.10427, -0.10427, -0.10427, -0.10427, -0.10427, -0.10427, -0.10427, -0.10427])names1.append(\"RHipYawPitch\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([-0.171766, -0.171766, -0.174835, -0.171766, -0.171766, -0.171766, -0.171766, -0.171766])names1.append(\"RKneePitch\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([-0.0858622, -0.0858622, -0.0873961, -0.0858622, -0.0858622, -0.0858622, -0.0858622, -0.0858622])names1.append(\"RShoulderPitch\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([1.62455, 1.66597, 1.61534, 1.29781, 1.62762, 1.6629, 1.64756, 1.29781])names1.append(\"RShoulderRoll\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([0.00609398, 0.0720561, 0.0628521, -0.579894, 0.0613179, 0.0613179, 0.0613179, -0.57836])names1.append(\"RWristYaw\")times1.append([0.56, 1.76, 2.96, 4.24, 5.44, 6.48, 7.92, 9.56])keys1.append([1.53549, 1.53549, 1.53703, 1.59072, 1.57231, 1.57231, 1.57231, 1.58458])names2 = list()times2 = list()keys2 = list()names2.append(\"HeadPitch\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([-0.147306, -0.00771189, -0.00771189, -0.147306, -0.0322559, -0.147306, -0.147306])names2.append(\"HeadYaw\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([0.0122299, 0.54913, 0.54913, 0.0137641, -0.589098, 0.0137641, 0.0137641])names2.append(\"LAnklePitch\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([0.0843279, 0.082794, 0.082794, 0.0843279, 0.082794, 0.0843279, 0.0843279])names2.append(\"LAnkleRoll\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([-0.125746, -0.125746, -0.125746, -0.125746, -0.125746, -0.125746, -0.125746])names2.append(\"LElbowRoll\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([-1.38669, -0.87127, -0.87127, -0.952573, -0.937232, -1.04308, -1.04308])names2.append(\"LElbowYaw\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([-1.55245, -1.97123, -1.97123, -0.50166, -0.492456, -0.622845, -0.622845])names2.append(\"LHand\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([0.2852, 0.2932, 0.2932, 0.2852, 0.2932, 0.2852, 0.2852])names2.append(\"LHipPitch\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([0.131966, 0.139636, 0.139636, 0.131966, 0.139636, 0.131966, 0.131966])names2.append(\"LHipRoll\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([0.10282, 0.093616, 0.093616, 0.10282, 0.093616, 0.10282, 0.10282])names2.append(\"LHipYawPitch\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([-0.174835, -0.171766, -0.171766, -0.174835, -0.171766, -0.174835, -0.174835])names2.append(\"LKneePitch\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([-0.0890141, -0.0890141, -0.0890141, -0.0890141, -0.0890141, -0.0890141, -0.0890141])names2.append(\"LShoulderPitch\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([1.53242, 1.74872, 1.74872, -1.13827, -1.10145, 0.472429, 0.472429])names2.append(\"LShoulderRoll\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([-0.0245859, 0.700996, 0.700996, 0.29602, 0.314428, 0.118076, 0.118076])names2.append(\"LWristYaw\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([-1.54018, -1.42666, -1.42666, -1.12293, -1.14134, -0.785451, -0.785451])names2.append(\"RAnklePitch\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([0.0874801, 0.093616, 0.093616, 0.0874801, 0.093616, 0.0874801, 0.0874801])names2.append(\"RAnkleRoll\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([0.127364, 0.135034, 0.135034, 0.127364, 0.135034, 0.127364, 0.127364])names2.append(\"RElbowRoll\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([1.09992, 1.11679, 1.0585, 1.06771, 0.61671, 0.60904, 0.880559])names2.append(\"RElbowYaw\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([0.651908, 0.662646, 0.335904, 0.346642, 0.944902, 0.961776, 0.645772])names2.append(\"RHand\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([0.288, 0.2864, 0.2864, 0.288, 0.2864, 0.288, 0.288])names2.append(\"RHipPitch\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([0.131882, 0.133416, 0.133416, 0.131882, 0.133416, 0.131882, 0.131882])names2.append(\"RHipRoll\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([-0.10427, -0.10427, -0.10427, -0.10427, -0.10427, -0.10427, -0.10427])names2.append(\"RHipYawPitch\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([-0.174835, -0.171766, -0.171766, -0.174835, -0.171766, -0.174835, -0.174835])names2.append(\"RKneePitch\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([-0.0873961, -0.0858622, -0.0858622, -0.0873961, -0.0858622, -0.0873961, -0.0873961])names2.append(\"RShoulderPitch\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([-1.03081, -0.990922, 0.475582, 0.457173, 0.512398, 0.506262, 0.464844])names2.append(\"RShoulderRoll\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([-0.366667, -0.391212, -0.181054, -0.170316, -0.951122, -0.981802, -0.0353239])names2.append(\"RWristYaw\")times2.append([0.8, 1.76, 3.04, 4.24, 5.96, 6.76, 8.2])keys2.append([1.75025, 1.73031, 1.38976, 1.35755, 1.06609, 1.02927, 0.812978])names3 = list()times3 = list()keys3 = list()names3.append(\"HeadPitch\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([-0.142704, -0.1335, 0.0904641, -0.154976, -0.0629361, -0.142704, -0.115092, -0.142704, -0.115092, -0.142704, -0.142704, 0.00455999])names3.append(\"HeadYaw\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([-0.00310993, -0.00157595, 0.00609398, -0.00157595, -0.016916, 0.00762796, 0.377322, 0.00762796, -0.527738, 0.00762796, 0.00762796, 0.309826])names3.append(\"LAnklePitch\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([0.0858622, 0.082794, 0.0858622, 0.082794, 0.0858622, 0.0858622, -0.259288, 0.0858622, 0.105804, 0.0858622, 0.0858622, 0.0919981])names3.append(\"LAnkleRoll\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([-0.130348, -0.125746, -0.130348, -0.125746, -0.130348, -0.130348, -0.0935321, -0.130348, -0.128814, -0.131882, -0.131882, -0.128814])names3.append(\"LElbowRoll\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([-1.42965, -0.056716, -1.40357, -0.0689881, -1.52169, -0.961776, -0.0475121, -0.961776, -0.954106, -1.18114, -1.4772, -1.3959])names3.append(\"LElbowYaw\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([-0.506262, -0.53234, -0.495524, -1.13674, -0.329852, -0.704148, -0.757838, -0.704148, -0.68574, -1.37451, -1.49569, -2.07401])names3.append(\"LHand\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([0.2932, 0.2892, 0.2892, 0.2892, 0.2892, 0.2932, 0.2892, 0.2932, 0.2892, 0.2932, 0.2932, 0.2892])names3.append(\"LHipPitch\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([0.131966, 0.124296, 0.128898, 0.124296, 0.128898, 0.131966, 0.128898, 0.131966, 0.0813439, 0.130432, 0.130432, 0.121228])names3.append(\"LHipRoll\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([0.096684, 0.092082, 0.096684, 0.092082, 0.096684, 0.096684, 0.096684, 0.0966839, 0.096684, 0.090548, 0.090548, 0.096684])names3.append(\"LHipYawPitch\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([-0.1733, -0.179436, -0.171766, -0.179436, -0.171766, -0.1733, -0.171766, -0.1733, -0.228524, -0.171766, -0.171766, -0.170232])names3.append(\"LKneePitch\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([-0.0874801, -0.092082, -0.0859461, -0.092082, -0.0859461, -0.0874801, 0.283748, -0.0874801, -0.0828779, -0.0859461, -0.0859461, -0.0828779])names3.append(\"LShoulderPitch\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([1.53089, 1.55697, 1.53703, 1.48794, 0.506178, -1.0539, -0.949588, -1.0539, -1.01708, 1.37902, 1.49868, 1.53549])names3.append(\"LShoulderRoll\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([0.720938, 1.09063, 0.714802, 0.607422, 0.489304, 0.408002, 1.12131, 0.408002, 0.432546, 0.542994, -0.07214, 0.0383081])names3.append(\"LWristYaw\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([-1.11679, -1.1306, -1.12907, -1.8209, -1.29934, -1.11219, -1.24718, -1.11219, -1.126, -1.58006, -1.54785, -1.63835])names3.append(\"RAnklePitch\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([0.09515, 0.093616, 0.0874801, 0.093616, 0.0874801, 0.093616, 0.0567999, 0.093616, -0.0689881, 0.092082, 0.092082, 0.0890141])names3.append(\"RAnkleRoll\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([0.128898, 0.122762, 0.130432, 0.122762, 0.130432, 0.128898, 0.153442, 0.128898, 0.067538, 0.130432, 0.130432, 0.122762])names3.append(\"RElbowRoll\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([1.4282, 0.0844119, 1.40519, 0.0598679, 1.5187, 0.978734, 1.00021, 0.978734, 0.0583339, 0.975666, 1.4374, 1.40979])names3.append(\"RElbowYaw\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([0.179436, 0.19631, 0.1733, 0.872804, 0.207048, 0.573674, 0.561402, 0.573674, 0.573674, 1.10444, 1.49868, 1.48334])names3.append(\"RHand\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([0.2864, 0.2916, 0.2916, 0.2916, 0.2916, 0.2864, 0.2916, 0.2864, 0.2916, 0.2864, 0.2864, 0.2916])names3.append(\"RHipPitch\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([0.131882, 0.125746, 0.124212, 0.125746, 0.124212, 0.121144, 0.184038, 0.121144, -0.190258, 0.118076, 0.118076, 0.118076])names3.append(\"RHipRoll\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([-0.099668, -0.0966001, -0.10427, -0.0966001, -0.10427, -0.099668, -0.12728, -0.0996681, -0.0919981, -0.0950661, -0.0950661, -0.102736])names3.append(\"RHipYawPitch\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([-0.1733, -0.179436, -0.171766, -0.179436, -0.171766, -0.1733, -0.171766, -0.1733, -0.228524, -0.171766, -0.171766, -0.170232])names3.append(\"RKneePitch\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([-0.0858622, -0.0889301, -0.0858622, -0.0889301, -0.0858622, -0.0858622, -0.0858622, -0.0858622, 0.335988, -0.0923279, -0.0919981, -0.0889301])names3.append(\"RShoulderPitch\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([1.34843, 1.32388, 1.38064, 1.31008, 0.526204, -1.10137, -1.06916, -1.10137, -1.06916, 1.13674, 1.4466, 1.45888])names3.append(\"RShoulderRoll\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([-0.794654, -1.09072, -0.77778, -0.635118, -0.553816, -0.49399, -0.513932, -0.493989, -1.09225, -0.675002, -0.046062, -0.046062])names3.append(\"RWristYaw\")times3.append([0.48, 1, 1.48, 2, 3, 4.28, 5.48, 6.8, 8.08, 9.16, 10.68, 11.8])keys3.append([1.13665, 1.34834, 1.12438, 1.82387, 1.25784, 1.13512, 1.16887, 1.13512, 1.47567, 1.22716, 1.67969, 1.66588])names4 = list()times4 = list()keys4 = list()names4.append(\"HeadPitch\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([0.133416, 0.131882, -0.142704, -0.142704, -0.142704, 0.12728, -0.142704, -0.142704])names4.append(\"HeadYaw\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([0.0199001, -0.305308, 0.00916195, 0.00916195, 0.00916195, -0.0399261, 0.00916195, 0.00916195])names4.append(\"LAnklePitch\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([0.0919981, 0.08126, 0.0858622, 0.0858622, 0.0858622, 0.0919981, 0.0413761, 0.0858622])names4.append(\"LAnkleRoll\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([-0.128814, -0.128814, -0.131882, -0.131882, -0.131882, -0.128814, -0.121144, -0.131882])names4.append(\"LElbowRoll\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([-1.38363, -1.38363, -1.26551, -1.27471, -0.0904641, -0.0659201, -0.049046, -0.049046])names4.append(\"LElbowYaw\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([-1.3607, -1.3607, -2.07554, -2.07554, -2.08567, -0.446436, -0.42496, -0.441834])names4.append(\"LHand\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([0.2892, 0.2892, 0.2932, 0.2932, 0.2932, 0.2892, 0.2932, 0.2932])names4.append(\"LHipPitch\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([0.131966, 0.131966, 0.130432, 0.130432, 0.130432, 0.131966, -0.31903, 0.130432])names4.append(\"LHipRoll\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([0.096684, 0.096684, 0.090548, 0.090548, 0.090548, 0.0997519, 0.024586, 0.090548])names4.append(\"LHipYawPitch\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([-0.170232, -0.170232, -0.171766, -0.171766, -0.171766, -0.167164, -0.268408, -0.171766])names4.append(\"LKneePitch\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([-0.0828779, -0.0828779, -0.0859461, -0.0859461, -0.0859461, -0.0828779, -0.0859461, -0.0859461])names4.append(\"LShoulderPitch\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([1.49561, 1.49561, 1.50635, 0.662646, 0.673384, 1.44192, 1.36982, 1.43271])names4.append(\"LShoulderRoll\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([-0.131966, -0.131966, 0.285282, 1.12745, 1.0891, 0.099668, 0.161028, 0.107338])names4.append(\"LWristYaw\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([-1.7396, -1.7396, -1.67977, -1.67977, -1.68284, -1.39138, -1.37604, -1.40825])names4.append(\"RAnklePitch\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([0.0890141, 0.0890141, 0.092082, 0.092082, 0.092082, 0.09515, 0.093616, 0.092082])names4.append(\"RAnkleRoll\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([0.122762, 0.122762, 0.130432, 0.130432, 0.130432, 0.119694, 0.128898, 0.130432])names4.append(\"RElbowRoll\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([1.33922, 1.32849, 1.33616, 1.34843, 0.268492, 0.23321, 0.21787, 0.217869])names4.append(\"RElbowYaw\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([1.48334, 2.07086, 2.08567, 2.07546, 2.08567, 0.431012, 0.506178, 0.423342])names4.append(\"RHand\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([0.2916, 0.2916, 0.2864, 0.2864, 0.2864, 0.2916, 0.2864, 0.2864])names4.append(\"RHipPitch\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([0.118076, 0.118076, 0.118076, 0.118076, 0.118076, 0.124212, -0.415756, 0.118076])names4.append(\"RHipRoll\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([-0.102736, -0.102736, -0.0950661, -0.0950661, -0.0950661, -0.10427, -0.0674541, -0.095066])names4.append(\"RHipYawPitch\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([-0.170232, -0.170232, -0.171766, -0.171766, -0.171766, -0.167164, -0.268408, -0.171766])names4.append(\"RKneePitch\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([-0.0889301, -0.0889301, -0.0919981, -0.0919981, -0.0919981, -0.078192, -0.0919981, -0.0812599])names4.append(\"RShoulderPitch\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([1.47115, 1.49262, 1.48956, 0.89283, 0.89283, 1.50643, 1.32542, 1.5141])names4.append(\"RShoulderRoll\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([0.139552, -0.216336, -0.23321, -1.19963, -1.0539, -0.200996, -0.250084, -0.211735])names4.append(\"RWristYaw\")times4.append([1.36, 2.68, 4, 5.36, 6.8, 8.28, 9.6, 10.8])keys4.append([1.66588, 1.4726, 1.44038, 1.3913, 1.70883, 1.52475, 1.50941, 1.50941])part1 = DancePart(10, 3, names1, times1, keys1)part2 = DancePart(8, 2, names2, times2, keys2)part3 = DancePart(13, 4, names3, times3, keys3)part4 = DancePart(12, 0, names4, times4, keys4)parts = [part1, part2, part3, part4]waltz = Dance(\"Waltz\", 4, parts) &emsp;&emsp;具体调用：123456789101112131415from naoqi import ALProxyimport waltzDanceimport timemotion = ALProxy(\"ALMotion\", NaoIP, Port)def dance(danceName): for i in range(danceName.get_num_parts()): motion.angleInterpolation(danceName.get_part(i).get_names(), danceName.get_part(i).get_keys(), danceName.get_part(i).get_times(), True) # time.sleep(danceName.get_part(i).get_pause_length()) time.sleep(0.3)waltz = waltzDance.waltzdance(waltz) &emsp;&emsp;当然也可以设计其他的舞蹈动作。 多线程实现边唱歌边跳舞利用python实现&emsp;&emsp;python中有专门的多线程模块threading，我们可以把跳舞作为主线程，唱歌作为一个子线程，在跳舞的同时，开启一个线程专门来播放音频，即可实现边唱歌边跳舞。&emsp;&emsp;首先创建个线程的对象，然后用start()函数来开启该线程。123import threadingmythread = threading.Thread(target=function_name, args=(function_parameter1, ))mythread.start() &emsp;&emsp;创建对象时，第一个参数是目标函数的函数名(不是调用，不需要加括号)，第二个参数是目标函数的参数，是可选参数，用元组的形式将参数封装起来，如果只有1个参数，后面的逗号不可以省略。然后调用start()开启线程。1234t1 = threading.Thread(target=playMusic)t1.start()dance.dance(waltz) 利用模块的post属性实现&emsp;&emsp;除了python的多线程可以实现此功能之外，ALAudioPlayer类中的post属性也可以实现。&emsp;&emsp;在NAOqi模块的一些持续性动作的类中，比如播放音频的ALAudioPlayer类的playFile()，运动类ALMotion中的moveTo()，都有一个post属性。所谓post属性就是将这个持续性动作的进程挂起，NAO可以在执行这个进程的同时进行其他的操作。其调用方式为在playFile()函数前加上.post。123audio = ALProxy(\"ALAudioPlayer\", '169.254.67.213', 9559)songfile = \"/home/nao/test/test_wav.wav\" audio.post.playFile(songfile) &emsp;&emsp;在post语句下面可以添加其他的功能，此时NAO会一直播放音频文件，并执行下面的语句，直到音频文件播放完毕。例如实现边跳舞边唱歌，只需在playFile()函数前加上post，然后在下面调用dance()函数。123def danceAndSong(danceName, songName): audio.post.playFile(songName) dance() &emsp;&emsp;除了播放音频的库可以加post之外，行走库中的moveTo()也可以，即可以实现边行走边执行其他的模块，比如边行走边调用NAO的视觉系统，例如我的另一篇博客：NAO高尔夫比赛：python初级版。 开机自启动边跳舞边唱歌&emsp;&emsp;如果想实现开机自启动某个程序，需要更改NAO的开机启动配置文件，其文件路径为/home/nao/naoqi/preferences/autoload.ini。使用Choregraph软件或者WinSCP软件将其下载到本地计算机中，然后使用VScode或其他编译器软件打开，找到其中的[python]，然后将需要启动的py文件路径放在下一行，重新启动NAO机器人就会自动执行该文件。&emsp;&emsp;需要将python文件中的IP地址改为NAO的本地IP，即&quot;127.0.0.1&quot;。 附：Choregraph实现开机自启动&emsp;&emsp;首先新建一个项目文件，编写代码。例如我写了一个说“hello world”的程序，命名为hello。&emsp;&emsp;然后保存并上传到机器人系统中，上传功能在应用程序视图。&emsp;&emsp;在应用程序中找到你的文件，并右击设置为默认状态。会出现一个小旗子。最后重启机器人，就会进入你写的程序中。&emsp;&emsp;注：NAO机器人开机后会进入自主模式，即开机后会站立并来回晃动。这个好像去不掉，可以在你写的代码里面进行处理。我实际测试下来，需要等待一段时间才会进入到开机自启动的项目中。","categories":[{"name":"NAO开发与应用","slug":"NAO开发与应用","permalink":"http://cxx0822.github.io/categories/NAO开发与应用/"}],"tags":[]},{"title":"西瓜数据集的分类：贝叶斯，决策树","slug":"西瓜数据集的分类：贝叶斯，决策树","date":"2018-12-28T03:19:11.000Z","updated":"2019-01-07T05:03:50.148Z","comments":true,"path":"2018/12/28/西瓜数据集的分类：贝叶斯，决策树/","link":"","permalink":"http://cxx0822.github.io/2018/12/28/西瓜数据集的分类：贝叶斯，决策树/","excerpt":"","text":"原始数据集123456789101112131415161718色泽 根蒂 敲声 纹理 脐部 触感 好瓜青绿 蜷缩 浊响 清晰 凹陷 硬滑 是乌黑 蜷缩 沉闷 清晰 凹陷 硬滑 是乌黑 蜷缩 浊响 清晰 凹陷 硬滑 是青绿 蜷缩 沉闷 清晰 凹陷 硬滑 是浅白 蜷缩 浊响 清晰 凹陷 硬滑 是青绿 稍蜷 浊响 清晰 稍凹 软粘 是乌黑 稍蜷 浊响 稍糊 稍凹 软粘 是乌黑 稍蜷 浊响 清晰 稍凹 硬滑 是乌黑 稍蜷 沉闷 稍糊 稍凹 硬滑 否青绿 硬挺 清脆 清晰 平坦 软粘 否浅白 硬挺 清脆 模糊 平坦 硬滑 否浅白 蜷缩 浊响 模糊 平坦 软粘 否青绿 稍蜷 浊响 稍糊 凹陷 硬滑 否浅白 稍蜷 沉闷 稍糊 凹陷 硬滑 否乌黑 稍蜷 浊响 清晰 稍凹 软粘 否浅白 蜷缩 浊响 模糊 平坦 硬滑 否青绿 蜷缩 沉闷 稍糊 稍凹 硬滑 否 &emsp;&emsp;数据集的第一行为特征和分类标题。一共有6个特征，17组数据，分类标签为是否为好瓜。每个数据用空格隔开。 贝叶斯算法准备数据集&emsp;&emsp;由于贝叶斯算法需要计算类别概率，所以在原始数据集中需要将文字的’是’和’否’改写为’1’和’0’。程序实现：12345678910def file2dataSet(filename): postingList = [] classVec = [] with open(filename) as fr: next(fr) # 跳过第一行 for line in fr.readlines(): line = line.strip().split(' ') postingList.append(line[:-1]) classVec.append(eval(line[-1])) # 去掉字符串的双引号 return postingList, classVec &emsp;&emsp;首先创建一个存储特征和分类的空列表，然后使用with语句打开文件。由于第一行是标题，所以要跳过该行，next()函数返回可迭代对象的下一个值。&emsp;&emsp;然后用strip()函数将原始数据集中的空白，换行符等都删掉。原始数据集是用空格隔开的，所以可用split(‘ ‘)函数将其分隔开，然后将特征和分类标签分别添加到特征和分类的列表中。&emsp;&emsp;line[:-1]表示特征，line[-1]表示标签，因为字符串返回的标签值会带有单引号，不可进行求和运算，eval()函数会把字符串当成有效的表达式来求值并返回计算结果，即可以去掉字符串的引号。 测试数据&emsp;&emsp;假设有1组西瓜数据：’青绿’, ‘蜷缩’, ‘浊响’, ‘清晰’, ‘凹陷’, ‘硬滑’，利用贝叶斯进行分类预测。&emsp;&emsp;贝叶斯算法可参照之前的博客。程序实现：12345678910111213if __name__ == '__main__': data, labels = file2dataSet(\"xiguaData.txt\") myVocabList = createVocabList(data) trainMat = [] for doc in data: trainMat.append(setOfWords2Vec(myVocabList, doc)) p0V, p1V, pAb = trainNBO(trainMat, labels) testEntry = &#123;'青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑'&#125; thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) result = classifyNB(thisDoc, p0V, p1V, pAb) print(testEntry, \" is classified as :\", result) &emsp;&emsp;首先利用之前的训练算法得到贝叶斯概率模型，然后将输入的数据转换为分类算法所需要的特征个数列表，即thisDoc，然后将其和概率模型得到的特征概率列表进行运算预测，最终其结果为属于’1’分类，即’好瓜’分类。 决策树算法准备数据集&emsp;&emsp;决策树算法的数据集分为两部分，一部分为已知分类类别的数据，另一部分为所有的特征标题，即[‘色泽’, ‘根蒂’, ‘敲声’, ‘纹理’, ‘脐部’, ‘触感’]。程序实现：12345678910def file2dataSet(filename): dataSet = [] with open(filename) as fr: next(fr) # 跳过第一行 for line in fr.readlines(): line = line.strip().split(' ') dataSet.append(line) labels = ['色泽', '根蒂', '敲声', '纹理', '脐部', '触感'] return dataSet, labels &emsp;&emsp;将原始数据集中的每行数据添加到dataSet中。这里的label是特征标签，不是分类标签。决策树是根据特征的熵来选择最佳分类的。 测试数据集&emsp;&emsp;还是之前的测试数据’青绿’, ‘蜷缩’, ‘浊响’, ‘清晰’, ‘凹陷’, ‘硬滑’，利用决策树进行分类预测。&emsp;&emsp;决策树算法可参照之前的博客。&emsp;&emsp;首先编写一个预测函数：程序实现：1234567891011def classify(inputTree, featLabels, testVec): firstStr = list(inputTree.keys())[0] secondDict = inputTree[firstStr] featIndex = featLabels.index(firstStr) for key in secondDict.keys(): if testVec[featIndex] == key: if type(secondDict[key]).__name__ == 'dict': classLabel = classify(secondDict[key], featLabels, testVec) else: classLabel = secondDict[key] return classLabel &emsp;&emsp;函数有3个参数，生成好的决策树，特征值和待预测的数据。函数使用了递归调用的方法。&emsp;&emsp;首先获得已知决策树的第一个健的值，即决策树的第一个分类特征，然后获得基于该特征的子树，并对应的找到对于待预测数据的特征是什么值，即判断下面要进入到哪一个子树。然后在判断现在的子树是叶子节点还是仍然是一个决策树，如果仍然是一个决策数，则递归调用自己，直到是叶子节点为止，即往下一层层分类，直到找到最终的叶子节点，也就分类完成了。&emsp;&emsp;代入具体的测试数据验证：程序实现：1234567if __name__ == '__main__': dataSet, labels = file2dataSet(\"xiguaData.txt\") mytree = createTree(dataSet, labels) print(mytree) testVec = ['青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑'] labels = ['色泽', '根蒂', '敲声', '纹理', '脐部', '触感'] classLabel = classify(mytree, labels, testVec) &emsp;&emsp;首先根据之前的决策树算法得到决策树模型，即{‘纹理’: {‘模糊’: ‘否’, ‘清晰’: {‘根蒂’: {‘稍蜷’: {‘色泽’: {‘乌黑’: {‘触感’: {‘软粘’: ‘否’, ‘硬滑’: ‘是’}}, ‘青绿’: ‘是’}}, ‘硬挺’: ‘否’, ‘蜷缩’: ‘是’}}, ‘稍糊’: {‘触感’: {‘软粘’: ‘是’, ‘硬滑’: ‘否’}}}} ，然后将待预测数据写出列表形式，并进行预测分类。此时需要将特征标签重新写一遍(之前训练决策树模型的时候删掉了一个特征)，然后代入之前的预测函数，最终得到的结果为”是”好瓜。和实际的结果也是一致的。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://cxx0822.github.io/categories/机器学习/"}],"tags":[]},{"title":"贝叶斯算法","slug":"贝叶斯算法","date":"2018-12-24T01:16:23.000Z","updated":"2019-06-29T02:50:34.582Z","comments":true,"path":"2018/12/24/贝叶斯算法/","link":"","permalink":"http://cxx0822.github.io/2018/12/24/贝叶斯算法/","excerpt":"","text":"贝叶斯算法一、基本原理&emsp;&emsp;贝叶斯算法属于有监督的学习算法，即原始数据集是已知的，其主要思想为根据在一定特征下属于哪个类别的条件概率大小来判断分类。&emsp;&emsp;条件概率是指事件A在另外一个事件B已经发生条件下的发生概率,记为P(A|B),B为条件，A为需要计算的概率。其计算公式为：P(A|B)=P(AB)/P(B),但一般来说A,B事件不是独立事件，即P(AB)不等于P(A)P(B)，所以一般用贝叶斯公式将其展开，即：&emsp;&emsp;对应于分类算法的意义，该公式可以表示为：&emsp;&emsp;P(类别|特征)：在一定特征下属于该类别的概率，即我们最终用来判断分类的概率。P(特征|类别)：在该类别下的特征概率。即在已知类别的情况下，每个特征出现的频数除以总的特征数。P(类别)：数据集中的每个类别的概率，即每个类别的次数除以数据集的总个数。P(特征)：特征的概率，由于数据集是一样的，其特征也一样，计算概率时，可忽略该项。&emsp;&emsp;哪个分类的条件概率大就判定为哪个类别，这也是贝叶斯算法的核心思想。其计算公式中最重要的部分就是每个特征的在对应类别下的概率。有多少个特征就要进行多少次计算，这里我们假设每个特征都是相互独立的，即任意2个特征都不相互影响分类结果，这也称为朴素贝叶斯算法。 二、算法实现（文本分类）&emsp;&emsp;对一个在线社区的留言板内容构建一个过滤器，如果某条留言使用了负面的语言，则将该留言标识为内容不当。&emsp;&emsp;首先需要将文本转换为数字向量以便为标识，然后基于这些向量来计算条件概率(贝叶斯算法)并在此基础上构建分类器。 准备数据&emsp;&emsp;首先将获得的每句话的文本数据存在于列表中，然后将所有的文本数据整合到一个列表中，此时列表中的所有单词元素应该是唯一的。最后创建一个文档向量，向量的每一个元素为1或0，分别表示词汇表中的单词在输入单词列表中是否存在。&emsp;&emsp;此时单词列表中的单词就是数据集的特征，即1个单词对应1个特征。程序实现：123456789def loadDataSet(): postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] classVec = [0, 1, 0, 1, 0, 1] return postingList, classVec 程序说明：&emsp;&emsp;postingList为文本数据。首先将每句话存放在一个列表中，然后将每个单词用逗号隔开。classVec为已知的分类标签，即事先对每句话划分好的标签。0表示这句话是正面的，1表示负面的。 123456def createVocabList(dataSet): vocabSet = set([]) # 遍历输入数据集中的每一句话 for document in dataSet: vocabSet = vocabSet | set(document) return list(vocabSet) &emsp;&emsp;createVocabList()函数生成一个包含所有数据集的单词列表，且列表中的单词都是唯一的。&emsp;&emsp;首先创建一个空列表，set()函数保证了其唯一性，然后分别将此与数据集中每一句的单词列表进行’与’操作，即将数据集的每句话中不重复的单词添加到空列表中。123456789def setOfWords2Vec(vocabList, inputSet): returnVec = [0] * len(vocabList) for word in inputSet: # 如果输入数据集中的单词在单词列表中则记为1 if word in vocabList: returnVec[vocabList.index(word)] = 1 else: print(\"the word: %s is not in my Vocabulary!\" % word) return returnVec &emsp;&emsp;setOfWords2Vec()创建一个文档向量。该函数对输入的数据中的每个单词进行搜索，如果该单词在单词列表中，则记为1。1234listOPosts, listClasses = loadDataSet()myVocabList = createVocabList(listOPosts)returnVec_0 = setOfWords2Vec(myVocabList, listOPosts[0])returnVec_1 = setOfWords2Vec(myVocabList, listOPosts[1]) 程序输出为：123['to', 'how', 'dog', 'ate', 'park', 'is', 'food', 'my', 'steak', 'flea', 'take', 'problems', 'buying', 'I', 'mr', 'love', 'posting', 'not', 'garbage', 'maybe', 'licks', 'dalmation', 'cute', 'please', 'quit', 'stop', 'help', 'him', 'stupid', 'so', 'worthless', 'has'][0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1][1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0] &emsp;&emsp;myVocabList就是单词列表，也就是该数据集的所有特征。第1个文档向量的输入数据为第1句话，第1个元素’0’表示’to’这个单词没有在第1句话出现，第2个文档向量的输入数据为第2句话，第1个元素’1’表示’to’这个单词在第2句话中有出现。&emsp;&emsp;一共有6组数据列表，每个列表的长度即单词列表的长度，即不重复单词的个数，也就是数据集的特征总数。 训练算法&emsp;&emsp;得到数据集的每组特征后，便可以利用贝叶斯的条件概率公式训练算法。其计算公式可以改写为：&emsp;&emsp;w表示特征，c表示分类。对每个类别都进行计算，最后比较对应的概率值大小就可以判断其分类。&emsp;&emsp;p(c)表示类别出现的概率，即在原始数据集中每个类型出现的概率。例如在数据集的6句话中，负面评论共3次，则类别0的概率为3/6=0.5。&emsp;&emsp;p(w|c)表示在该类别下该特征的频率。即在已知类别的情况下，每个特征出现的概率。由于原始数据集不仅只有1个特征，所以需要对每个特征进行计算并求积。即：&emsp;&emsp;每个特征的概率计算方法为：每个特征在某个类别下出现的次数/该类别下的总特征数。程序实现：12345678910111213141516171819def trainNBO(trainMatrix, trainCategory): numTrainDocs = len(trainMatrix) # 输入文档的长度，即共有几组数据 numWords = len(trainMatrix[0]) # 单词的个数，即每组数据有多少个单词 pAbusive = sum(trainCategory) / float(numTrainDocs) # 是负面标签的概率 p0Num = ones(numWords) p1Num = ones(numWords) p0Denom = 0.0 p1Denom = 0.0 for i in range(numTrainDocs): if trainCategory[i] == 1: # 计算当类别标签为1时，各个单词出现的次数 p1Num += trainMatrix[i] # 标签为1的类别下，每个单词出现的次数+1 p1Denom += sum(trainMatrix[i]) # 标签为1的类别下，单词在单词表中出现的总次数 else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) p1Vect = p1Num / p1Denom p0Vect = p0Num / p0Denom return p0Vect, p1Vect, pAbusive 程序说明：&emsp;&emsp;首先对所有的标签进行求和，并除以总的文档长度，得到概率p(c)。因为负面的记1，正面记0，所以求得的和即是负面标签的总数。然后初始化类别概率为0。在计算每个特征的概率时，首先要对类别进行判断，当满足该类别时，对其特征进行求和，即单词在单词列表中出现的次数，因为是用’1’和’0’来标记的，所以直接求和即可，同时也需要计算总的特征数，即在类别下单词在单词列表中出现的总次数。最后计算出概率。程序验证：12345678910111213141516171819202122listOPosts, listClasses = loadDataSet()myVocabList = createVocabList(listOPosts)trainMat = []for postinDoc in listOPosts: trainMat.append(setOfWords2Vec(myVocabList, postinDoc))p0V, p1V, pAb = trainNBO(trainMat, listClasses)``` ```pythonmyVocabList ['flea', 'licks', 'please', 'garbage', 'love', 'how', 'cute', 'park', 'my', 'dalmation', 'dog', 'so', 'mr', 'posting', 'stop', 'has', 'steak', 'ate', 'not', 'buying', 'help', 'maybe', 'worthless', 'quit', 'to', 'I', 'him', 'stupid', 'food', 'problems', 'take', 'is']p0V [0.04166667 0.04166667 0.04166667 0. 0.04166667 0.04166667 0.04166667 0. 0.125 0.04166667 0.04166667 0.04166667 0.04166667 0. 0.04166667 0.04166667 0.04166667 0.04166667 1. 0. 0.04166667 0. 0. 0. 0.04166667 0.04166667 0.08333333 0. 0. 0.04166667 0. 0.04166667]p1V [0. 0. 0. 0.05263158 0. 0. 1. 0.05263158 0. 0. 0.10526316 0. 2. 0.05263158 0.05263158 0. 0. 0. 0.05263158 0.05263158 0. 0.05263158 0.10526316 0.05263158 0.05263158 0. 0.05263158 0.15789474 0.05263158 0. 0.05263158 0. ]pAb 0.5 &emsp;&emsp;例如在该数据集中，在已知的单词表中，’flea’在正面的评论中出现1次，正面评论的总特征数为24，所以概率为1/24=0.041，而在负面评论中出现0次，所以概率为0。负面评论中概率最大的单词为’stupid’，共出现3次，负面评论的总特征数为19，概率为3/19=0.15，即’stupid’是最能表征负面评论的特征。 测试算法&emsp;&emsp;对原始数据集进行贝叶斯算法训练得到其模型后，便可以根据实际的输入数据进行预测分类了。&emsp;&emsp;但现在会出现一个问题，当计算多个特征概率的乘积以获得某个类别的概率时，如果其中一个特征的概率为0，则最后的乘积也是0。为了解决这个问题，可以将所有特征出现的次数初始化为1，并将分母初始化为2。其次当多个特征的概率很小时，其乘积会越来越小，从而会影响数值的精度，为此，可以用取对数的方法，将乘积转换为求和。训练算法程序改写为：1234567p0Num = ones(numWords)p1Num = ones(numWords)p0Denom = 2.0p1Denom = 2.0...p1Vect = log(p1Num / p1Denom)p0Vect = log(p0Num / p0Denom) 测试算法程序123456789101112131415161718192021def classifyNB(vec2Classify, p0vec, p1Vec, pClass1): # 利用贝叶斯公式计算概率 p1 = sum(vec2Classify * p1Vec) + log(pClass1) p0 = sum(vec2Classify * p0vec) + log(1.0 - pClass1) if p1 &gt; p0: return 1 else: return 0def testingNB(): listOPosts, listClasses = loadDataSet() myVocabList = createVocabList(listOPosts) trainMat = [] for postinDoc in listOPosts: trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) p0V, p1V, pAb = trainNBO(trainMat, listClasses) # 输入的新数据 testEntry = &#123;'love', 'my', 'dalmation'&#125; thisDoc = array(setOfWords2Vec(myVocabList, testEntry)) classifyNB(thisDoc, p0V, p1V, pAb) 程序说明：&emsp;&emsp;classifyNB()函数有4个输入，待输入的新数据在特征列表中每个特征出现的次数，训练模型得到的类别0和类别1的特征概率和类别0的概率。因为是二分类，所以类别1的概率=1-类别0的概率。&emsp;&emsp;首先按照类别，将新数据的每个特征出现的次数与其类别的特征概率进行相乘相加，然后加上对应类别的概率，最后根据总的概率大小来判断分类。 三、感悟 贝叶斯算法属于有监督的预测算法，根据原始数据集训练出概率模型，然后对新的数据集进行预测。 贝叶斯算法训练的概率模型是基于贝叶斯公式的，由于数据集存在多个特征，需要对每个类别下的特征进行概率计算。 计算每个类别的特征概率时，用该类别下每个特征出现的概率除以该类别下出现的总特征个数，而不是全部类别下的总特征个数，即存在有些特征是该类别没有的。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://cxx0822.github.io/categories/机器学习/"}],"tags":[]},{"title":"C++笔记","slug":"C++笔记","date":"2018-12-20T08:29:42.000Z","updated":"2019-03-12T01:11:16.500Z","comments":true,"path":"2018/12/20/C++笔记/","link":"","permalink":"http://cxx0822.github.io/2018/12/20/C++笔记/","excerpt":"","text":"第1章 预备知识 C++融合了3种不同的编程方式：C语言代表的过程性语言、C++在C语言基础上添加的类代表的面向对象语言、C++模板支持的泛型编程。 面向过程强调算法，面向对象强调数据。类是一种规范，它描述了这种新型数据格式，对象是根据这种规范构造的特定数据结构。即类是抽象的，对象是具体的。类是模板，对象是实例。 第2章 开始学习C++ C++能够使用printf()、scanf()和其他所有标准C输入和输出函数，只需要包含常规C语言的stdio.h文件。 namespace的使用方法：using namespace std;&emsp; 所有的名称都可以使用using std::cout; &emsp;&emsp;&emsp;&emsp;只使用std里面的cout。 类是用户定义的一种数据类型。 C++程序应当为程序中使用的每个函数提供原型。C++不允许将函数定义嵌套在另一个函数定义中。 main()函数的返回值返回给操作系统。即返回1则程序正确执行。 第3章 处理数据 面向对象编程(OOP)的本质是设计并扩展自己的数据类型。 基本类型：整数和浮点数； 复合类型：数组、字符串、指针和结构。 最小长度：short：16位；int：至少与short一样长；long：至少32位，且至少与int一样长；long long：至少64位，且至少与long一样长。（1个字节8位） 整数相加减时，如果超越了限制，其值将为范围另一端的取值。例：unsigned short类型的0-1将得到65535。 cin只接受输入流中的第一个字符，其余字符会继续存放在流中，后续可以使用。 浮点数在计算机中分成两部分存储，一部分表示值，另一部分用于对值进行放大和缩小。 浮点数的缺点： 浮点数运算的速度比整数的慢，并且精度会降低。例：123float a = 2.34E+22f;float b = a + 1.0f;cout &lt;&lt; b - a &lt;&lt; endl; &emsp;&emsp;输出的结果应为1，但实际的输出值为0。因为a是一个小数点左边有23位的数字，加上1，即在第23位加1，而float类型只能表示数字中的前6位或前7位，即有效位为6或7，因此修改第23位对这个值并没有任何影响。&emsp;&emsp;对于float，C++只能保证6位有效位。 C++会自动执行很多类型转换，但当较大整型转换为较小的整型时，原来的值 可能超出目标类型的取值范围，通常只复制右边的字节，即高位将被截掉。例： 12int a = 7.2E12;cout &lt;&lt; a &lt;&lt; endl; &emsp;&emsp;由于a的值超过了int的最大取值范围，因此在某些操作系统上，得到的值为2147483647(int类型的最大值)。 C++类型转换：typeName(value) 第4章 复合类型 只有在定义数组时才能使用初始化，初始化时可以省略等号，不能将一个数组赋给另一个数组。如果只对数组的一部分初始化，则其他位置为0，如果初始化数组时，方括号内为空，则编译器会计算元素个数。例：1short a[] = &#123;1, 2, 3, 4&#125;; &emsp;&emsp;编译器会计算short型数组包含4个元素。&emsp;&emsp;short指定了类型，a表示变量名，[]表明是一个数组。 字符串与字符数组12char NJUT[4] = &#123;'n', 'j', 'u', 't'&#125;;char NJUT_2[5] = &#123;'n', 'j', 'u', 't', '\\0'&#125;; &emsp;&emsp;这2个数组都是char数组，但只有第2个是字符串。即字符串必须是以’\\0’结尾的字符数组。对于第2个字符数组，cout会打印’njut’，即遇到’\\0’则停止，但对于第1个，会一直打印，直到遇到内存中有’\\0’为止。&emsp;&emsp;还有1种更简单的方法是用一个引号将这些字符括起来，即1char NJUT[] = \"njut\"; &emsp;&emsp;这种称为字符串常量，编译器会自动将’\\0’加上。”njut”指的是字符串所在的内存地址。 123char name[10] = \"cxx\";cout &lt;&lt; strlen(name);cout &lt;&lt; sizeof(name); &emsp;&emsp;strlen()返回的是存储在数组中的字符串的个数，即可见的字符个数，因此该返回值为3，而sizeof()计算的是整个数组的长度。如果要存储一个字符串，则数组的长度至少是strlen(字符串)+1。 cin、cin.getline()和cin.get()cin：使用空白(空格，制表符和换行符)来确定字符串的结束位置。所以cin只能1次读取1个单词(遇到空格会结束)，剩下的部分会继续存放在cin流中，直到下次的读取。cin.getline(name,size)：函数读取整行，通过换行符确定字符串的结束位置，第1个参数为用来存储输入的数组名称，第2个为要读取的字符串的字符数，其值为size-1，即最后1位用于字符串结尾的空字符。cin.get()：其中一种形式get(name,size)和getline(name,size)类似，都读到行尾，但get()不会读取并丢弃换行符，而是会留在输入流中，这样就会存在一个问题，例：12cin.get(name1, 10);cin.get(name2, 10); &emsp;&emsp;当读完第1个名字后，由于get()并不会丢弃换行符，所以第2次读取的时候会直接读到换行符，从而会认为已经到达行尾，不会读取任何字符。为了避免这个问题，可以采用无参数的get()函数，即cin.get()，该函数读取下一个字符(换行符也可以)，即采用下面的方法：123cin.get(name1, 10);cin.get()cin.get(name2, 10); &emsp;&emsp;也可以将这2个函数拼在一起，即cin.get(name1, 10).get()。 string对象和字符数组之间的主要区别是：可以将string对象声明为简单变量，而不是数组。12string name;string name = \"cxx\"; &emsp;&emsp;string相当于一个类，字符串相当于类的实例化，不需要在string后面加上长度。 创建结构体时首先要创建一个模板。例1234567891011struct inflatable&#123; char name[20]; float volume;&#125;;inflatable guest = &#123; \"Glorious Gloria\"; 1.88;&#125;; &emsp;&emsp;前一个为结构体的声明，后一个为结构体的赋值。结构体相当于用户自定义的一种结构类型。struct指定了类型为结构体，inflatable为这种新类型的名称。大括号的内容为结构体的成员。整体相当于一个对象的模板，后面的则是对象的实例化，guest为结构体的名称，其具体内容为大括号里面的内容。&emsp;&emsp;注意，不管是声明还是赋值，都是C++的语句，需要加分号结束。 可以创建多个值相同的枚举量1enum number &#123;first, second, third = 1. forth = 100, fifth&#125;; &emsp;&emsp;first在默认情况下为0，second和third都为1，fifth为101。枚举也相当于一个数据类型，初始化相当于模板，使用时需要先实例化，在调用里面的枚举值，即：12number mynumber;mynumber = first; &emsp;&emsp;类似于const限定符。 只有初始化指针时，才会出现*=&amp;，其他情况，*指的是解除引用，即取出地址存放的值，&amp;指的是地址运算符，即获得变量存在的地址。 12int a = 5;int * pt = &amp;a; &emsp;&emsp;相当于12int * pt;pt = &amp;a; &emsp;&emsp;int指定了类型为整型，pt是变量名，*表明是指针，和数组，结构体类似，指针也是个复合类型，需要同基本类型同时使用，即pt是一个整型的指针类型。&emsp;&emsp;指针指的是地址，只有在初始化时才可以将*和&amp;同时使用，否则指针变量后面的赋值一定是某个地址。 在对指针变量使用解除引用运算符(*)之前，一定要将指针初始化为一个确定的，适当的地址。否则指针将找不到该地址，将错误的地址，甚至是正在运行的程序地址返回并执行操作。 一定要配对的使用new和delete，否则会发生内存泄漏。delete只能删除new创建的指针。 指针和字符串 12345678910111213141516171819char flower[10] = \"rose\";cout &lt;&lt; flower &lt;&lt; \"s are red.\\n;``` &amp;emsp;&amp;emsp;如果给cout提供一个字符的地址，则将从这个字符开始打印，直到遇到空字符为止。 11. 指针与数组、字符串、结构体```C++double wages[3] = &#123;100.0, 200.0, 300.0&#125;;double * p1 = wages;char animal[10] = \"bear\";char * p2 = animal;struct info&#123; int year;&#125;;info s01;info * p3 = &amp;s01; &emsp;&emsp;指针在初始化时，后面需要赋值一个地址，数组名和字符串常量都是地址，所以不需要再加上取地址符。 vector和array123456vector&lt;typename&gt; vt(n_elem);array&lt;typename, n_elem&gt; arr;double a1[4] = &#123;1.2, 2.3, 3.4, 4.5&#125;;vector&lt;double&gt; a2(4);array&lt;double, 4&gt; a3 = &#123;1.2, 2.3, 3.4, 4.5&#125;; &emsp;&emsp;使用vector和array，需要添加头文件#include和#include，且都可以使用标准的数组索引访问数组中的元素。 new创建指针1typeName * pointer_name = new typeName; 第5章 循环和关系表达式 for循环：1234for (初始化;测试语句;更新语句)&#123; 主体;&#125; &emsp;&emsp;测试语句可以是任意表达式，C++会将结果强制转换为bool类型。&emsp;&emsp;C++中，在for和括号之间加上一个空格，以区别函数名和括号。 1cout.setf(ios_base::boolalpha); &emsp;&emsp;设置显示为布尔值。 第6章 分支语句和逻辑运算符 c++有if - else if - else 结构 isalpha()：测试字符是否为字母字符，isdigits()：测试字符是否为数字字符，isspace()：测试字符是否为空白(换行符，空格和制表符)，ispunct()：测试字符是否为标点符号。 第7章 函数 如果声明的返回值类型为double，而函数返回一个int表达式，则该int值会将强制转换为double类型。 C++的返回值不可以是数组，但可以将数组作为结构或对象组成部分来返回。 在函数中使用指针来处理数组 12345678910int sum_arr(int * arr, int n); //函数原型int sum = sum_arr(cookies, 10); //函数调用``` &amp;emsp;&amp;emsp;这里的cookies为数组名，表示数组中第一个元素的地址，而函数原型中，\\* arr表示的也是地址，当然也可以用arr[]替换，因为在c++中，当且仅当用于函数头或函数原型中，* arr 和 arr[]的含义是相同的。 &amp;emsp;&amp;emsp;在使用数组名作为参数时，并没有将数组的全部内容传递给函数，而是将数组的地址，包含的元素类型以及数目传递给函数，这样会大大减少内存空间。 4. 指针常量和常量指针```C++const int * pt = &amp;age; //常量指针int * const finger = &amp;sloth; //指针常量 &emsp;&emsp;变量名左边是变量的类型，当有多个类型修饰时，看最近的。&emsp;&emsp;变量pt最左边是*，代表的是指针，再往左是int，表明基本类型为整型，最左边是const，表明其值是一个常量。所以pt首先是一个指针，其存放的内容是变量age的地址，地址指向的是一个整型数据，且该数据是个常量，即const int 是共同修饰*pt的。所以变量pt指向的值是不可以改变的，即地址不可变。&emsp;&emsp;而变量finger最左边是const,代表的是常量，再往左是 *,表明是一个指针，最左边是int，表明基本类型为整型。所以finger首先是一个常量，只是该常量指向的是一个int型的指针，所以其指向的变量不可以更改。12345678910int gorp = 16;int chips = 12;const int * p_snack = &amp;gorp;*p_snack = 20; //禁止修改p_snack = &amp;chips; //可以修改int * const p_snack = &amp;gorp;*p_snack = 20; //可以修改p_snack = &amp;chips; //禁止修改 &emsp;&emsp;第一个创建的是常量指针，所以指针指向的地址不可以修改，该例中，p_snack指针指向的是gorp的地址，而该地址存放的变量值是16，所以*p_snack的值只能是16，不能是其他值。但是指针指向的变量是可以更改的，即原来是指向gorp变量地址的，现在可以改为指向chips变量地址，但其值仍为16。虽然不能直接修改，但可以通过改变chips的值修改，比如将chips的值改为20，那么现在该常量指针的值就是20了。&emsp;&emsp;第二个创建的是指针常量，所以指针指向的变量是不可以修改的。也就是变量p_snack是一个常量，其值是不可以修改的，只不过这个变量是指针，而指针的值是地址，所以p_snack只能指向chips变量，其值等于chips的地址，但可以修改该地址对应的值，即本来存放chips变量的地址，其值是12，可以修改为20。 二维数组与指针 1arr[a][b] = *(*(arr +a) + b) 函数与字符串数组 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;void show_char(const char * a[]);void show_string(string a[]);const char* Seasons_char[4] = &#123;\"Spring\", \"Summer\", \"Autumn\", \"winter\"&#125;;string Seasons_string[4] = &#123;\"Spring\", \"Summer\", \"Autumn\", \"winter\"&#125;;void show_char(const char * a[])&#123; for (int i = 0; i &lt; 4; i++) cout &lt;&lt; a[i] &lt;&lt; endl;&#125;void show_string(string a[])&#123; for (int i = 0; i &lt; 4; i++) cout &lt;&lt; a[i] &lt;&lt; endl;&#125;int main(int argc, char const *argv[])&#123; show_char(Seasons_char); show_string(Seasons_string); return 0;&#125; &emsp;&emsp;创建字符串数组的两种形式。第一种形式时，必须要加const。相当于二维数组char Seasons_char[][4]。 函数与结构体/结构体指针1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;iostream&gt;using namespace std;struct box&#123; char maker[40]; float height; float width; float length; float volume;&#125;;void show(box x)&#123; cout &lt;&lt; \"maker: \" &lt;&lt; x.maker &lt;&lt; endl; cout &lt;&lt; \"height: \" &lt;&lt; x.height &lt;&lt; endl; cout &lt;&lt; \"width: \" &lt;&lt; x.width &lt;&lt; endl; cout &lt;&lt; \"length: \" &lt;&lt; x.length &lt;&lt; endl; cout &lt;&lt; \"volume: \" &lt;&lt; x.volume &lt;&lt; endl;&#125;void show_pointer(box * x)&#123; x-&gt;volume = x-&gt;height * x-&gt;length * x-&gt;width; cout &lt;&lt; \"maker: \" &lt;&lt; x-&gt;maker &lt;&lt; endl; cout &lt;&lt; \"height: \" &lt;&lt; x-&gt;height &lt;&lt; endl; cout &lt;&lt; \"width: \" &lt;&lt; x-&gt;width &lt;&lt; endl; cout &lt;&lt; \"length: \" &lt;&lt; x-&gt;length &lt;&lt; endl; cout &lt;&lt; \"volume: \" &lt;&lt; x-&gt;volume &lt;&lt; endl;&#125;void show(box x);void show_pointer(box * x);int main(int argc, char const *argv[])&#123; box box1 = &#123;\"cxx\", 1, 1, 1.2&#125;; show(box1); show_pointer(&amp;box1); return 0;&#125; &emsp;&emsp;在C++中，结构体struct和整型int类似，也是一种数据类型，只不过是由用户自己定义的，所以可以将其看作是int型。&emsp;&emsp;上面代码创建了2个函数，一个是按值传递结构体，一个是按地址传递结构体。在使用结构体时要先定义一个结构体模板。&emsp;&emsp;按值传递的结构体中，函数头是void show(box x);表明其数据类型是box，而box就是定义好的结构体类型，相当于int x。在按值传递时，通过用点运算符访问成员变量，调用函数时，传递的参数为结构体的变量名，而按地址传递时，函数头是void show_pointer(box * x);表明其数据类型也是box，但是传递的是一个指针，也就是结构体变量的地址，在按地址传递时，需要用-&gt;运算符访问成员变量，调用函数时，传递的参数应该是结构体变量的地址，即在变量名前面加上取地址符。 函数调用时，传递的参数要么是变量名，要么是变量的地址，不需要加修饰符。 与普通的变量一样，函数也有地址。函数的地址是存储其机器语言代码的内存的开始地址。函数名即函数的地址，后面的括号表示调用，不需要加取地址符。这样就可以将函数名作为一个参数传递给另一个函数。 123double pam(int); //函数声明double (*pf)(int); //函数指针double * pf(int); //指针函数 &emsp;&emsp;声明一个函数pam，有1个参数为int，返回值为double。用一个指针(*pf)来替代函数名就可以声明一个函数指针，也就是必须要指针函数的参数类型和返回值类型。此时pam和*pf一样，都是函数名，而pf就是函数指针，即函数的地址。&emsp;&emsp;指针必须用括号括起来，否则pf会和后面的括号先结合，然后和*结合，变成指针函数，相当于创建一个pf(int)的函数，其返回值为double类型的指针。 未完待续…","categories":[{"name":"C++","slug":"C","permalink":"http://cxx0822.github.io/categories/C/"}],"tags":[]},{"title":"决策树算法","slug":"决策树算法","date":"2018-12-18T01:19:49.000Z","updated":"2019-01-08T03:29:21.310Z","comments":true,"path":"2018/12/18/决策树算法/","link":"","permalink":"http://cxx0822.github.io/2018/12/18/决策树算法/","excerpt":"","text":"决策树算法一、基本原理&emsp;&emsp;决策树类似于二叉树结构，按照数据集中的特征依次对其分类，直到所有的数据分类完毕。&emsp;&emsp;首先在已知数据集上确定哪个特征在划分数据分类时起决定性作用。为了找到决定性的特征，划分出最好的结果，必须要评估每个特征。此时，原始数据集就会被划分为几个子数据集分支。如果某个分支下的数据都属于同一类型，即该子集中的所有数据的特性均相同，则无需进一步对数据集进行划分。否则需要重复划分数据子集，直到所有具有相同类型的数据均在一个数据子集内。&emsp;&emsp;构造决策树最关键的是如何找到决定性的特征。是否划分的原则为子数据集中的所有数据是否均具有相同的特征。 二、算法实现&emsp;&emsp;伪代码函数decisionTree()：&emsp;&emsp;检测数据集中每个子项是否属于同一类：&emsp;&emsp;IF so&emsp;&emsp;&emsp;return 类标签&emsp;&emsp;Else&emsp;&emsp;&emsp;寻找划分数据集的最好特性&emsp;&emsp;&emsp;划分数据集&emsp;&emsp;&emsp;创建分支节点&emsp;&emsp;&emsp;&emsp;for 每个划分的子集&emsp;&emsp;&emsp;&emsp;&emsp;调用函数decisionTree()并将结果增加到分支节点中&emsp;&emsp;&emsp;return 分支节点 信息增益&emsp;&emsp;在划分数据集之前之后信息发生的变化称为信息增益。获得信息增益最高的特征就是最好的分类选择。计算信息增益常用的是香农熵(熵)。熵的定义为信息的期望值，表示数据的无序性。其计算公式为：python程序：1234567891011121314from math import logdef calcShannonEnt(dataSet): numEntries = len(dataSet) # 计算总数 labelCounts = &#123;&#125; for featVec in dataSet: currentLabel = featVec[-1] if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 # 计算每个分类的总数 shannonEnt = 0.0 for key in labelCounts: prob = float(labelCounts[key]) / numEntries shannonEnt -= prob * log(prob, 2) # 带入公式求和 return shannonEnt 程序说明：&emsp;&emsp;函数的输入为原始数据集，其中最后一列为分类标签。首先计算数据集的总个数，其次将数据集按照分类标签分类，并统计每个子类的总数。最后根据熵的计算公式，依次对每类的信息增益进行求和。 例：有一组已知分类标签的数据集，计算其香农熵。&emsp;&emsp;利用python创建数据集：1234def createDataSet(): dataSet = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] return dataSet, labels &emsp;&emsp;根据公式其香农熵为：&emsp;&emsp;数据集一共有5组数据，其中分类标签为’yes’的有3个，分类标签为’no’的有2个。熵越高，则混合的数据也越多。 划分数据集&emsp;&emsp;除了计算整体数据的信息熵，还需要计算划分数据集的熵。对每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是最好的划分方式。其计算公式为：&emsp;&emsp;首先按照每个特征划分数据集：python程序：12345678def splitDataSet(dataSet, axis, value): retDataSet = [] for featVec in dataSet: if featVec[axis] == value: reducedFeatVec = featVec[:axis] # 截取之前的数据到该特征 reducedFeatVec.extend(featVec[axis + 1:]) # 将该特征后的数据加入到列表中 retDataSet.append(reducedFeatVec) return retDataSet 程序说明：&emsp;&emsp;函数的三个输入分别为：待划分的数据集，数据集中的某个特征，该特征对应的特征值。例如splitDataSet(dataSet, 0, 1)表示划分出数据集dataSet中第0个特征为1的数据，即第1个特征为1的数据，python的列表索引从0开始。(dataSet即createDataSet()函数生成的数据集)&emsp;&emsp;首先利用for循环遍历整个数据集，当对应的特征等于给定的值后，先将该特征之前的数据添加到列表中，然后将之后的数据添加到列表中。&emsp;&emsp;splitDataSet(dataSet, 0, 1)的返回值为：[[1,’yes’], [1, ‘yes’], [0, ‘no’]] &emsp;&emsp;对每个特征划分完成后，需要计算每个数据子集的熵，从而选择最好的数据集划分方式。python程序：1234567891011121314151617181920def chooseBestFeatureToSplit(dataSet): numFeatures = len(dataSet[0]) - 1 # 减去数据集中的标签 baseEntropy = calcShannonEnt(dataSet) bestInfoGain = 0.0 bestFeature = -1 for i in range(numFeatures): featList = [example[i] for example in dataSet] # 获得数据集中某一列的值 uniqueVals = set(featList) # set():使数据为不重复的序列 newEntropy = 0.0 # 划分数据集，代入公式 for value in uniqueVals: subDataSet = splitDataSet(dataSet, i, value) prob = len(subDataSet) / float(len(dataSet)) newEntropy += prob * calcShannonEnt(subDataSet) infoGain = baseEntropy - newEntropy # 计算最大的熵，作为划分数据集的方式 if (infoGain &gt; bestInfoGain): bestInfoGain = infoGain bestFeature = i return bestFeature 程序说明：&emsp;&emsp;函数首先计算总的数据集的熵，然后将最好的熵和特征初始化为0和-1。然后需要将划分数据集，计算每个子集的熵，但首先要知道数据集中每个特征有多少特征值。函数首先获得每一个特征的所有特征值，然后利用set()将重复的值去掉。featList返回的就是每个特征对的所有特征值。createDataSet()生成的数据集的featList为：[1, 1, 1, 0, 0]和[1, 1, 0, 1, 1]。最后，对于划分好的每个数据集依次计算熵，并选择最大的熵作为最好的划分方式。&emsp;&emsp;H(1)、H(2)分别表示第1，2种分类结果，数据集中特征0，即第1个特征的分类结果为1和0。其中结果为1的数据集为：[[1,’yes’], [1, ‘yes’], [0, ‘no’]]，’yes’有2个，’no’有1个；结果为0的数据集为：[[1, ‘no’],[1, ‘no’]],’yes’有0个，’no’有2个。特征1的计算类似。 构建决策树&emsp;&emsp;划分好一次数据集之后，数据将被向下传递到下一个节点，然后在此节点上做进一步的划分。因此可用递归的原则处理原始数据集。&emsp;&emsp;递归的结束条件为：遍历完原始数据集的所有数据或者每个子集的数据特征均相同。 python程序实现： def createTree(dataSet, labels): classList = [example[-1] for example in dataSet] # 第一种终止条件 if classList.count(classList[0]) == len(classList): return classList[0] # 第二种终止条件 if len(dataSet[0]) == 1: return majorityCnt(classList) bestFeat = chooseBestFeatureToSplit(dataSet) # 划分数据集，选择最好的分类标签 bestFeatLabel = labels[bestFeat] myTree = {bestFeatLabel: {}} del(labels[bestFeat]) # 删除已分类好的标签 featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) # 按之前最好的分类标签继续往下分类 for value in uniqueVals: subLabels = labels[:] myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) # 列表的值为划分后的数据集 return myTree def majorityCnt(classList): classCount = {} for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] 程序说明：&emsp;&emsp;函数的输入参数为原始数据集和分类标签。首先判断该数据集是否能进行划分。其递归结束的条件有2个，一个是划分后的数据子集的所有特征属性均相同。即数据集中第一个特征的个数即该数据集的总特征个数。另一个是当遍历完所有的原始数据集的特征之后，仍然存在没有划分成包含唯一类别的子类，这时候采用出现出现次数最多的类别作为其返回值。然后计算数据集中的熵并选择最好的分类特征，再将此特征删除，得到数据子集，再依次对子集进行递归处理，直到分类完成。&emsp;&emsp;该程序最终的结果为：{‘no surfacing’: {0: ‘no’, 1: {‘flippers’: {0: ‘no’, 1: ‘yes’}}}}。即首先根据“不浮在水面是否可以生存”的特征分成“0”(否)和“1”(是)两类。其中“0”这一类的子数据集的特征均相同，不需要在划分。而“1”这一类再按照“是否有脚蹼”继续分类。 三、感悟3.1 python知识(1)extend和append的区别：extend:将一个序列添加到列表中；append：将一个对象添加到列表中；即append是将要添加的数据作为一个新的对象添加到列表中，不属于原来的列表，而extend添加后的数据仍然属于原来的列表。例:a = [1, 2], b = [3]a.extend(b) : [1, 2, 3]a.append(b) : [1, 3, [3]](2)set():使数据为不重复的序列 3.2 算法(1)决策树属于有监督的算法。因为其原始数据集的分类标签是已知的。(2)决策树类似于二叉数，将原始数据集按照每个特征的标签依次划分，直到所有的数据划分完成。(3)构建决策树主要分为两步，首先选择最好的划分类别，将原始数据集分成子数据集，其次判断递归的结束条件是否满足，即需不需要继续往下划分。(4)选择最好的划分类别时，常用的判断依据是计算每个特征的子数据集的熵，熵代表的是数据集的混乱程度，数据越混乱，熵越高，其分类标准也越好。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://cxx0822.github.io/categories/机器学习/"}],"tags":[]},{"title":"k-近邻算法","slug":"k-近邻算法","date":"2018-12-17T03:12:16.000Z","updated":"2019-01-08T06:53:23.264Z","comments":true,"path":"2018/12/17/k-近邻算法/","link":"","permalink":"http://cxx0822.github.io/2018/12/17/k-近邻算法/","excerpt":"","text":"k-近邻算法一、基本原理&emsp;&emsp;已知一个样本数据集合，包括每组数据及对应的标签，即数据集已经分类好。当再次输入一组数据时，将新数据的每个特征与样本集中的数据对应的特征进行比较，选择样本集中特征最相似（最近邻）的分类标签。&emsp;&emsp;我们只选择样本集中前k个最相似的数据，这也就是k-近邻算法的出处。最后选择k个最相似数据中出现次数最多的分类，作为新数据的分类，即判断这k个数据中在数据集中的分类标签各自是什么，最后判断最多的分类标签是什么，将此作为新数据的分类。 二、算法实现&emsp;&emsp;对未知类别属性的数据集中的每个点依次执行以下操作：(1)计算已知类别数据集中的点与当前点之前的距离；(特征比较)(2)按照距离递增次序排序；(3)选取与当前点距离最小的k个点；(最近邻)(4)确定前k个点所在的类别出现频率；(判断标签)(5)返回前k个点出现频率最高的类别作为当前点的预测分类。 python代码：123456789101112131415161718192021222324def classifyKNN(inputData, dateSet, labels, k): dataSetSize = dataSet.shape[0] # 返回dataSet的行数 # 计算距离，输入的数据与每一个数据集里面的数据求距离 diffMat = np.tile(inputData, (dataSetSize, 1)) - dataSet # tile()：将数据重复m行n列 sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances ** 0.5 # 返回距离中元素从小到大排序后的索引值，可理解为从小到大排序，返回是值对应的索引值 sortedDistIndices = distances.argsort() # argsort(): 返回数组值从小到大的索引值 classCount = &#123;&#125; for i in range(k): # 按照从小到大的顺序依次获得 对应距离的类别属于label的哪一类，并计数 voteLabel = labels[sortedDistIndices[i]] classCount[voteLabel] = classCount.get(voteLabel, 0) + 1 # 将字典按照对应键的值排序，即根据出现的频率次数排序 sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] 代码说明：&emsp;&emsp;函数有4个输入参数：输入数据inputData(1行m列)，原始数据集dateSet(m行n列)，分类标签label，即原始数据集中每行数据属于哪一类，分类基准k，即算法中的k。&emsp;&emsp;首先要计算输入数据与原始数据集中每行数据的距离。先将输入数据重复m行，在依次和数据集中的每行数据做距离运算。(最简单的距离为欧式距离。)&emsp;&emsp;其次对计算过的距离进行排序并计数，按照从小到大的顺序选取前k个。&emsp;&emsp;最后统计这前k个中出现频率最高的分类标签，并将此作为输入数据的分类标签。 三、项目实战——约会网站的配对效果&emsp;&emsp;项目流程：(1)收集数据：提供文本文件(2)准备数据：使用Python解析文本文件&emsp;&emsp;将收集到的原始数据转换为numpy数据，以便于后续分析(3)分析数据：使用matplotlib画二维散点图(4)训练算法：即编写KNN算法(5)测试算法：利用部分原始数据集中的数据作为测试样本，对于KNN算法进行测试(6)使用算法：输入一个新的数据进行预测python代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107# 项目1：优化约会网站的配对效果import numpy as npimport operator# 将数据分为两部分，即特征矩阵和对应的分类标签向量def file2matrix(filename): # 获取特征矩阵的行（numberOfLines）和列（3） fr = open(filename) arrayOfLines = fr.readlines() numberOfLines = len(arrayOfLines) returnMat = np.zeros((numberOfLines, 3)) classLabelVector = [] index = 0 # 行的索引值 for line in arrayOfLines: line = line.strip() listFromLine = line.split('\\t') # 前3列为特征矩阵 returnMat[index, :] = listFromLine[0:3] # 最后1列为标签值 if listFromLine[-1] == 'didntLike': classLabelVector.append(1) elif listFromLine[-1] == 'smallDoses': classLabelVector.append(2) elif listFromLine[-1] == 'largeDoses': classLabelVector.append(3) index += 1 # 下一行 return returnMat, classLabelVector# 归一化处理公式： newValue = (oldValue - min) / (max - min)def autoNorm(dataSet): # min()获取矩阵每一行的最小值，min(0)获取矩阵每一列的最小值 minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals normDataSet = np.zeros(np.shape(dataSet)) m = dataSet.shape[0] # 每一行数据均需要归一化处理 # tile()：将数据重复m行n列 normDataSet = dataSet - np.tile(ranges, (m, 1)) normDataSet = normDataSet / np.tile(ranges, (m, 1)) return normDataSet, ranges, minValsdef classifyKNN(inputData, dataSet, labels, k): dataSetSize = dataSet.shape[0] # 返回dataSet的行数 # 计算距离，输入的数据与每一个数据集里面的数据求距离 diffMat = np.tile(inputData, (dataSetSize, 1)) - dataSet sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances ** 0.5 # 返回距离中元素从小到大排序后的索引值，可理解为从小到大排序，返回是值对应的索引值 sortedDistIndices = distances.argsort() classCount = &#123;&#125; for i in range(k): # 按照从小到大的顺序依次获得 对应距离的类别属于label的哪一类，并计数 voteLabel = labels[sortedDistIndices[i]] classCount[voteLabel] = classCount.get(voteLabel, 0) + 1 # 将字典按照对应键的值排序，即根据出现的频率次数排序 sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0]def datingClassTest(): hoRatio = 0.1 # 选取10%的数据集 datingDataMat, datingLabels = file2matrix(filename) normMat, ranges, minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m * hoRatio) errorCount = 0.0 for i in range(numTestVecs): # 选取后10%作为测试数据：第numTestVecs行到第m行 classifierResult = classifyKNN(normMat[i, :], normMat[numTestVecs:m, :], datingLabels[numTestVecs:m], 3) if (classifierResult != datingLabels[i]): errorCount += 1.0 print(\"the total error rate is : %f\" % (errorCount / float(numTestVecs)))def classifyPerson(): percentTats = float(input(\"percentage of time spent playing video games?\")) ffMiles = float(input(\"frequent flier miles earned per year?\")) iceCream = float(input(\"liters of ice cream consumed per year?\")) datingDataMat, datingLabels = file2matrix(filename) normMat, ranges, minVals = autoNorm(datingDataMat) inputData = np.array([ffMiles, percentTats, iceCream]) classifierResult = classifyKNN((inputData - minVals) / ranges, normMat, datingLabels, 3) resultList = ['not at all', 'in small doses', 'in large doses'] print(\"You will probably like this person: \", resultList[classifierResult])if __name__ == '__main__': filename = \"datingTestSet.txt\" datingClassTest() classifyPerson() 四、感悟4.1 python知识(1)np.zero((m,n)):创建一个m*n的零矩阵(2)str.strip():删除字符串中的空白;str.split(‘\\t’):TAB键切分字符串(3)np.shape(A):返回A矩阵的行数和列数;A.shape[0]:返回A的行数;A.shape[1]:返回A的列数(4)np.title(a,(m,n)):将数据a重复m行n列 4.2 算法(1)k-近邻算法也称为KNN算法，属于有监督的算法。因为其原始数据集的分类标签是已知的。(2)k-近邻算法最重要的一部分为计算新数据与原始数据集的距离。其核心思想为距离越近越认为是该分类标签。(3)最后选择k个最相似数据中出现次数最多的分类，作为新数据的分类。这句话意思是，比如第1个距离最近的分类为好，第2个距离最近的分类为好，第3个距离最近的分类为坏，第4个距离最近的分类为好，那么前4个中，分类为好为3次，分类为坏为1次，则判定该新数据的分类为好。因此一般k的取值为奇数。(4)在k-近邻算法中，使用的距离为欧式距离，即平面内两点间的距离，在其他算法会使用其他的距离来判断。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://cxx0822.github.io/categories/机器学习/"}],"tags":[]},{"title":"NAO高尔夫比赛（python提高版）","slug":"NAO高尔夫比赛（python提高版）","date":"2018-12-17T02:29:29.000Z","updated":"2019-03-20T10:13:50.729Z","comments":true,"path":"2018/12/17/NAO高尔夫比赛（python提高版）/","link":"","permalink":"http://cxx0822.github.io/2018/12/17/NAO高尔夫比赛（python提高版）/","excerpt":"","text":"NAO高尔夫比赛（python提高版）比赛规则场地 图1 高尔夫比赛场地（白圈为球的起点，蓝圈为球洞） &emsp;&emsp;一共分为3个球洞，场地是一样的，只是球的起点位置不同。第一关起点离球洞3m处，第二关起点离场地左边界50cm处，第三关起点离场地下边界50cm处。 球洞 图2 球洞 &emsp;&emsp;球洞由黄杆和landmark组成。 比赛时间及要求&emsp;&emsp;总时长为23min，且击球总次数为10杆。球不可以出界，机器人可以出界。 比赛策略程序模块 图3 程序模块 &emsp;&emsp;高尔夫比赛主要用到了NAO机器人的视觉和运动模块。视觉模块包括红球、黄杆和landmark的识别。运动模块则包括步态、定位和击球等。 程序逻辑 图4 程序逻辑 &emsp;&emsp;主逻辑即：行走找球-&gt;球洞定位-&gt;击球。具体程序逻辑思想可参见python初级版的博客。 提高版的改进摇头找黄杆和找landmark的改进&emsp;&emsp;在初级版的程序设计中，采用的是先摇头找landmark，找不到则摇头找黄杆的策略。但在考虑到时间缩短的情况下，这样明显会浪费一些时间，因此在提高版的程序中，我们在一次摇头的动作中依次找landmark和黄杆。并将下一次定位时找黄杆或landmark的角度设置为上一个角度的大致范围，而不是从头开始找。 python代码：12345678910111213141516171819202122def moveheadToFindLandmarkandStick(self, yawAngles): ''' 摇头找Landmark和黄杆 返回值： 列表[targetDis, targetAngle, compensateAngle] ''' for yawAngle in yawAngles: isfindLandmark = self.findLandmark(pitchAngle=0, yawAngle=yawAngle) if isfindLandmark: targetDis = self.landmark_info[2] targetAngle = self.landmark_info[3] compensateAngle = 10 * rad break isfindStick = self.findStick(pitchAngle=0, yawAngle=yawAngle) if isfindStick: targetDis = -999 targetAngle = self.stickAngle compensateAngle = self.compensateAngle1(self.hitballtimes) break return [targetDis, targetAngle, compensateAngle] 程序说明：&emsp;&emsp;首先找landmark，因为landmark的识别精度和返回的信息比黄杆好，如果找到则返回距离和角度信息。由于黄杆无法返回距离信息，所以将其距离设置为很大的负值，以便于后面的球洞定位判断。 行走找球(walkToBall())的改进&emsp;&emsp;当NAO机器人在行走找球时，由于此时是动态的，或者当距离比较远时，摄像头会捕捉不到红球的信息，而初级版的代码是直接摇头再找一次，但此时机器人仍处于运动的状态，因此还是会出现找不到球的情况，甚至会出现走过红球的情况，所以当出现这种情况时，停下来再找一次是很有必要的。&emsp;&emsp;在改进的代码中，当出现这种情况时，首先停止行走，然后在原地找一下球，找到则继续，否则上下左右摇头找一下球。&emsp;&emsp;其次，由于之前的行走步态参数设置过大，当出现球的角度超过最大偏离角而校正时，由于步伐过大，导致机器人离球很近，从而出现球不在视野内的情况。所以我们将行走的步态参数改小了一点，并将最小接近球的距离放大一点，从而避免这种情况的发生。 python代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475def walkToBall(self, min_ball_d=0.4, max_ball_theta=0.15): ''' 行走到球附近，若没有找到球则往前走一点 参数： min_ball_d:最小接近球的距离 max_ball_theta:最大偏离角 ''' # 如果没有找到球则摇头找球 isfindBall = self.findBall() if isfindBall is False: while True: isfindBall = self.moveheadToFindball() if isfindBall is False: self.motionProxy.moveInit() self.motionProxy.moveTo(0.5, 0, 0, self.walkconfiguration.WalkLineMiddle_blue()) else: break self.motionProxy.moveTo(0, 0, self.ball_info[2], self.walkconfiguration.WalkCircleLittle_blue()) self.motionProxy.angleInterpolationWithSpeed(\"HeadYaw\", 0, 0.1) # 头部回正 moveTask_1 = self.motionProxy.post.moveTo(1.5 * self.ball_info[0], 0, 0, self.walkconfiguration.WalkLineMiddle_blue()) while True: isfindBall = self.findBall(self.pitchAngle, 0) if isfindBall is False: # 行走过程中没有找到球，则停下来找球 self.motionProxy.stop(moveTask_1) time.sleep(0.5) self.motionProxy.moveInit() isfindBall_2 = self.moveheadToFindball(pitchAngles=[self.pitchAngle, self.pitchAngle + 10, self.pitchAngle - 10], yawAngles=[-15, 15]) if isfindBall_2: moveTask_1 = self.motionProxy.post.moveTo(1.5 * self.ball_info[0], 0, 0, self.walkconfiguration.WalkLineMiddle_blue()) ball_d = ((self.ball_info[0] ** 2 + self.ball_info[1] ** 2) ** 0.5) ball_theta = abs(self.ball_info[2]) time.sleep(0.5) # 头不断往下低,防止看不到球 Names = [\"HeadPitch\"] if ball_d &gt; 0.6: self.pitchAngle = 0 elif 0.4 &lt; ball_d &lt; 0.6: self.pitchAngle = 10 elif 0.02 &lt; ball_d &lt; 0.4: self.pitchAngle = 20 self.motionProxy.angleInterpolationWithSpeed(Names, self.pitchAngle * rad, 0.1) # 到达最小距离内，停止 if min_ball_d / 3 &lt; ball_d &lt; min_ball_d: self.tts.say(\"I am right.\") self.motionProxy.stop(moveTask_1) time.sleep(0.5) self.motionProxy.moveInit() break # 偏差在范围内，继续走 elif 0.02 &lt; abs(ball_theta) &lt; max_ball_theta: # 到达最小接近球的距离 if min_ball_d / 3 &lt; ball_d &lt; min_ball_d: self.tts.say(\"I am right.\") self.motionProxy.stop(moveTask_1) time.sleep(0.5) self.motionProxy.moveInit() break else: continue # 超过最大偏离角 elif abs(ball_theta) &gt; max_ball_theta: self.tts.say(\"I am wrong.\") self.motionProxy.stop(moveTask_1) time.sleep(0.5) self.motionProxy.moveInit() self.motionProxy.moveTo(0, 0, self.ball_info[2], self.walkconfiguration.WalkLineMiddle_blue()) moveTask_1 = self.motionProxy.post.moveTo(1.5 * self.ball_info[0], 0, 0, self.walkconfiguration.WalkLineMiddle_blue()) 球洞定位的改进&emsp;&emsp;提高版在球洞定位上面仍然采用初级版的三角定位策略，具体可参见python初级版的博客。但较之前的策略，提高版将依次同时识别landmark和黄杆，即在一次摇头的过程中，先识别landmark，如果识别到，则进入landmark的三角定位，否则识别黄杆，进入黄杆的三角定位。 仍然存在的问题(1)识别黄杆后，仍然不能返回有效的距离信息，这不利于确定击球的力度。(2)击球力度应由距离来确定。应对力度和距离进行数据拟合，得到两者的关系，以确保每次击球的有效距离能达到预期值。(3)应尽量避免反手击球，如果不能在路径规划上解决，可以考虑绕着红球转一个大的圆弧。(4)设计一个切换比赛关数的函数。","categories":[{"name":"NAO高尔夫比赛","slug":"NAO高尔夫比赛","permalink":"http://cxx0822.github.io/categories/NAO高尔夫比赛/"}],"tags":[]},{"title":"MATLAB的GUI小游戏:你话我猜","slug":"MATLAB的GUI小游戏：你话我猜","date":"2018-11-09T09:20:25.000Z","updated":"2020-01-09T07:20:23.401Z","comments":true,"path":"2018/11/09/MATLAB的GUI小游戏：你话我猜/","link":"","permalink":"http://cxx0822.github.io/2018/11/09/MATLAB的GUI小游戏：你话我猜/","excerpt":"","text":"&emsp;&emsp;使用MATLAB中的GUI设计一个小游戏——你话我猜，最终效果如图所示：&emsp;&emsp;这是主界面，有四个选项，单击文字前面的图标即可跳转至子界面，如图所示： 这是子界面，有3个选项，单击Start倒计时开始，单击Right则答对个数+1，单击Next则调至下一题，直到倒计时为0。 一、创建GUI&emsp;&emsp;打开MATLAB（本游戏编译环境为2014a），选择HOME-&gt;New-&gt;GUI，在弹出的窗口中选择Blank GUI，点击OK，即可创建空白GUI。&emsp;&emsp;将新建的GUI命名为Games_1。在MATLAB左侧的Current Folder中可以看到有2个文件，一个为Games_1.m和Games_1.jpg，其中.m为MATLAB编译文件，.jpg为GUI文件。右击Games_1.jpg并选择Open in GUIDE即可看到GUI界面。&emsp;&emsp;左侧为控件栏，拖动至中间空白区域即可创建，中间区域为GUI界面，运行对应的.m文件即可显示界面。 二、在主界面添加背景图片和文字&emsp;&emsp;MATLAB中在GUI添加背景图片的常用做法是：创建一个坐标系，将图片添加至坐标系中，然后将坐标系的坐标轴设置为不可见，为了将文字写在图片上，可将该坐标系设置为下一层。打开Games_1.m，在function Games_1_OpeningFcn函数中输入代码。 123456Games.ax = axes('units','normalized', 'position',[0 0 1 1]); %单位化uistack(Games.ax,'down'); %把新创建的坐标系在下移一层Games.img = imread('Games1.jpg'); %打开图片Games.IH = image(Games.img); % 显示图片colormap gray % 设置窗口的调色板为灰色set(Games.ax,'handlevisibility','off','visible','off');%把创建的坐标系句柄设为不可见 &emsp;&emsp;其次，需要在图片上面打印文字。同样的创建一个坐标系并将其坐标轴设置为不可见，利用text()函数即可打印文字。 123456789Games2.ax=axes('units','normalized','pos',[0 0 1 1]); text(0.2,0.9,'游戏环节之你话我猜','fontsize',80) %第一个数横坐标，第二个数纵坐标text(0.5,0.8,'请任选一项，开始你的表演！','fontsize',40,'color','b') text(0.1,0.48,'成语','fontsize',80,'color','b')text(0.1,0.18,'体育','fontsize',80,'color','b')text(0.66,0.48,'演技王','fontsize',80,'color','b')text(0.63,0.18,'知识百科','fontsize',80,'color','b')text(0.80,0.04,'Copyright@cxx','fontsize',30)set(Games2.ax,'handlevisibility','off','visible','off'); &emsp;&emsp;最终效果如图所示： 三、添加控件和控件图标，实现单击图标即可跳转至子界面3.1 添加控件并实现单击控件跳转至子界面&emsp;&emsp;右击Games_1.fig，选择open in GUIDE，在控件栏中拖动Push Button至中间空白区域，双击控件即可更改属性。首先设置其Tag属性，将其更改为chengyu，以便区别，并将String属性去掉，不显示控件上的文字，并调整位置和大小，和文字位置相匹配。此时点击Games_1.m或Games_1.fig的运行即可看到控件，在Games_1.m中会出现该控件的回调函数，即function chengyu_Callback()，在GUI界面中右击控件选择View Callbacks中Callback也可跳转至该函数。回调函数用于响应控件时执行的操作。比如新建一个子界面Games_2，并在回调函数中添加run(‘Games_2’)，即可实现跳转。 3.2 设置控件的背景图片&emsp;&emsp;利用set()函数设置对应控件中的CData属性为读取的图片即可实现。设置前需要对图片的像素进行修改，以便能符合控件的大小。12345A=imread('button_start.jpg'); %读取图片 set(handles.chengyu,'CData',A); %将按钮的背景图片设置成Aset(handles.tiyu,'CData',A); set(handles.yanjiwang,'CData',A); set(handles.zhishibaike,'CData',A); &emsp;&emsp;最终效果如图所示： 四、子界面的界面设计和控件的回调函数4.1 子界面的界面设计&emsp;&emsp;子界面的背景和控件添加方法和主界面类似，为了提高程序的运行效率，可将所有的读取操作放在主界面中执行，即添加全局变量，将图片存储在全局变量中，然后在子界面相应的函数中调用该全局变量。（MATLAB中的全局变量需要先用global声明，每次调用前也需要声明。）然后创建3个edit text控件，分别用于显示词语、倒计时的时间和答对个数。最终效果如图所示： 4.2 读取文件并显示&emsp;&emsp;本游戏的玩法为：点击Start，显示框显示词语同时倒计时开始，当回答正确时点击Right，此时答对个数+1并读取下一个词语，当点击Next时，会直接读取下一个词语，但答对个数不变。在这过程中，倒计时一直在进行，直到倒计时显示为0，当所有词语都显示完后，点击Next和Right均无效。&emsp;&emsp;所以首先要从excel表中读取词语，为了提高程序运行效率，将读取操作写在主界面的函数中，并设置其为全局变量。123456789101112131415function Games_1_OpeningFcn(hObject, eventdata, handles, varargin)handles.output = hObject;guidata(hObject, handles);global Time;global class1_i; global class2_i; global class3_i; global class4_i;global class1_Len; global class2_Len; global class3_Len; global class4_Len;global questions1; global questions2; global questions3; global questions4; class1_Len=length(questions1);class2_Len=length(questions2); %词语变量长度class3_Len=length(questions3);class4_Len=length(questions4);class1_i=0;class2_i=0;class3_i=0;class4_i=0; %词语循环变量Time=90; %倒计时时间[~,questions1,~] = xlsread('question1.xlsx');%读取表格，并将其数据放在questions1中[~,questions2,~] = xlsread('question2.xlsx');[~,questions3,~] = xlsread('question3.xlsx');[~,questions4,~] = xlsread('question4.xlsx'); 4.3 Start控件&emsp;&emsp;然后在子界面的Start控件的回调函数中编写代码。12345678910111213141516171819202122232425function Start_Callback(hObject, eventdata, handles)set(handles.Question1,'string','');%清空Question1global Count_Right; global questions1;global class1_i ;global class1_Len;global Time;Count_Right=0; set(handles.Count_Right,'string',0);class1_i=class1_i+1;if class1_i &gt;0 &amp;&amp; class1_i&lt; class1_Len+1 questions_number = class1_i; question_select = questions1(questions_number); set(handles.Question1,'string',question_select);endif class1_i&gt;=class1_Len+1 set(handles.Question1,'string','没有了哦~');endt=clock; %记录开始的时间NowSecond=etime(clock,t); PrSecond=NowSecond;while NowSecond&lt;=Time; if ishandle(handles.second) set(handles.second,'string',num2str(floor(Time-NowSecond))); end pause(0.01) NowSecond=etime(clock,t); %计算当前流逝的时间(秒) end &emsp;&emsp;首先清空词语文本中的内容，即用set()函数将Question1文本控件的String属性设置为空。然后初始化答对个数为0。当词语变量大于0且小于词语数组总长度+1的时候，首先选择词语变量值对应的数组索引中的数据，然后在显示出来，当大于词语数组总长度+1的时候，则显示没有。&emsp;&emsp;第二段是对倒计时的设置。首先将当前时间放在变量t中，clock记录当前的时间，etime()表示现在时间和t之间的差值。当时间差小于指定的倒计时时间时，会一直倒计时。（不加if判断，当倒计时为0时会有警告，不加暂停程序会一直卡住。） 4.4 Right控件1234567891011121314function Right_Callback(hObject, eventdata, handles)set(handles.Question1,'string','');global questions1;global Count_Right ; global class1_i ;global class1_Len;Count_Right=Count_Right+1;set(handles.Count_Right,'string',Count_Right);class1_i=class1_i+1;if class1_i &gt;0 &amp;&amp; class1_i&lt; class1_Len+1 questions_number = class1_i; question_select = questions1(questions_number); set(handles.Question1,'string',question_select);endif class1_i&gt;=class1_Len+1 set(handles.Question1,'string','没有了哦~');end &emsp;&emsp;单击该控件，则答对个数文本值+1，其余和Start控件类似。 4.5 Next控件123456789101112function Next_Question1_Callback(hObject, eventdata, handles)set(handles.Question1,'string','');global questions1; global class1_i ;global class1_Len;class1_i=class1_i+1;if class1_i &gt;0 &amp;&amp; class1_i&lt; class1_Len+1 questions_number = class1_i; question_select = questions1(questions_number); set(handles.Question1,'string',question_select);endif class1_i&gt;=class1_Len+1 set(handles.Question1,'string','没有了哦~');end &emsp;&emsp;单击该控件，词语变量+1，和Start控件第一部分类似。 五、添加游戏背景音乐，当跳转至子界面时停止123[Y,FS]=audioread('Games1.mp3'); %播放歌曲Song_Current = audioplayer(Y,FS); %存储当前播放的歌曲play(Song_Current); &emsp;&emsp;将当前音乐存储在Song_Current变量中，并设置其为全局变量（方便子函数调用）。并且在每次调用子界面时添加stop(Song_Current)以终止音乐。当退出子界面时，需要重新打开该音乐。这是退出界面的回调函数，在GUI界面中，右击空白处可看到该回调函数，即CloseRequestFcn()。 123456function figure1_CloseRequestFcn(hObject, eventdata, handles) global Song_Current; [Y,FS]=audioread('Games1.mp3'); %播放歌曲 Song_Current = audioplayer(Y,FS); %存储当前播放的歌曲 play(Song_Current); delete(hObject); &emsp;&emsp;最后一步删除整个对象以结束。 六、总结&emsp;&emsp;本游戏整体比较简单，代码共享在这里，你话我猜。还望大佬们指点改进。","categories":[{"name":"MATLAB","slug":"MATLAB","permalink":"http://cxx0822.github.io/categories/MATLAB/"}],"tags":[]},{"title":"NAO高尔夫比赛（python初级版）","slug":"NAO高尔夫比赛（python初级版）","date":"2018-11-08T11:45:07.000Z","updated":"2019-03-20T10:14:54.495Z","comments":true,"path":"2018/11/08/NAO高尔夫比赛（python初级版）/","link":"","permalink":"http://cxx0822.github.io/2018/11/08/NAO高尔夫比赛（python初级版）/","excerpt":"","text":"主逻辑 &emsp;&emsp;主逻辑为：找球——&gt;定位——&gt;击球。 找球基本思想&emsp;&emsp;找球的过程可分为两步，即在场地找球和找到球之后走到球附近。&emsp;&emsp;初级版采用地毯式搜索在场地找球，即走一段停下来然后在原地摇头找球，找到则调整身体，使身体正对着球，并走到球附近，否则继续往前走。&emsp;&emsp;找到球后的行走过程中，机器人会不断的看球确认，一旦发现其身体方向并没有正对着球，则停止行走，并再次调整身体，直到走到指定位置。 python程序实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869def walkToBall(self, min_ball_d=0.35, max_ball_theta=0.15): ''' 行走到球附近，若没有找到球则往前走一点 参数： min_ball_d:最小接近球的距离 max_ball_theta:最大偏离角 ''' isfindBall = self.findBall() # 如果没有找到球则摇头找球 if isfindBall is False: while True: isfindBall = self.moveheadToFindball() if isfindBall is False: self.motionProxy.moveInit() self.motionProxy.moveTo(0.5, 0, 0) else: break # 否则找到球，调整机器人身体，使其正对着球 self.motionProxy.moveTo(0, 0, self.ball_info[2]) self.motionProxy.angleInterpolationWithSpeed(\"HeadYaw\", 0, 0.1) # 头部回正 moveTask_1 = self.motionProxy.post.moveTo(1.5 * self.ball_info[0], 0, 0) while True: isfindBall = self.findBall(self.pitchAngle, 0) if isfindBall is False: self.moveheadToFindball(pitchAngles=[-10], yawAngles=[-15, 15]) ball_d = ((self.ball_info[0] ** 2 + self.ball_info[1] ** 2) ** 0.5) ball_theta = abs(self.ball_info[2]) time.sleep(0.5) # 头不断往下低,防止看不到球 Names = [\"HeadPitch\"] if ball_d &gt; 0.4: self.pitchAngle = 0 elif 0.3 &lt; ball_d &lt; 0.4: self.pitchAngle = (-100 * ball_d + 40) elif 0.02 &lt; ball_d &lt; 0.3: self.pitchAngle = 20 self.motionProxy.angleInterpolationWithSpeed(Names, self.pitchAngle * rad, 0.1) # 到达最小距离内，停止 if min_ball_d / 3 &lt; ball_d &lt; min_ball_d: self.tts.say(\"I am right.\") self.motionProxy.stop(moveTask_1) time.sleep(0.5) self.motionProxy.moveInit() break # 偏差在范围内，继续走 elif 0.02 &lt; abs(ball_theta) &lt; max_ball_theta: # 到达最小接近球的距离 if min_ball_d / 3 &lt; ball_d &lt; min_ball_d: self.tts.say(\"I am right.\") self.motionProxy.stop(moveTask_1) time.sleep(0.5) self.motionProxy.moveInit() break else: continue # 超过最大偏离角 elif abs(ball_theta) &gt; max_ball_theta: self.tts.say(\"I am wrong.\") self.motionProxy.stop(moveTask_1) time.sleep(0.5) self.motionProxy.moveInit() self.motionProxy.moveTo(0, 0, self.ball_info[2]) moveTask_1 = self.motionProxy.post.moveTo(1.5 * self.ball_info[0], 0, 0) 程序说明&emsp;&emsp;首先调用看球函数findBall()进行一次找球，如果找到则跳到下面的If判断，否则返回False，进入If结构，即调用摇头找球函数moveheadTofindBall()进行摇头找球，如果还未找到则继续往前走。&emsp;&emsp;找到球后，首先根据返回的ball_Info属性（ball_info = [ball_x, ball_y, ball_theta]）的第3个值，调整身体位置，然后进入While True循环。&emsp;&emsp;进入该循环前，首先调用moveTo()函数，并调用其post属性使其进程挂起，即在行走的过程中，可以做其他事情。为了防止机器人的行走有误差，可将moveTo()函数的x参数加入一个系数，以确保机器人一定能走到球附近。&emsp;&emsp;在行走的过程中，机器人会每隔0.5s看一次球，并通过返回的ball_Info信息来判断是否到达指定位置和身体是否偏移球的位置。&emsp;&emsp;由于到达指定位置的优先级大于是否偏离球的位置，所以将是否到达指定位置的判断放在前面，即不管身体是否偏移，到达位置即可，（后续会再看一次球，调整回来）。&emsp;&emsp;如果未到达指定位置，但身体没有偏离到最大偏差角，即继续执行，一旦偏离最大偏差角，则停止行走，并立即调整身体位姿，并重新调用之前的moveTo()函数。&emsp;&emsp;注：在距离球越近时，机器人应根据距离调整低头的角度，以确定球在视野范围内。在进入走到球附近的程序中，如果没看到球，会进行一次小范围摇头找球，以确定机器人是以看到球的基础上进行校正。 定位三角定位基本思想&emsp;&emsp;定位的主要目的即让机器人调整至最佳击球位置。由于我们采用的是右手握杆击球，即球杆，球和球洞在同一条水平线上。 图1 机器人最佳击球位置 由于机器人到达球的附近时，并不一定是最佳击球位置，所以定位是高尔夫比赛中必不可少的一个环节。初级版采用的是三角定位的方式，即调整机器人的位姿，使得机器人、球和球洞之间的三角形为直角三角形。 图2 三角定位(反手锐角)&emsp;&emsp;如图2所示，红色圆点为球的位置，蓝色圆点为机器人的位置，黄色为黄杆(球洞)。机器人-球-球洞之间的夹角为α,机器人-球洞-球之间的夹角为β,球-机器人-球洞之间的夹角为γ（已知），机器人和球的距离为d（已知），球和球洞的距离为l。&emsp;&emsp;此时由于α为锐角，而最佳击球位置为直角，所以机器人应该向左绕一个以球为圆心，夹角为θ(θ = 0.5 * pi - α)的圆弧。并且此时应用反手击球。同理可得，如果α为钝角，θ = α - 0.5 * pi，机器人向右绕圆弧。&emsp;&emsp;如果机器人在另一侧，方法同上，但击球方式应为正手击球。&emsp;&emsp;但此时存在一个问题，只有黄杆上面的landmark（如图1所示），机器人才能识别出距离，但只有在1米的有效距离内识别出，而机器人可以在很远的距离内识别出黄杆，但无法返回距离。&emsp;&emsp;所以，在远距离时，只能利用黄杆来定位，但在机器人、球和黄杆组成的三角形中，由于已知的信息较少，无法正确解出三角形，因此我们必须假设一个信息是已知的。&emsp;&emsp;我们假设β是已知的，并且球和球洞的距离越远，该角度越小。利用这个假设，我们便可以让机器人通过moveTo()函数，使得到达最佳位置，即机器人-球-球洞的夹角为0.5pi。 python程序实现12345678910111213141516171819202122232425262728293031323334353637383940414243def moveCircle_stick(self, stickAngle, ball_d, compensateAngle1): ''' 绕球走半径为机器人与球的距离的圆弧，分正反手 参数: stickAngle:黄杆与机器人的角度 ball_d:机器人与球的距离 compensateAngle1:补偿角(机器人-杆-球的角度) ''' # 杆-球-机器人的夹角(分锐角和钝角) stick_ball_robotAngle = math.pi - compensateAngle1 * rad - abs(stickAngle) # 正手打 if stickAngle &gt;= 0: self.isbackhandFlag = False if 0 &lt;= stick_ball_robotAngle &lt; 0.5 * math.pi: moveAngle = 0.5 * math.pi - stick_ball_robotAngle move_d = (ball_d ** 2 + ball_d ** 2 - 2 * ball_d * ball_d * math.cos(moveAngle)) ** 0.5 self.motionProxy.moveInit() self.motionProxy.moveTo(0, -move_d, moveAngle) else: moveAngle = stick_ball_robotAngle - 0.5 * math.pi move_d = (ball_d ** 2 + ball_d ** 2 - 2 * ball_d * ball_d * math.cos(moveAngle)) ** 0.5 self.motionProxy.moveInit() self.motionProxy.moveTo(0, move_d, -moveAngle) # 反手打 else: self.isbackhandFlag = True stickAngle = abs(stickAngle) if 0 &lt;= stick_ball_robotAngle &lt; 0.5 * math.pi: moveAngle = 0.5 * math.pi - stick_ball_robotAngle move_d = (ball_d ** 2 + ball_d ** 2 - 2 * ball_d * ball_d * math.cos(0.5 * math.pi - stick_ball_robotAngle)) ** 0.5 self.motionProxy.moveInit() self.motionProxy.moveTo(0, move_d, -moveAngle) else: moveAngle = stick_ball_robotAngle - 0.5 * math.pi move_d = (ball_d ** 2 + ball_d ** 2 - 2 * ball_d * ball_d * math.cos(stick_ball_robotAngle - 0.5 * math.pi)) ** 0.5 self.motionProxy.moveInit() self.motionProxy.moveTo(0, -move_d, moveAngle) 程序说明&emsp;&emsp;首先计算出图2的α，用来判断是锐角还是钝角，其次需要判断机器人和黄杆的角度，如果为正值，应该为正手击球，否则应为反手击球。&emsp;&emsp;在moveTo()的函数中，需要给定的参数为y和θ的值。其中y的值可以用余弦定理求得，而θ为0.5pi与α的差值的绝对值。&emsp;&emsp;注:机器人以左手为y坐标系正半轴，正前方为x坐标系正半轴，设置moveTo()函数的参数时，要注意正负号。 图像定位基本思想&emsp;&emsp;经过三角定位后，机器人基本上到达最佳击球位置，但是我们的最佳击球位置不是(0,0),而是(0.20,-0.05),所以经过三角定位后，还需要左右和上下平移。&emsp;&emsp;之前的方法为利用看球后返回的x和y轴的信息，然后调用moveTo()函数，不断的调整，但实际测试下来，效果并不理想，主要有2个原因。&emsp;&emsp;第一如果机器人不是正对着球，看球的信息会不太准确，而且由于此时要求精度较高，该误差较大。第二机器人本身精度也有误差，特别是此时只是微小移动，很难快速收敛。&emsp;&emsp;针对以上问题，我们决定直接利用球在图像中的信息进行调整，即不需要在进行球的返回信息(x和y)计算，而且此时球在图像中的像素值很大，加入一个比例控制系数就会很快的收敛。 python程序实现123456789101112131415161718192021222324252627def locateWithImage(self, best_ball_x=400, best_ball_y=320, best_ball_radius=28): self.motionProxy.angleInterpolationWithSpeed([\"HeadPitch\", \"HeadYaw\"], [20 * rad, 0 * rad], 0.1) self.ballDetect.updateBallData(client=\"cx\", fitting=True) centerX, centerY, radius = self.ballDetect.getBallInfoInImage() bias_x = centerY - best_ball_y bias_y = centerX - best_ball_x Kp = 0.0008 if abs(radius - best_ball_radius) &lt; 15: if (abs(bias_x) &lt; 40 and abs(bias_y) &lt; 20): self.tts.say(\"I am OK\") return True else: self.motionProxy.moveInit() move_x = Kp * bias_x move_x = -1.0 / 100 if (move_x &lt; 0 and move_x &gt; -1.0 / 100) else move_x move_x = 1.0 / 100 if (move_x &gt; 0 and move_x &lt; 1.0 / 100) else move_x if abs(bias_x) &gt; 15: self.motionProxy.moveTo(-move_x, 0, 0) time.sleep(0.5) move_y = Kp * bias_y move_y = -1.0 / 100 if (move_y &lt; 0 and move_y &gt; -1.0 / 100) else move_y move_y = 1.0 / 100 if (move_y &gt; 0 and move_y &lt; 1.0 / 100) else move_y if abs(bias_y) &gt; 15: self.motionProxy.moveTo(0, -move_y, 0) time.sleep(0.5) 程序说明&emsp;&emsp;首先要测得最佳击球位置在图像中的像素位置，然后根据实际值与期望值的差得到误差，注意此时图像和机器人的坐标系不一样，然后每次调整位置时，加入一个比例系数Kp，实验测得，经过3-4次调整即可快速的收敛，到达最佳击球位置。&emsp;&emsp;注：三角定位要和图像定位结合使用，三角定位是为了调整机器人与球洞的角度，而图像定位是为了让其到达指定的(x,y)位置，为了更精准的到达指定位置，每次循环进行1次三角定位和2次图像定位。实验测得，经过3次左右的循环，便可完成定位，而且其效果也比较理想。 击球基本思想&emsp;&emsp;考虑到机器人与球洞的位置，我们设计了2种击球方式，即正手击球和反手击球。&emsp;&emsp;其方向可以通过黄杆或landmark的角度来判断。当角度为正值时，说明球洞在机器人的左手边，由于我们采用右手击球，所以应为正手击球，否则应为反手击球。 python程序实现12345678910111213141516171819202122232425262728293031323334353637383940414243def forehandToHitball(self, hitSpeed): ''' 正手击球 参数： hitSpeed:击球的力度 ''' # 手回正 names = [\"RShoulderPitch\", \"RShoulderRoll\", \"RElbowRoll\", \"RElbowYaw\", \"RWristYaw\"] maxSpeedFraction = 0.1 targetAngles = [[90 * rad, -20 * rad, 5 * rad, 90 * rad, 0 * rad], [80 * rad, -40 * rad, 5 * rad, 90 * rad, 0 * rad], [50 * rad, -45 * rad, 50 * rad, 90 * rad, -37 * rad], [50 * rad, 5 * rad, 50 * rad, 90 * rad, 20 * rad]] # 击球 for targetAngle in targetAngles: if targetAngle == targetAngles[-1]: maxSpeedFraction = hitSpeed self.motionProxy.angleInterpolationWithSpeed(names, targetAngle, maxSpeedFraction) time.sleep(0.5)def backhandToHitball(self, hitSpeed): ''' 反手击球 参数： hitSpeed：击球的力度 ''' names = [\"RShoulderPitch\", \"RShoulderRoll\", \"RElbowRoll\", \"RElbowYaw\", \"RWristYaw\"] maxSpeedFraction = 0.1 targetAngles = [[90 * rad, -20 * rad, 5 * rad, 90 * rad, 0 * rad], # 手回正 [80 * rad, -40 * rad, 5 * rad, 90 * rad, 0 * rad], [50 * rad, -45 * rad, 50 * rad, 90 * rad, 0 * rad], [50 * rad, 0 * rad, 70 * rad, 90 * rad, 40 * rad], [60 * rad, 2 * rad, 60 * rad, 90 * rad, 40 * rad], [60 * rad, -20 * rad, 60 * rad, 90 * rad, -20 * rad]] for targetAngle in targetAngles: if targetAngle == targetAngles[-1]: maxSpeedFraction = hitSpeed self.motionProxy.angleInterpolationWithSpeed(names, targetAngle, maxSpeedFraction) time.sleep(0.5) 程序说明&emsp;&emsp;无论正手还是反手，都是机器人手臂的一系列关节角变化，通过连续的给每个关节不同的角度，即可实现击球动作。&emsp;&emsp;注：最后一个关节角的速度是击球时的力度，所以其值应不同于其余值，应该给定一个较大的值。 存在的问题和不足&emsp;&emsp;初级版的程序虽然整体上可以实现功能，但都是基于理想的情况下，实际测试下来，程序仍存在一些未知的bug，所以在高级版的程序中，我们要对其进行优化，主要包括以下几个方面。 逻辑思想： 所有的找球，看黄杆/landmark的程序必须考虑完整，即分为看到和没看到2种情况，后续的动作一定要基于之前是找到的情况。 当机器人一直往前走仍找不到球，或因为没看到球而认为没有球一直往前走，这种情况下该怎么办？ 定位时没有看到球，或者看球错误导致球超出视野范围外，需要让机器人摇头找下球，重新定位。 击球的力度能和距离关联，实现不同的距离给定不同的力度。 当球的位置超过球洞，即机器人需绕道球洞后面进行击球，此时应考虑如何避开球洞。 python程序： 每次摇头找球或找黄杆/landmark时，第一次需要摇头，后面则不需要摇头，调用之前的值即可。 利用try-except对程序进行异常处理，防止实际比赛时，程序报错。 注：本博客采用的视觉算法是我师兄的一篇博客：NAO机器人高尔夫中的视觉系统设计","categories":[{"name":"NAO高尔夫比赛","slug":"NAO高尔夫比赛","permalink":"http://cxx0822.github.io/categories/NAO高尔夫比赛/"}],"tags":[]},{"title":"仿人气动手臂:硬件连接图","slug":"仿人气动手臂：硬件连接图","date":"2018-11-08T09:18:07.000Z","updated":"2020-04-21T05:54:35.469Z","comments":true,"path":"2018/11/08/仿人气动手臂：硬件连接图/","link":"","permalink":"http://cxx0822.github.io/2018/11/08/仿人气动手臂：硬件连接图/","excerpt":"","text":"硬件总图 LabVIEW数据采集卡&emsp;&emsp;LabVIEW数据采集卡插到PC机的PCI插槽中。并用连接线与LabVIEW接线盒连接。 LabVIEW接线盒模拟输入端&emsp;&emsp;模拟输入端和气压传感器相连。接线盒的输入端采用单端接入，即连接AI+和GND。气压传感器的白色接线端为信号线，棕色接线端为正极，蓝色接线端为负极。&emsp;&emsp;连接方式：将AI+(接线盒中的任意一个模拟输入端)与气压传感器的白色接线端相接，GND和下一个待接的GND相接(保证接线盒内的所有的地线都连通即可)。气压传感器的棕色接线端和蓝色接线端分别接入电源接线盒的正极和负极，给气压传感器供电。&emsp;&emsp;注：每个LabVIEW接线盒内要有1根地线与电源接线盒的地线(-V)相接。 模拟输出端&emsp;&emsp;模拟输出端和气动比例阀相连。气动比例阀的白色接线端为信号线，棕色接线端为正极，蓝色接线端为负极。&emsp;&emsp;连接方式：将AO(接线盒中的任意一个模拟输出端)与气动比例阀的白色接线端相接，GND和下一个待接的GND相接(保证接线盒内的所有的地线都连通即可)。气动比例阀的棕色接线端和蓝色接线端分别接入电源接线盒的正极和负极，给气动比例阀供电。 气缸使用&emsp;&emsp;开启：①将抽气开关按下，阀门开关闭合。②接通电源。③打开抽气开关(提上去)，气缸开始充气，待气压显示表到达指定气压值时(0.6MPa)，按下开关。④打开阀门开关，即可出气。&emsp;&emsp;关闭：①关闭阀门开关，停止出气。②拔掉电源，气缸每隔一段时间自动漏气。","categories":[],"tags":[]}]}